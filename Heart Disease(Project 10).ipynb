{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import rcParams\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>140</td>\n",
       "      <td>260</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>130</td>\n",
       "      <td>209</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>127</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>132</td>\n",
       "      <td>218</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp trestbps chol fbs  restecg  thalach exang oldpeak slope ca  \\\n",
       "0   63    1   4      140  260   0        1      112     1       3     2  ?   \n",
       "1   44    1   4      130  209   0        1      127     0       0     ?  ?   \n",
       "2   60    1   4      132  218   0        1      140     1     1.5     3  ?   \n",
       "\n",
       "  thal  num  \n",
       "0    ?    2  \n",
       "1    ?    0  \n",
       "2    ?    2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hd=pd.read_csv(\"heart_d.csv\")\n",
    "hd.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>restecg</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>59.350000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>3.505000</td>\n",
       "      <td>0.735000</td>\n",
       "      <td>0.745000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.811697</td>\n",
       "      <td>0.171015</td>\n",
       "      <td>0.795701</td>\n",
       "      <td>0.683455</td>\n",
       "      <td>0.436955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>60.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>64.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age         sex          cp     restecg         num\n",
       "count  200.000000  200.000000  200.000000  200.000000  200.000000\n",
       "mean    59.350000    0.970000    3.505000    0.735000    0.745000\n",
       "std      7.811697    0.171015    0.795701    0.683455    0.436955\n",
       "min     35.000000    0.000000    1.000000    0.000000    0.000000\n",
       "25%     55.000000    1.000000    3.000000    0.000000    0.000000\n",
       "50%     60.000000    1.000000    4.000000    1.000000    1.000000\n",
       "75%     64.000000    1.000000    4.000000    1.000000    1.000000\n",
       "max     77.000000    1.000000    4.000000    2.000000    1.000000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 14 columns):\n",
      "age         200 non-null int64\n",
      "sex         200 non-null int64\n",
      "cp          200 non-null int64\n",
      "trestbps    200 non-null object\n",
      "chol        200 non-null object\n",
      "fbs         200 non-null object\n",
      "restecg     200 non-null int64\n",
      " thalach    200 non-null object\n",
      "exang       200 non-null object\n",
      "oldpeak     200 non-null object\n",
      "slope       200 non-null object\n",
      "ca          200 non-null object\n",
      "thal        200 non-null object\n",
      "num         200 non-null int64\n",
      "dtypes: int64(5), object(9)\n",
      "memory usage: 22.0+ KB\n"
     ]
    }
   ],
   "source": [
    "hd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0\n",
       "sex         0\n",
       "cp          0\n",
       "trestbps    0\n",
       "chol        0\n",
       "fbs         0\n",
       "restecg     0\n",
       " thalach    0\n",
       "exang       0\n",
       "oldpeak     0\n",
       "slope       0\n",
       "ca          0\n",
       "thal        0\n",
       "num         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hd.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>restecg</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.034230</td>\n",
       "      <td>-0.038280</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.189692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>0.034230</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.038036</td>\n",
       "      <td>0.060621</td>\n",
       "      <td>0.098854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cp</th>\n",
       "      <td>-0.038280</td>\n",
       "      <td>0.038036</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.034790</td>\n",
       "      <td>0.256614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>restecg</th>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.060621</td>\n",
       "      <td>0.034790</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.059146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num</th>\n",
       "      <td>0.189692</td>\n",
       "      <td>0.098854</td>\n",
       "      <td>0.256614</td>\n",
       "      <td>-0.059146</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age       sex        cp   restecg       num\n",
       "age      1.000000  0.034230 -0.038280  0.002400  0.189692\n",
       "sex      0.034230  1.000000  0.038036  0.060621  0.098854\n",
       "cp      -0.038280  0.038036  1.000000  0.034790  0.256614\n",
       "restecg  0.002400  0.060621  0.034790  1.000000 -0.059146\n",
       "num      0.189692  0.098854  0.256614 -0.059146  1.000000"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hd.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0\n",
       "sex         0\n",
       "cp          0\n",
       "trestbps    0\n",
       "chol        0\n",
       "fbs         0\n",
       "restecg     0\n",
       " thalach    0\n",
       "exang       0\n",
       "oldpeak     0\n",
       "slope       0\n",
       "ca          0\n",
       "thal        0\n",
       "num         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hd.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd['num'] = hd.num.map({0: 0, 1: 1, 2: 1, 3: 1, 4: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd=hd.replace('?',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>140</td>\n",
       "      <td>260</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>130</td>\n",
       "      <td>209</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>127</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp trestbps chol fbs  restecg  thalach exang oldpeak slope ca  \\\n",
       "0   63    1   4      140  260   0        1      112     1       3     2  0   \n",
       "1   44    1   4      130  209   0        1      127     0       0     0  0   \n",
       "\n",
       "  thal  num  \n",
       "0    0    1  \n",
       "1    0    0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0\n",
       "sex         0\n",
       "cp          0\n",
       "trestbps    0\n",
       "chol        0\n",
       "fbs         0\n",
       "restecg     0\n",
       " thalach    0\n",
       "exang       0\n",
       "oldpeak     0\n",
       "slope       0\n",
       "ca          0\n",
       "thal        0\n",
       "num         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hd.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='sex', data=hd, palette='hls', hue='num')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "JobRole_count = hd['num'].value_counts()\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.barplot(JobRole_count.index, JobRole_count.values, alpha=0.9)\n",
    "plt.title('Relation of chol and oldpeak')\n",
    "plt.ylabel('chol', fontsize=12)\n",
    "plt.xlabel('oldpeak', fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 8))  \n",
    "sns.violinplot(y='sex', x='num', hue='num', data=hd, ax=ax, palette=\"Set3\")  \n",
    "ax.set_title(' sex-wise representation')  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 3600x1080 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hd_corr = hd.corr()\n",
    "\n",
    "plt.figure(figsize=(50, 15))\n",
    "\n",
    "sns.heatmap(hd_corr[(hd_corr >= 0.4) | (hd_corr <= 0.2)], \n",
    "            cmap='PRGn', vmax=1.0, vmin=-1.0, linewidths=0.2,\n",
    "            annot=True, annot_kws={\"size\": 12}, square=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=hd.iloc[:,2:13]\n",
    "y=hd.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 11)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sc = StandardScaler()\n",
    "ss = sc.fit_transform(x)\n",
    "print(ss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1)\n"
     ]
    }
   ],
   "source": [
    "target=hd['num'].values.reshape(-1,1)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(ss,target, test_size= 0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69423548\n",
      "Iteration 2, loss = 0.69347594\n",
      "Iteration 3, loss = 0.69240079\n",
      "Iteration 4, loss = 0.69105048\n",
      "Iteration 5, loss = 0.68946350\n",
      "Iteration 6, loss = 0.68767532\n",
      "Iteration 7, loss = 0.68571697\n",
      "Iteration 8, loss = 0.68361558\n",
      "Iteration 9, loss = 0.68139975\n",
      "Iteration 10, loss = 0.67909310\n",
      "Iteration 11, loss = 0.67672054\n",
      "Iteration 12, loss = 0.67429880\n",
      "Iteration 13, loss = 0.67184432\n",
      "Iteration 14, loss = 0.66936965\n",
      "Iteration 15, loss = 0.66688777\n",
      "Iteration 16, loss = 0.66441351\n",
      "Iteration 17, loss = 0.66196080\n",
      "Iteration 18, loss = 0.65953072\n",
      "Iteration 19, loss = 0.65713349\n",
      "Iteration 20, loss = 0.65477831\n",
      "Iteration 21, loss = 0.65247204\n",
      "Iteration 22, loss = 0.65021615\n",
      "Iteration 23, loss = 0.64801348\n",
      "Iteration 24, loss = 0.64585969\n",
      "Iteration 25, loss = 0.64375932\n",
      "Iteration 26, loss = 0.64171662\n",
      "Iteration 27, loss = 0.63973387\n",
      "Iteration 28, loss = 0.63781005\n",
      "Iteration 29, loss = 0.63594364\n",
      "Iteration 30, loss = 0.63413389\n",
      "Iteration 31, loss = 0.63237781\n",
      "Iteration 32, loss = 0.63067557\n",
      "Iteration 33, loss = 0.62902775\n",
      "Iteration 34, loss = 0.62743558\n",
      "Iteration 35, loss = 0.62589336\n",
      "Iteration 36, loss = 0.62440015\n",
      "Iteration 37, loss = 0.62295728\n",
      "Iteration 38, loss = 0.62156077\n",
      "Iteration 39, loss = 0.62020835\n",
      "Iteration 40, loss = 0.61889800\n",
      "Iteration 41, loss = 0.61763097\n",
      "Iteration 42, loss = 0.61640529\n",
      "Iteration 43, loss = 0.61521932\n",
      "Iteration 44, loss = 0.61407030\n",
      "Iteration 45, loss = 0.61295880\n",
      "Iteration 46, loss = 0.61188182\n",
      "Iteration 47, loss = 0.61083653\n",
      "Iteration 48, loss = 0.60981968\n",
      "Iteration 49, loss = 0.60883271\n",
      "Iteration 50, loss = 0.60787374\n",
      "Iteration 51, loss = 0.60694055\n",
      "Iteration 52, loss = 0.60603224\n",
      "Iteration 53, loss = 0.60514851\n",
      "Iteration 54, loss = 0.60428815\n",
      "Iteration 55, loss = 0.60345106\n",
      "Iteration 56, loss = 0.60263473\n",
      "Iteration 57, loss = 0.60183812\n",
      "Iteration 58, loss = 0.60105932\n",
      "Iteration 59, loss = 0.60029911\n",
      "Iteration 60, loss = 0.59955527\n",
      "Iteration 61, loss = 0.59882694\n",
      "Iteration 62, loss = 0.59811493\n",
      "Iteration 63, loss = 0.59741778\n",
      "Iteration 64, loss = 0.59673416\n",
      "Iteration 65, loss = 0.59606343\n",
      "Iteration 66, loss = 0.59540640\n",
      "Iteration 67, loss = 0.59476130\n",
      "Iteration 68, loss = 0.59412708\n",
      "Iteration 69, loss = 0.59350297\n",
      "Iteration 70, loss = 0.59288894\n",
      "Iteration 71, loss = 0.59228474\n",
      "Iteration 72, loss = 0.59168947\n",
      "Iteration 73, loss = 0.59110273\n",
      "Iteration 74, loss = 0.59052568\n",
      "Iteration 75, loss = 0.58995719\n",
      "Iteration 76, loss = 0.58939626\n",
      "Iteration 77, loss = 0.58884359\n",
      "Iteration 78, loss = 0.58829901\n",
      "Iteration 79, loss = 0.58776158\n",
      "Iteration 80, loss = 0.58723199\n",
      "Iteration 81, loss = 0.58671015\n",
      "Iteration 82, loss = 0.58619522\n",
      "Iteration 83, loss = 0.58568686\n",
      "Iteration 84, loss = 0.58518534\n",
      "Iteration 85, loss = 0.58468974\n",
      "Iteration 86, loss = 0.58419842\n",
      "Iteration 87, loss = 0.58371243\n",
      "Iteration 88, loss = 0.58323172\n",
      "Iteration 89, loss = 0.58275580\n",
      "Iteration 90, loss = 0.58228417\n",
      "Iteration 91, loss = 0.58181660\n",
      "Iteration 92, loss = 0.58135358\n",
      "Iteration 93, loss = 0.58089304\n",
      "Iteration 94, loss = 0.58043568\n",
      "Iteration 95, loss = 0.57998187\n",
      "Iteration 96, loss = 0.57953192\n",
      "Iteration 97, loss = 0.57908568\n",
      "Iteration 98, loss = 0.57864251\n",
      "Iteration 99, loss = 0.57820255\n",
      "Iteration 100, loss = 0.57776467\n",
      "Iteration 101, loss = 0.57733003\n",
      "Iteration 102, loss = 0.57689886\n",
      "Iteration 103, loss = 0.57647005\n",
      "Iteration 104, loss = 0.57604341\n",
      "Iteration 105, loss = 0.57561930\n",
      "Iteration 106, loss = 0.57519795\n",
      "Iteration 107, loss = 0.57478018\n",
      "Iteration 108, loss = 0.57436485\n",
      "Iteration 109, loss = 0.57395185\n",
      "Iteration 110, loss = 0.57354103\n",
      "Iteration 111, loss = 0.57313181\n",
      "Iteration 112, loss = 0.57272430\n",
      "Iteration 113, loss = 0.57231967\n",
      "Iteration 114, loss = 0.57191757\n",
      "Iteration 115, loss = 0.57151754\n",
      "Iteration 116, loss = 0.57111974\n",
      "Iteration 117, loss = 0.57072444\n",
      "Iteration 118, loss = 0.57033219\n",
      "Iteration 119, loss = 0.56994119\n",
      "Iteration 120, loss = 0.56955263\n",
      "Iteration 121, loss = 0.56916614\n",
      "Iteration 122, loss = 0.56878177\n",
      "Iteration 123, loss = 0.56839885\n",
      "Iteration 124, loss = 0.56801812\n",
      "Iteration 125, loss = 0.56763850\n",
      "Iteration 126, loss = 0.56726042\n",
      "Iteration 127, loss = 0.56688244\n",
      "Iteration 128, loss = 0.56650426\n",
      "Iteration 129, loss = 0.56612601\n",
      "Iteration 130, loss = 0.56574841\n",
      "Iteration 131, loss = 0.56537057\n",
      "Iteration 132, loss = 0.56499466\n",
      "Iteration 133, loss = 0.56462198\n",
      "Iteration 134, loss = 0.56425107\n",
      "Iteration 135, loss = 0.56388258\n",
      "Iteration 136, loss = 0.56351671\n",
      "Iteration 137, loss = 0.56315293\n",
      "Iteration 138, loss = 0.56279015\n",
      "Iteration 139, loss = 0.56242846\n",
      "Iteration 140, loss = 0.56206807\n",
      "Iteration 141, loss = 0.56170973\n",
      "Iteration 142, loss = 0.56135267\n",
      "Iteration 143, loss = 0.56099642\n",
      "Iteration 144, loss = 0.56064115\n",
      "Iteration 145, loss = 0.56028727\n",
      "Iteration 146, loss = 0.55993478\n",
      "Iteration 147, loss = 0.55958382\n",
      "Iteration 148, loss = 0.55923529\n",
      "Iteration 149, loss = 0.55888817\n",
      "Iteration 150, loss = 0.55854270\n",
      "Iteration 151, loss = 0.55819931\n",
      "Iteration 152, loss = 0.55785714\n",
      "Iteration 153, loss = 0.55751667\n",
      "Iteration 154, loss = 0.55717801\n",
      "Iteration 155, loss = 0.55684039\n",
      "Iteration 156, loss = 0.55650383\n",
      "Iteration 157, loss = 0.55616958\n",
      "Iteration 158, loss = 0.55583638\n",
      "Iteration 159, loss = 0.55550433\n",
      "Iteration 160, loss = 0.55517348\n",
      "Iteration 161, loss = 0.55484318\n",
      "Iteration 162, loss = 0.55451452\n",
      "Iteration 163, loss = 0.55418702\n",
      "Iteration 164, loss = 0.55386044\n",
      "Iteration 165, loss = 0.55353510\n",
      "Iteration 166, loss = 0.55321048\n",
      "Iteration 167, loss = 0.55288653\n",
      "Iteration 168, loss = 0.55256307\n",
      "Iteration 169, loss = 0.55224098\n",
      "Iteration 170, loss = 0.55191980\n",
      "Iteration 171, loss = 0.55159971\n",
      "Iteration 172, loss = 0.55128167\n",
      "Iteration 173, loss = 0.55096468\n",
      "Iteration 174, loss = 0.55064863\n",
      "Iteration 175, loss = 0.55033315\n",
      "Iteration 176, loss = 0.55001826\n",
      "Iteration 177, loss = 0.54970410\n",
      "Iteration 178, loss = 0.54939077\n",
      "Iteration 179, loss = 0.54907818\n",
      "Iteration 180, loss = 0.54876642\n",
      "Iteration 181, loss = 0.54845525\n",
      "Iteration 182, loss = 0.54814465\n",
      "Iteration 183, loss = 0.54783498\n",
      "Iteration 184, loss = 0.54752594\n",
      "Iteration 185, loss = 0.54721740\n",
      "Iteration 186, loss = 0.54690913\n",
      "Iteration 187, loss = 0.54660115\n",
      "Iteration 188, loss = 0.54629367\n",
      "Iteration 189, loss = 0.54598636\n",
      "Iteration 190, loss = 0.54567985\n",
      "Iteration 191, loss = 0.54537407\n",
      "Iteration 192, loss = 0.54506924\n",
      "Iteration 193, loss = 0.54476526\n",
      "Iteration 194, loss = 0.54446267\n",
      "Iteration 195, loss = 0.54416183\n",
      "Iteration 196, loss = 0.54386126\n",
      "Iteration 197, loss = 0.54356188\n",
      "Iteration 198, loss = 0.54326404\n",
      "Iteration 199, loss = 0.54296709\n",
      "Iteration 200, loss = 0.54267176\n",
      "Iteration 201, loss = 0.54237684\n",
      "Iteration 202, loss = 0.54208304\n",
      "Iteration 203, loss = 0.54178987\n",
      "Iteration 204, loss = 0.54149755\n",
      "Iteration 205, loss = 0.54120609\n",
      "Iteration 206, loss = 0.54091499\n",
      "Iteration 207, loss = 0.54062471\n",
      "Iteration 208, loss = 0.54033462\n",
      "Iteration 209, loss = 0.54004479\n",
      "Iteration 210, loss = 0.53975572\n",
      "Iteration 211, loss = 0.53946815\n",
      "Iteration 212, loss = 0.53918167\n",
      "Iteration 213, loss = 0.53889561\n",
      "Iteration 214, loss = 0.53861019\n",
      "Iteration 215, loss = 0.53832523\n",
      "Iteration 216, loss = 0.53804118\n",
      "Iteration 217, loss = 0.53775758\n",
      "Iteration 218, loss = 0.53747445\n",
      "Iteration 219, loss = 0.53719162\n",
      "Iteration 220, loss = 0.53690932\n",
      "Iteration 221, loss = 0.53662776\n",
      "Iteration 222, loss = 0.53634691\n",
      "Iteration 223, loss = 0.53606664\n",
      "Iteration 224, loss = 0.53578710\n",
      "Iteration 225, loss = 0.53550785\n",
      "Iteration 226, loss = 0.53522934\n",
      "Iteration 227, loss = 0.53495135\n",
      "Iteration 228, loss = 0.53467397\n",
      "Iteration 229, loss = 0.53439749\n",
      "Iteration 230, loss = 0.53412149\n",
      "Iteration 231, loss = 0.53384574\n",
      "Iteration 232, loss = 0.53357004\n",
      "Iteration 233, loss = 0.53329488\n",
      "Iteration 234, loss = 0.53302025\n",
      "Iteration 235, loss = 0.53274591\n",
      "Iteration 236, loss = 0.53247217\n",
      "Iteration 237, loss = 0.53219895\n",
      "Iteration 238, loss = 0.53192620\n",
      "Iteration 239, loss = 0.53165305\n",
      "Iteration 240, loss = 0.53138019\n",
      "Iteration 241, loss = 0.53110746\n",
      "Iteration 242, loss = 0.53083498\n",
      "Iteration 243, loss = 0.53056279\n",
      "Iteration 244, loss = 0.53029133\n",
      "Iteration 245, loss = 0.53002053\n",
      "Iteration 246, loss = 0.52975010\n",
      "Iteration 247, loss = 0.52948018\n",
      "Iteration 248, loss = 0.52921063\n",
      "Iteration 249, loss = 0.52894195\n",
      "Iteration 250, loss = 0.52867360\n",
      "Iteration 251, loss = 0.52840545\n",
      "Iteration 252, loss = 0.52813813\n",
      "Iteration 253, loss = 0.52787116\n",
      "Iteration 254, loss = 0.52760448\n",
      "Iteration 255, loss = 0.52733822\n",
      "Iteration 256, loss = 0.52707240\n",
      "Iteration 257, loss = 0.52680750\n",
      "Iteration 258, loss = 0.52654348\n",
      "Iteration 259, loss = 0.52628025\n",
      "Iteration 260, loss = 0.52601776\n",
      "Iteration 261, loss = 0.52575590\n",
      "Iteration 262, loss = 0.52549442\n",
      "Iteration 263, loss = 0.52523279\n",
      "Iteration 264, loss = 0.52497116\n",
      "Iteration 265, loss = 0.52470979\n",
      "Iteration 266, loss = 0.52444893\n",
      "Iteration 267, loss = 0.52418828\n",
      "Iteration 268, loss = 0.52392779\n",
      "Iteration 269, loss = 0.52366736\n",
      "Iteration 270, loss = 0.52340744\n",
      "Iteration 271, loss = 0.52314759\n",
      "Iteration 272, loss = 0.52288806\n",
      "Iteration 273, loss = 0.52262885\n",
      "Iteration 274, loss = 0.52237003\n",
      "Iteration 275, loss = 0.52211183\n",
      "Iteration 276, loss = 0.52185413\n",
      "Iteration 277, loss = 0.52159620\n",
      "Iteration 278, loss = 0.52133858\n",
      "Iteration 279, loss = 0.52108151\n",
      "Iteration 280, loss = 0.52082472\n",
      "Iteration 281, loss = 0.52056858\n",
      "Iteration 282, loss = 0.52031268\n",
      "Iteration 283, loss = 0.52005680\n",
      "Iteration 284, loss = 0.51980119\n",
      "Iteration 285, loss = 0.51954643\n",
      "Iteration 286, loss = 0.51929225\n",
      "Iteration 287, loss = 0.51903858\n",
      "Iteration 288, loss = 0.51878571\n",
      "Iteration 289, loss = 0.51853390\n",
      "Iteration 290, loss = 0.51828264\n",
      "Iteration 291, loss = 0.51803223\n",
      "Iteration 292, loss = 0.51778250\n",
      "Iteration 293, loss = 0.51753365\n",
      "Iteration 294, loss = 0.51728564\n",
      "Iteration 295, loss = 0.51703774\n",
      "Iteration 296, loss = 0.51679009\n",
      "Iteration 297, loss = 0.51654277\n",
      "Iteration 298, loss = 0.51629583\n",
      "Iteration 299, loss = 0.51604924\n",
      "Iteration 300, loss = 0.51580304\n",
      "Iteration 301, loss = 0.51555708\n",
      "Iteration 302, loss = 0.51531141\n",
      "Iteration 303, loss = 0.51506616\n",
      "Iteration 304, loss = 0.51482137\n",
      "Iteration 305, loss = 0.51457703\n",
      "Iteration 306, loss = 0.51433303\n",
      "Iteration 307, loss = 0.51408921\n",
      "Iteration 308, loss = 0.51384563\n",
      "Iteration 309, loss = 0.51360253\n",
      "Iteration 310, loss = 0.51335988\n",
      "Iteration 311, loss = 0.51311752\n",
      "Iteration 312, loss = 0.51287627\n",
      "Iteration 313, loss = 0.51263563\n",
      "Iteration 314, loss = 0.51239568\n",
      "Iteration 315, loss = 0.51215591\n",
      "Iteration 316, loss = 0.51191645\n",
      "Iteration 317, loss = 0.51167748\n",
      "Iteration 318, loss = 0.51143867\n",
      "Iteration 319, loss = 0.51120024\n",
      "Iteration 320, loss = 0.51096252\n",
      "Iteration 321, loss = 0.51072570\n",
      "Iteration 322, loss = 0.51048924\n",
      "Iteration 323, loss = 0.51025329\n",
      "Iteration 324, loss = 0.51001761\n",
      "Iteration 325, loss = 0.50978217\n",
      "Iteration 326, loss = 0.50954698\n",
      "Iteration 327, loss = 0.50931205\n",
      "Iteration 328, loss = 0.50907767\n",
      "Iteration 329, loss = 0.50884369\n",
      "Iteration 330, loss = 0.50860996\n",
      "Iteration 331, loss = 0.50837659\n",
      "Iteration 332, loss = 0.50814363\n",
      "Iteration 333, loss = 0.50791121\n",
      "Iteration 334, loss = 0.50767938\n",
      "Iteration 335, loss = 0.50744807\n",
      "Iteration 336, loss = 0.50721743\n",
      "Iteration 337, loss = 0.50698718\n",
      "Iteration 338, loss = 0.50675715\n",
      "Iteration 339, loss = 0.50652723\n",
      "Iteration 340, loss = 0.50629755\n",
      "Iteration 341, loss = 0.50606816\n",
      "Iteration 342, loss = 0.50583871\n",
      "Iteration 343, loss = 0.50560984\n",
      "Iteration 344, loss = 0.50538136\n",
      "Iteration 345, loss = 0.50515302\n",
      "Iteration 346, loss = 0.50492498\n",
      "Iteration 347, loss = 0.50469744\n",
      "Iteration 348, loss = 0.50447047\n",
      "Iteration 349, loss = 0.50424370\n",
      "Iteration 350, loss = 0.50401701\n",
      "Iteration 351, loss = 0.50379060\n",
      "Iteration 352, loss = 0.50356440\n",
      "Iteration 353, loss = 0.50333842\n",
      "Iteration 354, loss = 0.50311267\n",
      "Iteration 355, loss = 0.50288710\n",
      "Iteration 356, loss = 0.50266182\n",
      "Iteration 357, loss = 0.50243700\n",
      "Iteration 358, loss = 0.50221301\n",
      "Iteration 359, loss = 0.50198909\n",
      "Iteration 360, loss = 0.50176514\n",
      "Iteration 361, loss = 0.50154115\n",
      "Iteration 362, loss = 0.50131740\n",
      "Iteration 363, loss = 0.50109378\n",
      "Iteration 364, loss = 0.50087036\n",
      "Iteration 365, loss = 0.50064789\n",
      "Iteration 366, loss = 0.50042580\n",
      "Iteration 367, loss = 0.50020380\n",
      "Iteration 368, loss = 0.49998235\n",
      "Iteration 369, loss = 0.49976129\n",
      "Iteration 370, loss = 0.49954073\n",
      "Iteration 371, loss = 0.49932083\n",
      "Iteration 372, loss = 0.49910156\n",
      "Iteration 373, loss = 0.49888281\n",
      "Iteration 374, loss = 0.49866333\n",
      "Iteration 375, loss = 0.49844382\n",
      "Iteration 376, loss = 0.49822422\n",
      "Iteration 377, loss = 0.49800468\n",
      "Iteration 378, loss = 0.49778527\n",
      "Iteration 379, loss = 0.49756591\n",
      "Iteration 380, loss = 0.49734698\n",
      "Iteration 381, loss = 0.49712820\n",
      "Iteration 382, loss = 0.49690922\n",
      "Iteration 383, loss = 0.49669030\n",
      "Iteration 384, loss = 0.49647185\n",
      "Iteration 385, loss = 0.49625349\n",
      "Iteration 386, loss = 0.49603533\n",
      "Iteration 387, loss = 0.49581749\n",
      "Iteration 388, loss = 0.49559969\n",
      "Iteration 389, loss = 0.49538188\n",
      "Iteration 390, loss = 0.49516355\n",
      "Iteration 391, loss = 0.49494499\n",
      "Iteration 392, loss = 0.49472654\n",
      "Iteration 393, loss = 0.49450826\n",
      "Iteration 394, loss = 0.49429011\n",
      "Iteration 395, loss = 0.49407219\n",
      "Iteration 396, loss = 0.49385408\n",
      "Iteration 397, loss = 0.49363500\n",
      "Iteration 398, loss = 0.49341596\n",
      "Iteration 399, loss = 0.49319691\n",
      "Iteration 400, loss = 0.49297810\n",
      "Iteration 401, loss = 0.49275899\n",
      "Iteration 402, loss = 0.49253971\n",
      "Iteration 403, loss = 0.49232033\n",
      "Iteration 404, loss = 0.49210100\n",
      "Iteration 405, loss = 0.49188218\n",
      "Iteration 406, loss = 0.49166319\n",
      "Iteration 407, loss = 0.49144430\n",
      "Iteration 408, loss = 0.49122521\n",
      "Iteration 409, loss = 0.49100655\n",
      "Iteration 410, loss = 0.49078817\n",
      "Iteration 411, loss = 0.49056968\n",
      "Iteration 412, loss = 0.49035129\n",
      "Iteration 413, loss = 0.49013317\n",
      "Iteration 414, loss = 0.48991539\n",
      "Iteration 415, loss = 0.48969778\n",
      "Iteration 416, loss = 0.48948035\n",
      "Iteration 417, loss = 0.48926334\n",
      "Iteration 418, loss = 0.48904724\n",
      "Iteration 419, loss = 0.48883177\n",
      "Iteration 420, loss = 0.48861632\n",
      "Iteration 421, loss = 0.48840064\n",
      "Iteration 422, loss = 0.48818501\n",
      "Iteration 423, loss = 0.48796924\n",
      "Iteration 424, loss = 0.48775372\n",
      "Iteration 425, loss = 0.48753855\n",
      "Iteration 426, loss = 0.48732386\n",
      "Iteration 427, loss = 0.48710952\n",
      "Iteration 428, loss = 0.48689545\n",
      "Iteration 429, loss = 0.48668152\n",
      "Iteration 430, loss = 0.48646778\n",
      "Iteration 431, loss = 0.48625367\n",
      "Iteration 432, loss = 0.48603963\n",
      "Iteration 433, loss = 0.48582564\n",
      "Iteration 434, loss = 0.48561189\n",
      "Iteration 435, loss = 0.48539821\n",
      "Iteration 436, loss = 0.48518450\n",
      "Iteration 437, loss = 0.48497111\n",
      "Iteration 438, loss = 0.48475787\n",
      "Iteration 439, loss = 0.48454541\n",
      "Iteration 440, loss = 0.48433358\n",
      "Iteration 441, loss = 0.48412203\n",
      "Iteration 442, loss = 0.48391090\n",
      "Iteration 443, loss = 0.48370032\n",
      "Iteration 444, loss = 0.48349021\n",
      "Iteration 445, loss = 0.48328013\n",
      "Iteration 446, loss = 0.48307022\n",
      "Iteration 447, loss = 0.48286028\n",
      "Iteration 448, loss = 0.48265059\n",
      "Iteration 449, loss = 0.48244116\n",
      "Iteration 450, loss = 0.48223226\n",
      "Iteration 451, loss = 0.48202314\n",
      "Iteration 452, loss = 0.48181407\n",
      "Iteration 453, loss = 0.48160537\n",
      "Iteration 454, loss = 0.48139713\n",
      "Iteration 455, loss = 0.48118918\n",
      "Iteration 456, loss = 0.48098159\n",
      "Iteration 457, loss = 0.48077449\n",
      "Iteration 458, loss = 0.48056776\n",
      "Iteration 459, loss = 0.48036130\n",
      "Iteration 460, loss = 0.48015536\n",
      "Iteration 461, loss = 0.47994974\n",
      "Iteration 462, loss = 0.47974434\n",
      "Iteration 463, loss = 0.47953922\n",
      "Iteration 464, loss = 0.47933438\n",
      "Iteration 465, loss = 0.47912962\n",
      "Iteration 466, loss = 0.47892527\n",
      "Iteration 467, loss = 0.47872213\n",
      "Iteration 468, loss = 0.47851946\n",
      "Iteration 469, loss = 0.47831712\n",
      "Iteration 470, loss = 0.47811506\n",
      "Iteration 471, loss = 0.47791319\n",
      "Iteration 472, loss = 0.47771151\n",
      "Iteration 473, loss = 0.47751029\n",
      "Iteration 474, loss = 0.47730967\n",
      "Iteration 475, loss = 0.47710960\n",
      "Iteration 476, loss = 0.47690993\n",
      "Iteration 477, loss = 0.47671087\n",
      "Iteration 478, loss = 0.47651186\n",
      "Iteration 479, loss = 0.47631311\n",
      "Iteration 480, loss = 0.47611465\n",
      "Iteration 481, loss = 0.47591657\n",
      "Iteration 482, loss = 0.47571826\n",
      "Iteration 483, loss = 0.47552026\n",
      "Iteration 484, loss = 0.47532232\n",
      "Iteration 485, loss = 0.47512431\n",
      "Iteration 486, loss = 0.47492645\n",
      "Iteration 487, loss = 0.47472874\n",
      "Iteration 488, loss = 0.47453143\n",
      "Iteration 489, loss = 0.47433429\n",
      "Iteration 490, loss = 0.47413736\n",
      "Iteration 491, loss = 0.47394065\n",
      "Iteration 492, loss = 0.47374411\n",
      "Iteration 493, loss = 0.47354790\n",
      "Iteration 494, loss = 0.47335166\n",
      "Iteration 495, loss = 0.47315564\n",
      "Iteration 496, loss = 0.47295981\n",
      "Iteration 497, loss = 0.47276419\n",
      "Iteration 498, loss = 0.47256871\n",
      "Iteration 499, loss = 0.47237347\n",
      "Iteration 500, loss = 0.47217885\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=500, alpha=0.0001,\n",
    "                     solver='sgd', verbose=10,  random_state=21,tol=0.000000001)\n",
    "mlp=mlp.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.72054769\n",
      "Iteration 2, loss = 0.71987507\n",
      "Iteration 3, loss = 0.71906665\n",
      "Iteration 4, loss = 0.71795755\n",
      "Iteration 5, loss = 0.71660402\n",
      "Iteration 6, loss = 0.71502996\n",
      "Iteration 7, loss = 0.71335529\n",
      "Iteration 8, loss = 0.71149498\n",
      "Iteration 9, loss = 0.70955923\n",
      "Iteration 10, loss = 0.70765423\n",
      "Iteration 11, loss = 0.70576997\n",
      "Iteration 12, loss = 0.70380060\n",
      "Iteration 13, loss = 0.70177610\n",
      "Iteration 14, loss = 0.69979521\n",
      "Iteration 15, loss = 0.69776059\n",
      "Iteration 16, loss = 0.69561109\n",
      "Iteration 17, loss = 0.69341117\n",
      "Iteration 18, loss = 0.69149310\n",
      "Iteration 19, loss = 0.68965285\n",
      "Iteration 20, loss = 0.68790251\n",
      "Iteration 21, loss = 0.68619263\n",
      "Iteration 22, loss = 0.68464806\n",
      "Iteration 23, loss = 0.68288729\n",
      "Iteration 24, loss = 0.68102423\n",
      "Iteration 25, loss = 0.67897952\n",
      "Iteration 26, loss = 0.67711626\n",
      "Iteration 27, loss = 0.67516944\n",
      "Iteration 28, loss = 0.67336731\n",
      "Iteration 29, loss = 0.67151684\n",
      "Iteration 30, loss = 0.66953458\n",
      "Iteration 31, loss = 0.66732126\n",
      "Iteration 32, loss = 0.66501415\n",
      "Iteration 33, loss = 0.66275275\n",
      "Iteration 34, loss = 0.66043953\n",
      "Iteration 35, loss = 0.65825901\n",
      "Iteration 36, loss = 0.65608889\n",
      "Iteration 37, loss = 0.65401309\n",
      "Iteration 38, loss = 0.65211666\n",
      "Iteration 39, loss = 0.65010842\n",
      "Iteration 40, loss = 0.64782321\n",
      "Iteration 41, loss = 0.64563116\n",
      "Iteration 42, loss = 0.64353515\n",
      "Iteration 43, loss = 0.64150669\n",
      "Iteration 44, loss = 0.63952659\n",
      "Iteration 45, loss = 0.63740332\n",
      "Iteration 46, loss = 0.63522256\n",
      "Iteration 47, loss = 0.63309442\n",
      "Iteration 48, loss = 0.63098199\n",
      "Iteration 49, loss = 0.62887802\n",
      "Iteration 50, loss = 0.62680729\n",
      "Iteration 51, loss = 0.62460412\n",
      "Iteration 52, loss = 0.62244039\n",
      "Iteration 53, loss = 0.62024894\n",
      "Iteration 54, loss = 0.61814135\n",
      "Iteration 55, loss = 0.61612727\n",
      "Iteration 56, loss = 0.61411397\n",
      "Iteration 57, loss = 0.61204580\n",
      "Iteration 58, loss = 0.60976910\n",
      "Iteration 59, loss = 0.60759128\n",
      "Iteration 60, loss = 0.60550032\n",
      "Iteration 61, loss = 0.60354353\n",
      "Iteration 62, loss = 0.60158595\n",
      "Iteration 63, loss = 0.59969110\n",
      "Iteration 64, loss = 0.59775918\n",
      "Iteration 65, loss = 0.59575246\n",
      "Iteration 66, loss = 0.59367678\n",
      "Iteration 67, loss = 0.59152289\n",
      "Iteration 68, loss = 0.58954154\n",
      "Iteration 69, loss = 0.58767140\n",
      "Iteration 70, loss = 0.58583271\n",
      "Iteration 71, loss = 0.58390412\n",
      "Iteration 72, loss = 0.58196116\n",
      "Iteration 73, loss = 0.57991890\n",
      "Iteration 74, loss = 0.57771232\n",
      "Iteration 75, loss = 0.57571617\n",
      "Iteration 76, loss = 0.57375762\n",
      "Iteration 77, loss = 0.57186852\n",
      "Iteration 78, loss = 0.57007247\n",
      "Iteration 79, loss = 0.56831536\n",
      "Iteration 80, loss = 0.56639490\n",
      "Iteration 81, loss = 0.56454554\n",
      "Iteration 82, loss = 0.56278905\n",
      "Iteration 83, loss = 0.56073837\n",
      "Iteration 84, loss = 0.55852990\n",
      "Iteration 85, loss = 0.55624979\n",
      "Iteration 86, loss = 0.55433733\n",
      "Iteration 87, loss = 0.55256034\n",
      "Iteration 88, loss = 0.55071048\n",
      "Iteration 89, loss = 0.54890639\n",
      "Iteration 90, loss = 0.54725141\n",
      "Iteration 91, loss = 0.54521223\n",
      "Iteration 92, loss = 0.54337469\n",
      "Iteration 93, loss = 0.54142091\n",
      "Iteration 94, loss = 0.53931763\n",
      "Iteration 95, loss = 0.53730275\n",
      "Iteration 96, loss = 0.53497330\n",
      "Iteration 97, loss = 0.53271302\n",
      "Iteration 98, loss = 0.53061314\n",
      "Iteration 99, loss = 0.52837791\n",
      "Iteration 100, loss = 0.52636316\n",
      "Iteration 101, loss = 0.52407065\n",
      "Iteration 102, loss = 0.52170339\n",
      "Iteration 103, loss = 0.51965595\n",
      "Iteration 104, loss = 0.51774087\n",
      "Iteration 105, loss = 0.51545400\n",
      "Iteration 106, loss = 0.51341265\n",
      "Iteration 107, loss = 0.51120762\n",
      "Iteration 108, loss = 0.50922430\n",
      "Iteration 109, loss = 0.50771529\n",
      "Iteration 110, loss = 0.50637687\n",
      "Iteration 111, loss = 0.50519818\n",
      "Iteration 112, loss = 0.50369718\n",
      "Iteration 113, loss = 0.50212517\n",
      "Iteration 114, loss = 0.50111127\n",
      "Iteration 115, loss = 0.49939630\n",
      "Iteration 116, loss = 0.49777154\n",
      "Iteration 117, loss = 0.49628322\n",
      "Iteration 118, loss = 0.49505167\n",
      "Iteration 119, loss = 0.49403221\n",
      "Iteration 120, loss = 0.49276038\n",
      "Iteration 121, loss = 0.49103755\n",
      "Iteration 122, loss = 0.48911125\n",
      "Iteration 123, loss = 0.48731850\n",
      "Iteration 124, loss = 0.48515439\n",
      "Iteration 125, loss = 0.48308088\n",
      "Iteration 126, loss = 0.48097900\n",
      "Iteration 127, loss = 0.47872771\n",
      "Iteration 128, loss = 0.47624490\n",
      "Iteration 129, loss = 0.47389762\n",
      "Iteration 130, loss = 0.47152506\n",
      "Iteration 131, loss = 0.46927427\n",
      "Iteration 132, loss = 0.46731828\n",
      "Iteration 133, loss = 0.46535073\n",
      "Iteration 134, loss = 0.46365585\n",
      "Iteration 135, loss = 0.46205795\n",
      "Iteration 136, loss = 0.46044916\n",
      "Iteration 137, loss = 0.45892989\n",
      "Iteration 138, loss = 0.45764488\n",
      "Iteration 139, loss = 0.45611783\n",
      "Iteration 140, loss = 0.45451655\n",
      "Iteration 141, loss = 0.45293050\n",
      "Iteration 142, loss = 0.45126534\n",
      "Iteration 143, loss = 0.44983783\n",
      "Iteration 144, loss = 0.44839502\n",
      "Iteration 145, loss = 0.44685882\n",
      "Iteration 146, loss = 0.44529906\n",
      "Iteration 147, loss = 0.44388588\n",
      "Iteration 148, loss = 0.44272965\n",
      "Iteration 149, loss = 0.44176225\n",
      "Iteration 150, loss = 0.44054395\n",
      "Iteration 151, loss = 0.43924817\n",
      "Iteration 152, loss = 0.43791332\n",
      "Iteration 153, loss = 0.43644887\n",
      "Iteration 154, loss = 0.43485967\n",
      "Iteration 155, loss = 0.43322728\n",
      "Iteration 156, loss = 0.43163159\n",
      "Iteration 157, loss = 0.43014144\n",
      "Iteration 158, loss = 0.42871283\n",
      "Iteration 159, loss = 0.42756576\n",
      "Iteration 160, loss = 0.42632890\n",
      "Iteration 161, loss = 0.42506977\n",
      "Iteration 162, loss = 0.42416247\n",
      "Iteration 163, loss = 0.42333662\n",
      "Iteration 164, loss = 0.42295031\n",
      "Iteration 165, loss = 0.42246084\n",
      "Iteration 166, loss = 0.42177297\n",
      "Iteration 167, loss = 0.42099430\n",
      "Iteration 168, loss = 0.41997365\n",
      "Iteration 169, loss = 0.41881307\n",
      "Iteration 170, loss = 0.41767686\n",
      "Iteration 171, loss = 0.41667157\n",
      "Iteration 172, loss = 0.41570888\n",
      "Iteration 173, loss = 0.41473849\n",
      "Iteration 174, loss = 0.41372585\n",
      "Iteration 175, loss = 0.41264692\n",
      "Iteration 176, loss = 0.41190794\n",
      "Iteration 177, loss = 0.41116919\n",
      "Iteration 178, loss = 0.41040012\n",
      "Iteration 179, loss = 0.40962401\n",
      "Iteration 180, loss = 0.40882546\n",
      "Iteration 181, loss = 0.40805508\n",
      "Iteration 182, loss = 0.40738513\n",
      "Iteration 183, loss = 0.40692059\n",
      "Iteration 184, loss = 0.40638469\n",
      "Iteration 185, loss = 0.40569295\n",
      "Iteration 186, loss = 0.40500547\n",
      "Iteration 187, loss = 0.40432746\n",
      "Iteration 188, loss = 0.40363076\n",
      "Iteration 189, loss = 0.40273529\n",
      "Iteration 190, loss = 0.40191436\n",
      "Iteration 191, loss = 0.40130296\n",
      "Iteration 192, loss = 0.40080904\n",
      "Iteration 193, loss = 0.40026159\n",
      "Iteration 194, loss = 0.39964329\n",
      "Iteration 195, loss = 0.39873584\n",
      "Iteration 196, loss = 0.39785783\n",
      "Iteration 197, loss = 0.39713607\n",
      "Iteration 198, loss = 0.39661447\n",
      "Iteration 199, loss = 0.39596056\n",
      "Iteration 200, loss = 0.39527414\n",
      "Iteration 201, loss = 0.39448478\n",
      "Iteration 202, loss = 0.39379320\n",
      "Iteration 203, loss = 0.39300929\n",
      "Iteration 204, loss = 0.39215272\n",
      "Iteration 205, loss = 0.39148452\n",
      "Iteration 206, loss = 0.39042682\n",
      "Iteration 207, loss = 0.38947394\n",
      "Iteration 208, loss = 0.38859052\n",
      "Iteration 209, loss = 0.38754942\n",
      "Iteration 210, loss = 0.38649710\n",
      "Iteration 211, loss = 0.38526231\n",
      "Iteration 212, loss = 0.38430143\n",
      "Iteration 213, loss = 0.38346914\n",
      "Iteration 214, loss = 0.38266832\n",
      "Iteration 215, loss = 0.38192218\n",
      "Iteration 216, loss = 0.38113802\n",
      "Iteration 217, loss = 0.38035208\n",
      "Iteration 218, loss = 0.37956730\n",
      "Iteration 219, loss = 0.37875088\n",
      "Iteration 220, loss = 0.37805525\n",
      "Iteration 221, loss = 0.37730161\n",
      "Iteration 222, loss = 0.37642811\n",
      "Iteration 223, loss = 0.37560764\n",
      "Iteration 224, loss = 0.37490979\n",
      "Iteration 225, loss = 0.37421884\n",
      "Iteration 226, loss = 0.37366831\n",
      "Iteration 227, loss = 0.37307470\n",
      "Iteration 228, loss = 0.37243182\n",
      "Iteration 229, loss = 0.37178690\n",
      "Iteration 230, loss = 0.37136497\n",
      "Iteration 231, loss = 0.37084282\n",
      "Iteration 232, loss = 0.37035452\n",
      "Iteration 233, loss = 0.36988856\n",
      "Iteration 234, loss = 0.36952892\n",
      "Iteration 235, loss = 0.36921300\n",
      "Iteration 236, loss = 0.36893773\n",
      "Iteration 237, loss = 0.36835883\n",
      "Iteration 238, loss = 0.36773409\n",
      "Iteration 239, loss = 0.36699199\n",
      "Iteration 240, loss = 0.36632496\n",
      "Iteration 241, loss = 0.36566969\n",
      "Iteration 242, loss = 0.36504699\n",
      "Iteration 243, loss = 0.36453609\n",
      "Iteration 244, loss = 0.36399904\n",
      "Iteration 245, loss = 0.36363571\n",
      "Iteration 246, loss = 0.36329859\n",
      "Iteration 247, loss = 0.36291312\n",
      "Iteration 248, loss = 0.36251433\n",
      "Iteration 249, loss = 0.36211193\n",
      "Iteration 250, loss = 0.36167841\n",
      "Iteration 251, loss = 0.36125907\n",
      "Iteration 252, loss = 0.36083204\n",
      "Iteration 253, loss = 0.36041808\n",
      "Iteration 254, loss = 0.35998954\n",
      "Iteration 255, loss = 0.35952981\n",
      "Iteration 256, loss = 0.35904216\n",
      "Iteration 257, loss = 0.35853441\n",
      "Iteration 258, loss = 0.35803696\n",
      "Iteration 259, loss = 0.35758489\n",
      "Iteration 260, loss = 0.35736984\n",
      "Iteration 261, loss = 0.35696383\n",
      "Iteration 262, loss = 0.35654331\n",
      "Iteration 263, loss = 0.35616693\n",
      "Iteration 264, loss = 0.35555921\n",
      "Iteration 265, loss = 0.35514093\n",
      "Iteration 266, loss = 0.35471514\n",
      "Iteration 267, loss = 0.35429540\n",
      "Iteration 268, loss = 0.35387137\n",
      "Iteration 269, loss = 0.35333909\n",
      "Iteration 270, loss = 0.35291301\n",
      "Iteration 271, loss = 0.35251962\n",
      "Iteration 272, loss = 0.35229631\n",
      "Iteration 273, loss = 0.35200788\n",
      "Iteration 274, loss = 0.35165401\n",
      "Iteration 275, loss = 0.35171317\n",
      "Iteration 276, loss = 0.35177614\n",
      "Iteration 277, loss = 0.35156333\n",
      "Iteration 278, loss = 0.35131384\n",
      "Iteration 279, loss = 0.35115162\n",
      "Iteration 280, loss = 0.35083024\n",
      "Iteration 281, loss = 0.35031255\n",
      "Iteration 282, loss = 0.34933616\n",
      "Iteration 283, loss = 0.34829373\n",
      "Iteration 284, loss = 0.34749307\n",
      "Iteration 285, loss = 0.34682413\n",
      "Iteration 286, loss = 0.34620540\n",
      "Iteration 287, loss = 0.34565404\n",
      "Iteration 288, loss = 0.34510472\n",
      "Iteration 289, loss = 0.34468878\n",
      "Iteration 290, loss = 0.34422757\n",
      "Iteration 291, loss = 0.34385535\n",
      "Iteration 292, loss = 0.34344221\n",
      "Iteration 293, loss = 0.34308611\n",
      "Iteration 294, loss = 0.34270060\n",
      "Iteration 295, loss = 0.34228558\n",
      "Iteration 296, loss = 0.34189979\n",
      "Iteration 297, loss = 0.34155806\n",
      "Iteration 298, loss = 0.34141016\n",
      "Iteration 299, loss = 0.34163196\n",
      "Iteration 300, loss = 0.34135143\n",
      "Iteration 301, loss = 0.34092304\n",
      "Iteration 302, loss = 0.34048361\n",
      "Iteration 303, loss = 0.34015260\n",
      "Iteration 304, loss = 0.33992429\n",
      "Iteration 305, loss = 0.33945357\n",
      "Iteration 306, loss = 0.33910417\n",
      "Iteration 307, loss = 0.33864239\n",
      "Iteration 308, loss = 0.33827132\n",
      "Iteration 309, loss = 0.33797838\n",
      "Iteration 310, loss = 0.33798985\n",
      "Iteration 311, loss = 0.33799558\n",
      "Iteration 312, loss = 0.33774427\n",
      "Iteration 313, loss = 0.33766030\n",
      "Iteration 314, loss = 0.33766226\n",
      "Iteration 315, loss = 0.33724649\n",
      "Iteration 316, loss = 0.33664546\n",
      "Iteration 317, loss = 0.33609272\n",
      "Iteration 318, loss = 0.33550950\n",
      "Iteration 319, loss = 0.33495323\n",
      "Iteration 320, loss = 0.33437418\n",
      "Iteration 321, loss = 0.33379527\n",
      "Iteration 322, loss = 0.33342730\n",
      "Iteration 323, loss = 0.33299242\n",
      "Iteration 324, loss = 0.33256094\n",
      "Iteration 325, loss = 0.33233038\n",
      "Iteration 326, loss = 0.33216048\n",
      "Iteration 327, loss = 0.33195389\n",
      "Iteration 328, loss = 0.33228947\n",
      "Iteration 329, loss = 0.33198188\n",
      "Iteration 330, loss = 0.33184200\n",
      "Iteration 331, loss = 0.33171802\n",
      "Iteration 332, loss = 0.33188836\n",
      "Iteration 333, loss = 0.33198890\n",
      "Iteration 334, loss = 0.33228612\n",
      "Iteration 335, loss = 0.33226087\n",
      "Iteration 336, loss = 0.33143735\n",
      "Iteration 337, loss = 0.33059468\n",
      "Iteration 338, loss = 0.32975703\n",
      "Iteration 339, loss = 0.32891196\n",
      "Iteration 340, loss = 0.32834817\n",
      "Iteration 341, loss = 0.32750418\n",
      "Iteration 342, loss = 0.32694534\n",
      "Iteration 343, loss = 0.32667020\n",
      "Iteration 344, loss = 0.32634496\n",
      "Iteration 345, loss = 0.32600701\n",
      "Iteration 346, loss = 0.32535459\n",
      "Iteration 347, loss = 0.32491865\n",
      "Iteration 348, loss = 0.32447435\n",
      "Iteration 349, loss = 0.32392642\n",
      "Iteration 350, loss = 0.32333114\n",
      "Iteration 351, loss = 0.32286588\n",
      "Iteration 352, loss = 0.32250848\n",
      "Iteration 353, loss = 0.32225272\n",
      "Iteration 354, loss = 0.32182742\n",
      "Iteration 355, loss = 0.32143653\n",
      "Iteration 356, loss = 0.32109512\n",
      "Iteration 357, loss = 0.32075806\n",
      "Iteration 358, loss = 0.32042211\n",
      "Iteration 359, loss = 0.32010239\n",
      "Iteration 360, loss = 0.31982338\n",
      "Iteration 361, loss = 0.31955300\n",
      "Iteration 362, loss = 0.31919954\n",
      "Iteration 363, loss = 0.31871999\n",
      "Iteration 364, loss = 0.31831166\n",
      "Iteration 365, loss = 0.31806561\n",
      "Iteration 366, loss = 0.31797913\n",
      "Iteration 367, loss = 0.31777461\n",
      "Iteration 368, loss = 0.31738809\n",
      "Iteration 369, loss = 0.31706034\n",
      "Iteration 370, loss = 0.31677855\n",
      "Iteration 371, loss = 0.31646207\n",
      "Iteration 372, loss = 0.31612491\n",
      "Iteration 373, loss = 0.31581076\n",
      "Iteration 374, loss = 0.31554732\n",
      "Iteration 375, loss = 0.31527869\n",
      "Iteration 376, loss = 0.31507843\n",
      "Iteration 377, loss = 0.31517636\n",
      "Iteration 378, loss = 0.31497874\n",
      "Iteration 379, loss = 0.31446435\n",
      "Iteration 380, loss = 0.31403567\n",
      "Iteration 381, loss = 0.31357291\n",
      "Iteration 382, loss = 0.31324162\n",
      "Iteration 383, loss = 0.31295298\n",
      "Iteration 384, loss = 0.31273013\n",
      "Iteration 385, loss = 0.31244878\n",
      "Iteration 386, loss = 0.31213818\n",
      "Iteration 387, loss = 0.31187684\n",
      "Iteration 388, loss = 0.31158195\n",
      "Iteration 389, loss = 0.31131050\n",
      "Iteration 390, loss = 0.31103169\n",
      "Iteration 391, loss = 0.31080916\n",
      "Iteration 392, loss = 0.31047300\n",
      "Iteration 393, loss = 0.31021272\n",
      "Iteration 394, loss = 0.30998922\n",
      "Iteration 395, loss = 0.30988996\n",
      "Iteration 396, loss = 0.30981267\n",
      "Iteration 397, loss = 0.30943225\n",
      "Iteration 398, loss = 0.30917758\n",
      "Iteration 399, loss = 0.30878313\n",
      "Iteration 400, loss = 0.30845511\n",
      "Iteration 401, loss = 0.30811057\n",
      "Iteration 402, loss = 0.30780934\n",
      "Iteration 403, loss = 0.30756696\n",
      "Iteration 404, loss = 0.30740508\n",
      "Iteration 405, loss = 0.30732162\n",
      "Iteration 406, loss = 0.30739772\n",
      "Iteration 407, loss = 0.30759826\n",
      "Iteration 408, loss = 0.30771813\n",
      "Iteration 409, loss = 0.30823497\n",
      "Iteration 410, loss = 0.30842922\n",
      "Iteration 411, loss = 0.30827004\n",
      "Iteration 412, loss = 0.30854685\n",
      "Iteration 413, loss = 0.30910702\n",
      "Iteration 414, loss = 0.30933175\n",
      "Iteration 415, loss = 0.30971225\n",
      "Iteration 416, loss = 0.30980860\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71760789\n",
      "Iteration 2, loss = 0.71706379\n",
      "Iteration 3, loss = 0.71639618\n",
      "Iteration 4, loss = 0.71548045\n",
      "Iteration 5, loss = 0.71429576\n",
      "Iteration 6, loss = 0.71287395\n",
      "Iteration 7, loss = 0.71135105\n",
      "Iteration 8, loss = 0.70966737\n",
      "Iteration 9, loss = 0.70790480\n",
      "Iteration 10, loss = 0.70617674\n",
      "Iteration 11, loss = 0.70454956\n",
      "Iteration 12, loss = 0.70282271\n",
      "Iteration 13, loss = 0.70100787\n",
      "Iteration 14, loss = 0.69920021\n",
      "Iteration 15, loss = 0.69733937\n",
      "Iteration 16, loss = 0.69546975\n",
      "Iteration 17, loss = 0.69351881\n",
      "Iteration 18, loss = 0.69183033\n",
      "Iteration 19, loss = 0.69009462\n",
      "Iteration 20, loss = 0.68831877\n",
      "Iteration 21, loss = 0.68646365\n",
      "Iteration 22, loss = 0.68463249\n",
      "Iteration 23, loss = 0.68270686\n",
      "Iteration 24, loss = 0.68073340\n",
      "Iteration 25, loss = 0.67865734\n",
      "Iteration 26, loss = 0.67673375\n",
      "Iteration 27, loss = 0.67477307\n",
      "Iteration 28, loss = 0.67294277\n",
      "Iteration 29, loss = 0.67107303\n",
      "Iteration 30, loss = 0.66927315\n",
      "Iteration 31, loss = 0.66725363\n",
      "Iteration 32, loss = 0.66516394\n",
      "Iteration 33, loss = 0.66317745\n",
      "Iteration 34, loss = 0.66111203\n",
      "Iteration 35, loss = 0.65899468\n",
      "Iteration 36, loss = 0.65693956\n",
      "Iteration 37, loss = 0.65496898\n",
      "Iteration 38, loss = 0.65311868\n",
      "Iteration 39, loss = 0.65135349\n",
      "Iteration 40, loss = 0.64960897\n",
      "Iteration 41, loss = 0.64786792\n",
      "Iteration 42, loss = 0.64613979\n",
      "Iteration 43, loss = 0.64441745\n",
      "Iteration 44, loss = 0.64275756\n",
      "Iteration 45, loss = 0.64087490\n",
      "Iteration 46, loss = 0.63900632\n",
      "Iteration 47, loss = 0.63716235\n",
      "Iteration 48, loss = 0.63528713\n",
      "Iteration 49, loss = 0.63340541\n",
      "Iteration 50, loss = 0.63160728\n",
      "Iteration 51, loss = 0.62964319\n",
      "Iteration 52, loss = 0.62766834\n",
      "Iteration 53, loss = 0.62563107\n",
      "Iteration 54, loss = 0.62362607\n",
      "Iteration 55, loss = 0.62168775\n",
      "Iteration 56, loss = 0.61980884\n",
      "Iteration 57, loss = 0.61779402\n",
      "Iteration 58, loss = 0.61555284\n",
      "Iteration 59, loss = 0.61345651\n",
      "Iteration 60, loss = 0.61145040\n",
      "Iteration 61, loss = 0.60961146\n",
      "Iteration 62, loss = 0.60782647\n",
      "Iteration 63, loss = 0.60608291\n",
      "Iteration 64, loss = 0.60427273\n",
      "Iteration 65, loss = 0.60242034\n",
      "Iteration 66, loss = 0.60059294\n",
      "Iteration 67, loss = 0.59866961\n",
      "Iteration 68, loss = 0.59690204\n",
      "Iteration 69, loss = 0.59525617\n",
      "Iteration 70, loss = 0.59364723\n",
      "Iteration 71, loss = 0.59196921\n",
      "Iteration 72, loss = 0.59031162\n",
      "Iteration 73, loss = 0.58852761\n",
      "Iteration 74, loss = 0.58648611\n",
      "Iteration 75, loss = 0.58464300\n",
      "Iteration 76, loss = 0.58285532\n",
      "Iteration 77, loss = 0.58123608\n",
      "Iteration 78, loss = 0.57968524\n",
      "Iteration 79, loss = 0.57810932\n",
      "Iteration 80, loss = 0.57637415\n",
      "Iteration 81, loss = 0.57474452\n",
      "Iteration 82, loss = 0.57337616\n",
      "Iteration 83, loss = 0.57168410\n",
      "Iteration 84, loss = 0.56978714\n",
      "Iteration 85, loss = 0.56778026\n",
      "Iteration 86, loss = 0.56608434\n",
      "Iteration 87, loss = 0.56442038\n",
      "Iteration 88, loss = 0.56266554\n",
      "Iteration 89, loss = 0.56076485\n",
      "Iteration 90, loss = 0.55903359\n",
      "Iteration 91, loss = 0.55694650\n",
      "Iteration 92, loss = 0.55508554\n",
      "Iteration 93, loss = 0.55314386\n",
      "Iteration 94, loss = 0.55112482\n",
      "Iteration 95, loss = 0.54921138\n",
      "Iteration 96, loss = 0.54700160\n",
      "Iteration 97, loss = 0.54486761\n",
      "Iteration 98, loss = 0.54275408\n",
      "Iteration 99, loss = 0.54055238\n",
      "Iteration 100, loss = 0.53856803\n",
      "Iteration 101, loss = 0.53635655\n",
      "Iteration 102, loss = 0.53414268\n",
      "Iteration 103, loss = 0.53224222\n",
      "Iteration 104, loss = 0.53060417\n",
      "Iteration 105, loss = 0.52867471\n",
      "Iteration 106, loss = 0.52696220\n",
      "Iteration 107, loss = 0.52527480\n",
      "Iteration 108, loss = 0.52364988\n",
      "Iteration 109, loss = 0.52227858\n",
      "Iteration 110, loss = 0.52100063\n",
      "Iteration 111, loss = 0.51981425\n",
      "Iteration 112, loss = 0.51831173\n",
      "Iteration 113, loss = 0.51667191\n",
      "Iteration 114, loss = 0.51543836\n",
      "Iteration 115, loss = 0.51354336\n",
      "Iteration 116, loss = 0.51177896\n",
      "Iteration 117, loss = 0.51029120\n",
      "Iteration 118, loss = 0.50910812\n",
      "Iteration 119, loss = 0.50815094\n",
      "Iteration 120, loss = 0.50697922\n",
      "Iteration 121, loss = 0.50536911\n",
      "Iteration 122, loss = 0.50354228\n",
      "Iteration 123, loss = 0.50180396\n",
      "Iteration 124, loss = 0.49969696\n",
      "Iteration 125, loss = 0.49769007\n",
      "Iteration 126, loss = 0.49561950\n",
      "Iteration 127, loss = 0.49349493\n",
      "Iteration 128, loss = 0.49107256\n",
      "Iteration 129, loss = 0.48876265\n",
      "Iteration 130, loss = 0.48646009\n",
      "Iteration 131, loss = 0.48422132\n",
      "Iteration 132, loss = 0.48228346\n",
      "Iteration 133, loss = 0.48032629\n",
      "Iteration 134, loss = 0.47863153\n",
      "Iteration 135, loss = 0.47700751\n",
      "Iteration 136, loss = 0.47534569\n",
      "Iteration 137, loss = 0.47380260\n",
      "Iteration 138, loss = 0.47251622\n",
      "Iteration 139, loss = 0.47097601\n",
      "Iteration 140, loss = 0.46928628\n",
      "Iteration 141, loss = 0.46763727\n",
      "Iteration 142, loss = 0.46589165\n",
      "Iteration 143, loss = 0.46432753\n",
      "Iteration 144, loss = 0.46281300\n",
      "Iteration 145, loss = 0.46119706\n",
      "Iteration 146, loss = 0.45958929\n",
      "Iteration 147, loss = 0.45809948\n",
      "Iteration 148, loss = 0.45691458\n",
      "Iteration 149, loss = 0.45596214\n",
      "Iteration 150, loss = 0.45480151\n",
      "Iteration 151, loss = 0.45353169\n",
      "Iteration 152, loss = 0.45222049\n",
      "Iteration 153, loss = 0.45072215\n",
      "Iteration 154, loss = 0.44916159\n",
      "Iteration 155, loss = 0.44755853\n",
      "Iteration 156, loss = 0.44597210\n",
      "Iteration 157, loss = 0.44452423\n",
      "Iteration 158, loss = 0.44310808\n",
      "Iteration 159, loss = 0.44198172\n",
      "Iteration 160, loss = 0.44065135\n",
      "Iteration 161, loss = 0.43943124\n",
      "Iteration 162, loss = 0.43859232\n",
      "Iteration 163, loss = 0.43788257\n",
      "Iteration 164, loss = 0.43734856\n",
      "Iteration 165, loss = 0.43670795\n",
      "Iteration 166, loss = 0.43586220\n",
      "Iteration 167, loss = 0.43490784\n",
      "Iteration 168, loss = 0.43371453\n",
      "Iteration 169, loss = 0.43230994\n",
      "Iteration 170, loss = 0.43100933\n",
      "Iteration 171, loss = 0.42991629\n",
      "Iteration 172, loss = 0.42884401\n",
      "Iteration 173, loss = 0.42776434\n",
      "Iteration 174, loss = 0.42662101\n",
      "Iteration 175, loss = 0.42528537\n",
      "Iteration 176, loss = 0.42433003\n",
      "Iteration 177, loss = 0.42344961\n",
      "Iteration 178, loss = 0.42254989\n",
      "Iteration 179, loss = 0.42168810\n",
      "Iteration 180, loss = 0.42078123\n",
      "Iteration 181, loss = 0.41993950\n",
      "Iteration 182, loss = 0.41908513\n",
      "Iteration 183, loss = 0.41840166\n",
      "Iteration 184, loss = 0.41765351\n",
      "Iteration 185, loss = 0.41683226\n",
      "Iteration 186, loss = 0.41601692\n",
      "Iteration 187, loss = 0.41522734\n",
      "Iteration 188, loss = 0.41443196\n",
      "Iteration 189, loss = 0.41351634\n",
      "Iteration 190, loss = 0.41263866\n",
      "Iteration 191, loss = 0.41192287\n",
      "Iteration 192, loss = 0.41135331\n",
      "Iteration 193, loss = 0.41074190\n",
      "Iteration 194, loss = 0.41004810\n",
      "Iteration 195, loss = 0.40921195\n",
      "Iteration 196, loss = 0.40839167\n",
      "Iteration 197, loss = 0.40767534\n",
      "Iteration 198, loss = 0.40712723\n",
      "Iteration 199, loss = 0.40649795\n",
      "Iteration 200, loss = 0.40580560\n",
      "Iteration 201, loss = 0.40496678\n",
      "Iteration 202, loss = 0.40406599\n",
      "Iteration 203, loss = 0.40311880\n",
      "Iteration 204, loss = 0.40212178\n",
      "Iteration 205, loss = 0.40113560\n",
      "Iteration 206, loss = 0.40018383\n",
      "Iteration 207, loss = 0.39936034\n",
      "Iteration 208, loss = 0.39854737\n",
      "Iteration 209, loss = 0.39765290\n",
      "Iteration 210, loss = 0.39679981\n",
      "Iteration 211, loss = 0.39603199\n",
      "Iteration 212, loss = 0.39559918\n",
      "Iteration 213, loss = 0.39516403\n",
      "Iteration 214, loss = 0.39449416\n",
      "Iteration 215, loss = 0.39377647\n",
      "Iteration 216, loss = 0.39290630\n",
      "Iteration 217, loss = 0.39197518\n",
      "Iteration 218, loss = 0.39100667\n",
      "Iteration 219, loss = 0.38999922\n",
      "Iteration 220, loss = 0.38908266\n",
      "Iteration 221, loss = 0.38816947\n",
      "Iteration 222, loss = 0.38723425\n",
      "Iteration 223, loss = 0.38645509\n",
      "Iteration 224, loss = 0.38588729\n",
      "Iteration 225, loss = 0.38519492\n",
      "Iteration 226, loss = 0.38461617\n",
      "Iteration 227, loss = 0.38394529\n",
      "Iteration 228, loss = 0.38321955\n",
      "Iteration 229, loss = 0.38257850\n",
      "Iteration 230, loss = 0.38205000\n",
      "Iteration 231, loss = 0.38147541\n",
      "Iteration 232, loss = 0.38092640\n",
      "Iteration 233, loss = 0.38043620\n",
      "Iteration 234, loss = 0.38005530\n",
      "Iteration 235, loss = 0.37970198\n",
      "Iteration 236, loss = 0.37928055\n",
      "Iteration 237, loss = 0.37863246\n",
      "Iteration 238, loss = 0.37799956\n",
      "Iteration 239, loss = 0.37733945\n",
      "Iteration 240, loss = 0.37669884\n",
      "Iteration 241, loss = 0.37596409\n",
      "Iteration 242, loss = 0.37524731\n",
      "Iteration 243, loss = 0.37455147\n",
      "Iteration 244, loss = 0.37391627\n",
      "Iteration 245, loss = 0.37328919\n",
      "Iteration 246, loss = 0.37267543\n",
      "Iteration 247, loss = 0.37210907\n",
      "Iteration 248, loss = 0.37157848\n",
      "Iteration 249, loss = 0.37108584\n",
      "Iteration 250, loss = 0.37052944\n",
      "Iteration 251, loss = 0.37002939\n",
      "Iteration 252, loss = 0.36945900\n",
      "Iteration 253, loss = 0.36902305\n",
      "Iteration 254, loss = 0.36855973\n",
      "Iteration 255, loss = 0.36815099\n",
      "Iteration 256, loss = 0.36770397\n",
      "Iteration 257, loss = 0.36720260\n",
      "Iteration 258, loss = 0.36659325\n",
      "Iteration 259, loss = 0.36590479\n",
      "Iteration 260, loss = 0.36542983\n",
      "Iteration 261, loss = 0.36490969\n",
      "Iteration 262, loss = 0.36444572\n",
      "Iteration 263, loss = 0.36406814\n",
      "Iteration 264, loss = 0.36347323\n",
      "Iteration 265, loss = 0.36307985\n",
      "Iteration 266, loss = 0.36264577\n",
      "Iteration 267, loss = 0.36218361\n",
      "Iteration 268, loss = 0.36171673\n",
      "Iteration 269, loss = 0.36122763\n",
      "Iteration 270, loss = 0.36087080\n",
      "Iteration 271, loss = 0.36057315\n",
      "Iteration 272, loss = 0.36045735\n",
      "Iteration 273, loss = 0.36024348\n",
      "Iteration 274, loss = 0.35991661\n",
      "Iteration 275, loss = 0.35984479\n",
      "Iteration 276, loss = 0.35975607\n",
      "Iteration 277, loss = 0.35954493\n",
      "Iteration 278, loss = 0.35941772\n",
      "Iteration 279, loss = 0.35938613\n",
      "Iteration 280, loss = 0.35919975\n",
      "Iteration 281, loss = 0.35876026\n",
      "Iteration 282, loss = 0.35799715\n",
      "Iteration 283, loss = 0.35695434\n",
      "Iteration 284, loss = 0.35612655\n",
      "Iteration 285, loss = 0.35535537\n",
      "Iteration 286, loss = 0.35459066\n",
      "Iteration 287, loss = 0.35380838\n",
      "Iteration 288, loss = 0.35313253\n",
      "Iteration 289, loss = 0.35264289\n",
      "Iteration 290, loss = 0.35216790\n",
      "Iteration 291, loss = 0.35172223\n",
      "Iteration 292, loss = 0.35128442\n",
      "Iteration 293, loss = 0.35088221\n",
      "Iteration 294, loss = 0.35050369\n",
      "Iteration 295, loss = 0.35018253\n",
      "Iteration 296, loss = 0.34995003\n",
      "Iteration 297, loss = 0.34973425\n",
      "Iteration 298, loss = 0.34973251\n",
      "Iteration 299, loss = 0.35001070\n",
      "Iteration 300, loss = 0.34983451\n",
      "Iteration 301, loss = 0.34931132\n",
      "Iteration 302, loss = 0.34880360\n",
      "Iteration 303, loss = 0.34842829\n",
      "Iteration 304, loss = 0.34813765\n",
      "Iteration 305, loss = 0.34763694\n",
      "Iteration 306, loss = 0.34724675\n",
      "Iteration 307, loss = 0.34675967\n",
      "Iteration 308, loss = 0.34629420\n",
      "Iteration 309, loss = 0.34589514\n",
      "Iteration 310, loss = 0.34559568\n",
      "Iteration 311, loss = 0.34527035\n",
      "Iteration 312, loss = 0.34486070\n",
      "Iteration 313, loss = 0.34452653\n",
      "Iteration 314, loss = 0.34423641\n",
      "Iteration 315, loss = 0.34377333\n",
      "Iteration 316, loss = 0.34327028\n",
      "Iteration 317, loss = 0.34280358\n",
      "Iteration 318, loss = 0.34238627\n",
      "Iteration 319, loss = 0.34194603\n",
      "Iteration 320, loss = 0.34150387\n",
      "Iteration 321, loss = 0.34105155\n",
      "Iteration 322, loss = 0.34073647\n",
      "Iteration 323, loss = 0.34040502\n",
      "Iteration 324, loss = 0.34016971\n",
      "Iteration 325, loss = 0.34009034\n",
      "Iteration 326, loss = 0.34005314\n",
      "Iteration 327, loss = 0.33997980\n",
      "Iteration 328, loss = 0.34048029\n",
      "Iteration 329, loss = 0.34028136\n",
      "Iteration 330, loss = 0.34036729\n",
      "Iteration 331, loss = 0.34041979\n",
      "Iteration 332, loss = 0.34073828\n",
      "Iteration 333, loss = 0.34027599\n",
      "Iteration 334, loss = 0.33964851\n",
      "Iteration 335, loss = 0.33884765\n",
      "Iteration 336, loss = 0.33773395\n",
      "Iteration 337, loss = 0.33675873\n",
      "Iteration 338, loss = 0.33593576\n",
      "Iteration 339, loss = 0.33531274\n",
      "Iteration 340, loss = 0.33479155\n",
      "Iteration 341, loss = 0.33421438\n",
      "Iteration 342, loss = 0.33381082\n",
      "Iteration 343, loss = 0.33349972\n",
      "Iteration 344, loss = 0.33316946\n",
      "Iteration 345, loss = 0.33273936\n",
      "Iteration 346, loss = 0.33233698\n",
      "Iteration 347, loss = 0.33202272\n",
      "Iteration 348, loss = 0.33170207\n",
      "Iteration 349, loss = 0.33147404\n",
      "Iteration 350, loss = 0.33122157\n",
      "Iteration 351, loss = 0.33093586\n",
      "Iteration 352, loss = 0.33060651\n",
      "Iteration 353, loss = 0.33016096\n",
      "Iteration 354, loss = 0.32972806\n",
      "Iteration 355, loss = 0.32936275\n",
      "Iteration 356, loss = 0.32905643\n",
      "Iteration 357, loss = 0.32880533\n",
      "Iteration 358, loss = 0.32855898\n",
      "Iteration 359, loss = 0.32834137\n",
      "Iteration 360, loss = 0.32824531\n",
      "Iteration 361, loss = 0.32829178\n",
      "Iteration 362, loss = 0.32825364\n",
      "Iteration 363, loss = 0.32767786\n",
      "Iteration 364, loss = 0.32713656\n",
      "Iteration 365, loss = 0.32668334\n",
      "Iteration 366, loss = 0.32637172\n",
      "Iteration 367, loss = 0.32615730\n",
      "Iteration 368, loss = 0.32595632\n",
      "Iteration 369, loss = 0.32580384\n",
      "Iteration 370, loss = 0.32564541\n",
      "Iteration 371, loss = 0.32562559\n",
      "Iteration 372, loss = 0.32568454\n",
      "Iteration 373, loss = 0.32560245\n",
      "Iteration 374, loss = 0.32567705\n",
      "Iteration 375, loss = 0.32568883\n",
      "Iteration 376, loss = 0.32598291\n",
      "Iteration 377, loss = 0.32650962\n",
      "Iteration 378, loss = 0.32633916\n",
      "Iteration 379, loss = 0.32564130\n",
      "Iteration 380, loss = 0.32502774\n",
      "Iteration 381, loss = 0.32404676\n",
      "Iteration 382, loss = 0.32316027\n",
      "Iteration 383, loss = 0.32247391\n",
      "Iteration 384, loss = 0.32194080\n",
      "Iteration 385, loss = 0.32153067\n",
      "Iteration 386, loss = 0.32124162\n",
      "Iteration 387, loss = 0.32099859\n",
      "Iteration 388, loss = 0.32066804\n",
      "Iteration 389, loss = 0.32038455\n",
      "Iteration 390, loss = 0.32012853\n",
      "Iteration 391, loss = 0.32001058\n",
      "Iteration 392, loss = 0.31973198\n",
      "Iteration 393, loss = 0.31949244\n",
      "Iteration 394, loss = 0.31926127\n",
      "Iteration 395, loss = 0.31922181\n",
      "Iteration 396, loss = 0.31919820\n",
      "Iteration 397, loss = 0.31880207\n",
      "Iteration 398, loss = 0.31823144\n",
      "Iteration 399, loss = 0.31773016\n",
      "Iteration 400, loss = 0.31734539\n",
      "Iteration 401, loss = 0.31724564\n",
      "Iteration 402, loss = 0.31715894\n",
      "Iteration 403, loss = 0.31729438\n",
      "Iteration 404, loss = 0.31732050\n",
      "Iteration 405, loss = 0.31739572\n",
      "Iteration 406, loss = 0.31765249\n",
      "Iteration 407, loss = 0.31778604\n",
      "Iteration 408, loss = 0.31793108\n",
      "Iteration 409, loss = 0.31845725\n",
      "Iteration 410, loss = 0.31856642\n",
      "Iteration 411, loss = 0.31853287\n",
      "Iteration 412, loss = 0.31888554\n",
      "Iteration 413, loss = 0.31940572\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71998787\n",
      "Iteration 2, loss = 0.71944067\n",
      "Iteration 3, loss = 0.71875250\n",
      "Iteration 4, loss = 0.71779959\n",
      "Iteration 5, loss = 0.71664850\n",
      "Iteration 6, loss = 0.71527644\n",
      "Iteration 7, loss = 0.71377973\n",
      "Iteration 8, loss = 0.71208300\n",
      "Iteration 9, loss = 0.71021467\n",
      "Iteration 10, loss = 0.70825321\n",
      "Iteration 11, loss = 0.70644295\n",
      "Iteration 12, loss = 0.70454019\n",
      "Iteration 13, loss = 0.70253325\n",
      "Iteration 14, loss = 0.70054983\n",
      "Iteration 15, loss = 0.69852700\n",
      "Iteration 16, loss = 0.69643447\n",
      "Iteration 17, loss = 0.69415883\n",
      "Iteration 18, loss = 0.69213633\n",
      "Iteration 19, loss = 0.69015028\n",
      "Iteration 20, loss = 0.68811085\n",
      "Iteration 21, loss = 0.68600223\n",
      "Iteration 22, loss = 0.68390056\n",
      "Iteration 23, loss = 0.68177437\n",
      "Iteration 24, loss = 0.67957366\n",
      "Iteration 25, loss = 0.67730387\n",
      "Iteration 26, loss = 0.67517353\n",
      "Iteration 27, loss = 0.67303321\n",
      "Iteration 28, loss = 0.67097585\n",
      "Iteration 29, loss = 0.66881447\n",
      "Iteration 30, loss = 0.66668273\n",
      "Iteration 31, loss = 0.66440963\n",
      "Iteration 32, loss = 0.66202438\n",
      "Iteration 33, loss = 0.65975829\n",
      "Iteration 34, loss = 0.65747996\n",
      "Iteration 35, loss = 0.65523832\n",
      "Iteration 36, loss = 0.65310895\n",
      "Iteration 37, loss = 0.65086502\n",
      "Iteration 38, loss = 0.64874834\n",
      "Iteration 39, loss = 0.64675980\n",
      "Iteration 40, loss = 0.64484025\n",
      "Iteration 41, loss = 0.64296410\n",
      "Iteration 42, loss = 0.64100316\n",
      "Iteration 43, loss = 0.63902241\n",
      "Iteration 44, loss = 0.63712625\n",
      "Iteration 45, loss = 0.63504969\n",
      "Iteration 46, loss = 0.63299989\n",
      "Iteration 47, loss = 0.63099055\n",
      "Iteration 48, loss = 0.62896119\n",
      "Iteration 49, loss = 0.62687396\n",
      "Iteration 50, loss = 0.62484687\n",
      "Iteration 51, loss = 0.62267295\n",
      "Iteration 52, loss = 0.62049869\n",
      "Iteration 53, loss = 0.61813715\n",
      "Iteration 54, loss = 0.61587405\n",
      "Iteration 55, loss = 0.61371123\n",
      "Iteration 56, loss = 0.61166779\n",
      "Iteration 57, loss = 0.60957189\n",
      "Iteration 58, loss = 0.60732003\n",
      "Iteration 59, loss = 0.60525402\n",
      "Iteration 60, loss = 0.60331535\n",
      "Iteration 61, loss = 0.60146443\n",
      "Iteration 62, loss = 0.59968435\n",
      "Iteration 63, loss = 0.59784641\n",
      "Iteration 64, loss = 0.59595478\n",
      "Iteration 65, loss = 0.59399930\n",
      "Iteration 66, loss = 0.59202384\n",
      "Iteration 67, loss = 0.58997293\n",
      "Iteration 68, loss = 0.58811796\n",
      "Iteration 69, loss = 0.58635735\n",
      "Iteration 70, loss = 0.58459901\n",
      "Iteration 71, loss = 0.58263850\n",
      "Iteration 72, loss = 0.58066402\n",
      "Iteration 73, loss = 0.57847410\n",
      "Iteration 74, loss = 0.57619032\n",
      "Iteration 75, loss = 0.57408568\n",
      "Iteration 76, loss = 0.57209510\n",
      "Iteration 77, loss = 0.57028886\n",
      "Iteration 78, loss = 0.56856927\n",
      "Iteration 79, loss = 0.56686197\n",
      "Iteration 80, loss = 0.56501902\n",
      "Iteration 81, loss = 0.56306411\n",
      "Iteration 82, loss = 0.56132533\n",
      "Iteration 83, loss = 0.55923213\n",
      "Iteration 84, loss = 0.55702679\n",
      "Iteration 85, loss = 0.55480933\n",
      "Iteration 86, loss = 0.55290949\n",
      "Iteration 87, loss = 0.55110513\n",
      "Iteration 88, loss = 0.54926732\n",
      "Iteration 89, loss = 0.54737990\n",
      "Iteration 90, loss = 0.54563798\n",
      "Iteration 91, loss = 0.54358400\n",
      "Iteration 92, loss = 0.54170675\n",
      "Iteration 93, loss = 0.53974616\n",
      "Iteration 94, loss = 0.53781305\n",
      "Iteration 95, loss = 0.53598605\n",
      "Iteration 96, loss = 0.53382086\n",
      "Iteration 97, loss = 0.53172023\n",
      "Iteration 98, loss = 0.52941042\n",
      "Iteration 99, loss = 0.52703709\n",
      "Iteration 100, loss = 0.52492748\n",
      "Iteration 101, loss = 0.52263553\n",
      "Iteration 102, loss = 0.52037803\n",
      "Iteration 103, loss = 0.51844122\n",
      "Iteration 104, loss = 0.51679742\n",
      "Iteration 105, loss = 0.51489515\n",
      "Iteration 106, loss = 0.51319900\n",
      "Iteration 107, loss = 0.51159966\n",
      "Iteration 108, loss = 0.50972166\n",
      "Iteration 109, loss = 0.50818412\n",
      "Iteration 110, loss = 0.50682439\n",
      "Iteration 111, loss = 0.50588806\n",
      "Iteration 112, loss = 0.50463514\n",
      "Iteration 113, loss = 0.50319443\n",
      "Iteration 114, loss = 0.50211590\n",
      "Iteration 115, loss = 0.50060694\n",
      "Iteration 116, loss = 0.49909415\n",
      "Iteration 117, loss = 0.49784330\n",
      "Iteration 118, loss = 0.49640270\n",
      "Iteration 119, loss = 0.49475142\n",
      "Iteration 120, loss = 0.49300150\n",
      "Iteration 121, loss = 0.49099736\n",
      "Iteration 122, loss = 0.48891790\n",
      "Iteration 123, loss = 0.48697819\n",
      "Iteration 124, loss = 0.48484717\n",
      "Iteration 125, loss = 0.48277558\n",
      "Iteration 126, loss = 0.48076369\n",
      "Iteration 127, loss = 0.47870371\n",
      "Iteration 128, loss = 0.47644736\n",
      "Iteration 129, loss = 0.47433056\n",
      "Iteration 130, loss = 0.47219209\n",
      "Iteration 131, loss = 0.47014035\n",
      "Iteration 132, loss = 0.46834938\n",
      "Iteration 133, loss = 0.46655268\n",
      "Iteration 134, loss = 0.46498821\n",
      "Iteration 135, loss = 0.46364136\n",
      "Iteration 136, loss = 0.46232281\n",
      "Iteration 137, loss = 0.46083869\n",
      "Iteration 138, loss = 0.45964113\n",
      "Iteration 139, loss = 0.45814711\n",
      "Iteration 140, loss = 0.45650970\n",
      "Iteration 141, loss = 0.45491235\n",
      "Iteration 142, loss = 0.45319321\n",
      "Iteration 143, loss = 0.45172755\n",
      "Iteration 144, loss = 0.45027842\n",
      "Iteration 145, loss = 0.44873454\n",
      "Iteration 146, loss = 0.44717608\n",
      "Iteration 147, loss = 0.44573342\n",
      "Iteration 148, loss = 0.44456161\n",
      "Iteration 149, loss = 0.44373576\n",
      "Iteration 150, loss = 0.44267158\n",
      "Iteration 151, loss = 0.44150078\n",
      "Iteration 152, loss = 0.44031213\n",
      "Iteration 153, loss = 0.43893527\n",
      "Iteration 154, loss = 0.43739109\n",
      "Iteration 155, loss = 0.43576946\n",
      "Iteration 156, loss = 0.43413738\n",
      "Iteration 157, loss = 0.43262355\n",
      "Iteration 158, loss = 0.43118680\n",
      "Iteration 159, loss = 0.43004591\n",
      "Iteration 160, loss = 0.42880474\n",
      "Iteration 161, loss = 0.42762972\n",
      "Iteration 162, loss = 0.42674001\n",
      "Iteration 163, loss = 0.42592584\n",
      "Iteration 164, loss = 0.42530226\n",
      "Iteration 165, loss = 0.42456874\n",
      "Iteration 166, loss = 0.42390084\n",
      "Iteration 167, loss = 0.42313321\n",
      "Iteration 168, loss = 0.42213366\n",
      "Iteration 169, loss = 0.42093292\n",
      "Iteration 170, loss = 0.41982286\n",
      "Iteration 171, loss = 0.41891365\n",
      "Iteration 172, loss = 0.41801287\n",
      "Iteration 173, loss = 0.41706597\n",
      "Iteration 174, loss = 0.41605368\n",
      "Iteration 175, loss = 0.41485531\n",
      "Iteration 176, loss = 0.41405390\n",
      "Iteration 177, loss = 0.41327431\n",
      "Iteration 178, loss = 0.41246402\n",
      "Iteration 179, loss = 0.41165106\n",
      "Iteration 180, loss = 0.41079143\n",
      "Iteration 181, loss = 0.41004087\n",
      "Iteration 182, loss = 0.40923310\n",
      "Iteration 183, loss = 0.40855745\n",
      "Iteration 184, loss = 0.40780890\n",
      "Iteration 185, loss = 0.40699463\n",
      "Iteration 186, loss = 0.40617575\n",
      "Iteration 187, loss = 0.40534616\n",
      "Iteration 188, loss = 0.40449686\n",
      "Iteration 189, loss = 0.40357285\n",
      "Iteration 190, loss = 0.40266112\n",
      "Iteration 191, loss = 0.40185518\n",
      "Iteration 192, loss = 0.40124353\n",
      "Iteration 193, loss = 0.40061145\n",
      "Iteration 194, loss = 0.39996795\n",
      "Iteration 195, loss = 0.39918548\n",
      "Iteration 196, loss = 0.39843259\n",
      "Iteration 197, loss = 0.39775507\n",
      "Iteration 198, loss = 0.39725578\n",
      "Iteration 199, loss = 0.39667620\n",
      "Iteration 200, loss = 0.39600654\n",
      "Iteration 201, loss = 0.39520473\n",
      "Iteration 202, loss = 0.39437413\n",
      "Iteration 203, loss = 0.39349096\n",
      "Iteration 204, loss = 0.39256180\n",
      "Iteration 205, loss = 0.39170208\n",
      "Iteration 206, loss = 0.39086545\n",
      "Iteration 207, loss = 0.39014926\n",
      "Iteration 208, loss = 0.38944236\n",
      "Iteration 209, loss = 0.38868123\n",
      "Iteration 210, loss = 0.38806213\n",
      "Iteration 211, loss = 0.38731448\n",
      "Iteration 212, loss = 0.38669757\n",
      "Iteration 213, loss = 0.38606527\n",
      "Iteration 214, loss = 0.38538747\n",
      "Iteration 215, loss = 0.38471231\n",
      "Iteration 216, loss = 0.38397759\n",
      "Iteration 217, loss = 0.38327028\n",
      "Iteration 218, loss = 0.38252413\n",
      "Iteration 219, loss = 0.38174326\n",
      "Iteration 220, loss = 0.38096624\n",
      "Iteration 221, loss = 0.38014076\n",
      "Iteration 222, loss = 0.37932026\n",
      "Iteration 223, loss = 0.37856609\n",
      "Iteration 224, loss = 0.37806798\n",
      "Iteration 225, loss = 0.37743474\n",
      "Iteration 226, loss = 0.37699136\n",
      "Iteration 227, loss = 0.37647883\n",
      "Iteration 228, loss = 0.37585002\n",
      "Iteration 229, loss = 0.37523479\n",
      "Iteration 230, loss = 0.37490456\n",
      "Iteration 231, loss = 0.37452778\n",
      "Iteration 232, loss = 0.37398616\n",
      "Iteration 233, loss = 0.37351068\n",
      "Iteration 234, loss = 0.37314961\n",
      "Iteration 235, loss = 0.37279018\n",
      "Iteration 236, loss = 0.37233244\n",
      "Iteration 237, loss = 0.37174820\n",
      "Iteration 238, loss = 0.37125824\n",
      "Iteration 239, loss = 0.37070087\n",
      "Iteration 240, loss = 0.37024598\n",
      "Iteration 241, loss = 0.36962435\n",
      "Iteration 242, loss = 0.36901643\n",
      "Iteration 243, loss = 0.36839273\n",
      "Iteration 244, loss = 0.36783048\n",
      "Iteration 245, loss = 0.36726869\n",
      "Iteration 246, loss = 0.36669662\n",
      "Iteration 247, loss = 0.36621828\n",
      "Iteration 248, loss = 0.36584945\n",
      "Iteration 249, loss = 0.36540547\n",
      "Iteration 250, loss = 0.36488337\n",
      "Iteration 251, loss = 0.36440558\n",
      "Iteration 252, loss = 0.36385250\n",
      "Iteration 253, loss = 0.36344703\n",
      "Iteration 254, loss = 0.36301210\n",
      "Iteration 255, loss = 0.36266010\n",
      "Iteration 256, loss = 0.36229418\n",
      "Iteration 257, loss = 0.36178047\n",
      "Iteration 258, loss = 0.36121764\n",
      "Iteration 259, loss = 0.36062454\n",
      "Iteration 260, loss = 0.36012546\n",
      "Iteration 261, loss = 0.35961835\n",
      "Iteration 262, loss = 0.35914341\n",
      "Iteration 263, loss = 0.35870466\n",
      "Iteration 264, loss = 0.35824607\n",
      "Iteration 265, loss = 0.35790221\n",
      "Iteration 266, loss = 0.35751971\n",
      "Iteration 267, loss = 0.35711658\n",
      "Iteration 268, loss = 0.35669584\n",
      "Iteration 269, loss = 0.35625979\n",
      "Iteration 270, loss = 0.35594483\n",
      "Iteration 271, loss = 0.35566679\n",
      "Iteration 272, loss = 0.35559104\n",
      "Iteration 273, loss = 0.35542694\n",
      "Iteration 274, loss = 0.35514152\n",
      "Iteration 275, loss = 0.35509684\n",
      "Iteration 276, loss = 0.35484936\n",
      "Iteration 277, loss = 0.35454582\n",
      "Iteration 278, loss = 0.35434866\n",
      "Iteration 279, loss = 0.35423407\n",
      "Iteration 280, loss = 0.35400822\n",
      "Iteration 281, loss = 0.35362755\n",
      "Iteration 282, loss = 0.35318165\n",
      "Iteration 283, loss = 0.35246236\n",
      "Iteration 284, loss = 0.35180704\n",
      "Iteration 285, loss = 0.35115552\n",
      "Iteration 286, loss = 0.35057150\n",
      "Iteration 287, loss = 0.35001686\n",
      "Iteration 288, loss = 0.34970785\n",
      "Iteration 289, loss = 0.34926462\n",
      "Iteration 290, loss = 0.34881881\n",
      "Iteration 291, loss = 0.34839283\n",
      "Iteration 292, loss = 0.34796465\n",
      "Iteration 293, loss = 0.34754388\n",
      "Iteration 294, loss = 0.34730342\n",
      "Iteration 295, loss = 0.34702505\n",
      "Iteration 296, loss = 0.34676667\n",
      "Iteration 297, loss = 0.34650143\n",
      "Iteration 298, loss = 0.34629365\n",
      "Iteration 299, loss = 0.34628321\n",
      "Iteration 300, loss = 0.34598748\n",
      "Iteration 301, loss = 0.34553830\n",
      "Iteration 302, loss = 0.34513193\n",
      "Iteration 303, loss = 0.34475826\n",
      "Iteration 304, loss = 0.34440528\n",
      "Iteration 305, loss = 0.34408903\n",
      "Iteration 306, loss = 0.34380061\n",
      "Iteration 307, loss = 0.34354061\n",
      "Iteration 308, loss = 0.34329519\n",
      "Iteration 309, loss = 0.34300752\n",
      "Iteration 310, loss = 0.34269276\n",
      "Iteration 311, loss = 0.34237866\n",
      "Iteration 312, loss = 0.34210482\n",
      "Iteration 313, loss = 0.34181923\n",
      "Iteration 314, loss = 0.34156411\n",
      "Iteration 315, loss = 0.34120383\n",
      "Iteration 316, loss = 0.34087421\n",
      "Iteration 317, loss = 0.34051625\n",
      "Iteration 318, loss = 0.34013725\n",
      "Iteration 319, loss = 0.33980266\n",
      "Iteration 320, loss = 0.33944715\n",
      "Iteration 321, loss = 0.33910857\n",
      "Iteration 322, loss = 0.33887819\n",
      "Iteration 323, loss = 0.33864428\n",
      "Iteration 324, loss = 0.33838749\n",
      "Iteration 325, loss = 0.33822089\n",
      "Iteration 326, loss = 0.33809939\n",
      "Iteration 327, loss = 0.33795806\n",
      "Iteration 328, loss = 0.33809013\n",
      "Iteration 329, loss = 0.33772744\n",
      "Iteration 330, loss = 0.33766120\n",
      "Iteration 331, loss = 0.33760738\n",
      "Iteration 332, loss = 0.33785877\n",
      "Iteration 333, loss = 0.33747631\n",
      "Iteration 334, loss = 0.33698854\n",
      "Iteration 335, loss = 0.33633482\n",
      "Iteration 336, loss = 0.33552886\n",
      "Iteration 337, loss = 0.33484487\n",
      "Iteration 338, loss = 0.33423716\n",
      "Iteration 339, loss = 0.33379994\n",
      "Iteration 340, loss = 0.33335892\n",
      "Iteration 341, loss = 0.33295668\n",
      "Iteration 342, loss = 0.33266797\n",
      "Iteration 343, loss = 0.33234899\n",
      "Iteration 344, loss = 0.33212186\n",
      "Iteration 345, loss = 0.33181739\n",
      "Iteration 346, loss = 0.33149297\n",
      "Iteration 347, loss = 0.33126057\n",
      "Iteration 348, loss = 0.33099871\n",
      "Iteration 349, loss = 0.33074764\n",
      "Iteration 350, loss = 0.33044494\n",
      "Iteration 351, loss = 0.33019686\n",
      "Iteration 352, loss = 0.32996217\n",
      "Iteration 353, loss = 0.32979062\n",
      "Iteration 354, loss = 0.32951340\n",
      "Iteration 355, loss = 0.32934095\n",
      "Iteration 356, loss = 0.32917677\n",
      "Iteration 357, loss = 0.32901295\n",
      "Iteration 358, loss = 0.32883195\n",
      "Iteration 359, loss = 0.32864994\n",
      "Iteration 360, loss = 0.32857447\n",
      "Iteration 361, loss = 0.32863442\n",
      "Iteration 362, loss = 0.32853872\n",
      "Iteration 363, loss = 0.32840636\n",
      "Iteration 364, loss = 0.32803029\n",
      "Iteration 365, loss = 0.32771515\n",
      "Iteration 366, loss = 0.32745956\n",
      "Iteration 367, loss = 0.32721741\n",
      "Iteration 368, loss = 0.32689700\n",
      "Iteration 369, loss = 0.32663672\n",
      "Iteration 370, loss = 0.32636164\n",
      "Iteration 371, loss = 0.32626511\n",
      "Iteration 372, loss = 0.32620302\n",
      "Iteration 373, loss = 0.32602194\n",
      "Iteration 374, loss = 0.32592880\n",
      "Iteration 375, loss = 0.32581652\n",
      "Iteration 376, loss = 0.32598271\n",
      "Iteration 377, loss = 0.32643948\n",
      "Iteration 378, loss = 0.32618463\n",
      "Iteration 379, loss = 0.32543763\n",
      "Iteration 380, loss = 0.32477953\n",
      "Iteration 381, loss = 0.32390627\n",
      "Iteration 382, loss = 0.32312262\n",
      "Iteration 383, loss = 0.32252493\n",
      "Iteration 384, loss = 0.32204048\n",
      "Iteration 385, loss = 0.32164549\n",
      "Iteration 386, loss = 0.32131494\n",
      "Iteration 387, loss = 0.32101677\n",
      "Iteration 388, loss = 0.32076049\n",
      "Iteration 389, loss = 0.32053138\n",
      "Iteration 390, loss = 0.32034336\n",
      "Iteration 391, loss = 0.32015849\n",
      "Iteration 392, loss = 0.31986282\n",
      "Iteration 393, loss = 0.31962588\n",
      "Iteration 394, loss = 0.31938955\n",
      "Iteration 395, loss = 0.31927902\n",
      "Iteration 396, loss = 0.31920981\n",
      "Iteration 397, loss = 0.31901442\n",
      "Iteration 398, loss = 0.31868889\n",
      "Iteration 399, loss = 0.31849162\n",
      "Iteration 400, loss = 0.31824892\n",
      "Iteration 401, loss = 0.31845387\n",
      "Iteration 402, loss = 0.31861451\n",
      "Iteration 403, loss = 0.31898525\n",
      "Iteration 404, loss = 0.31907976\n",
      "Iteration 405, loss = 0.31928285\n",
      "Iteration 406, loss = 0.31965971\n",
      "Iteration 407, loss = 0.31989657\n",
      "Iteration 408, loss = 0.32010962\n",
      "Iteration 409, loss = 0.32071236\n",
      "Iteration 410, loss = 0.32090408\n",
      "Iteration 411, loss = 0.32091564\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71488268\n",
      "Iteration 2, loss = 0.71431260\n",
      "Iteration 3, loss = 0.71363257\n",
      "Iteration 4, loss = 0.71288667\n",
      "Iteration 5, loss = 0.71201190\n",
      "Iteration 6, loss = 0.71090076\n",
      "Iteration 7, loss = 0.70964329\n",
      "Iteration 8, loss = 0.70799878\n",
      "Iteration 9, loss = 0.70611730\n",
      "Iteration 10, loss = 0.70410229\n",
      "Iteration 11, loss = 0.70228646\n",
      "Iteration 12, loss = 0.70032056\n",
      "Iteration 13, loss = 0.69818891\n",
      "Iteration 14, loss = 0.69607120\n",
      "Iteration 15, loss = 0.69392390\n",
      "Iteration 16, loss = 0.69167992\n",
      "Iteration 17, loss = 0.68927076\n",
      "Iteration 18, loss = 0.68704340\n",
      "Iteration 19, loss = 0.68490238\n",
      "Iteration 20, loss = 0.68276548\n",
      "Iteration 21, loss = 0.68054823\n",
      "Iteration 22, loss = 0.67832180\n",
      "Iteration 23, loss = 0.67611078\n",
      "Iteration 24, loss = 0.67385957\n",
      "Iteration 25, loss = 0.67156834\n",
      "Iteration 26, loss = 0.66934995\n",
      "Iteration 27, loss = 0.66711658\n",
      "Iteration 28, loss = 0.66485377\n",
      "Iteration 29, loss = 0.66255475\n",
      "Iteration 30, loss = 0.66038082\n",
      "Iteration 31, loss = 0.65812421\n",
      "Iteration 32, loss = 0.65579553\n",
      "Iteration 33, loss = 0.65351893\n",
      "Iteration 34, loss = 0.65130217\n",
      "Iteration 35, loss = 0.64926188\n",
      "Iteration 36, loss = 0.64736703\n",
      "Iteration 37, loss = 0.64516039\n",
      "Iteration 38, loss = 0.64288503\n",
      "Iteration 39, loss = 0.64063328\n",
      "Iteration 40, loss = 0.63850639\n",
      "Iteration 41, loss = 0.63647386\n",
      "Iteration 42, loss = 0.63442066\n",
      "Iteration 43, loss = 0.63229077\n",
      "Iteration 44, loss = 0.63022950\n",
      "Iteration 45, loss = 0.62807967\n",
      "Iteration 46, loss = 0.62591172\n",
      "Iteration 47, loss = 0.62374695\n",
      "Iteration 48, loss = 0.62155017\n",
      "Iteration 49, loss = 0.61933768\n",
      "Iteration 50, loss = 0.61704150\n",
      "Iteration 51, loss = 0.61475754\n",
      "Iteration 52, loss = 0.61251942\n",
      "Iteration 53, loss = 0.61021521\n",
      "Iteration 54, loss = 0.60798575\n",
      "Iteration 55, loss = 0.60585831\n",
      "Iteration 56, loss = 0.60386600\n",
      "Iteration 57, loss = 0.60185793\n",
      "Iteration 58, loss = 0.59967211\n",
      "Iteration 59, loss = 0.59763418\n",
      "Iteration 60, loss = 0.59568753\n",
      "Iteration 61, loss = 0.59379728\n",
      "Iteration 62, loss = 0.59187907\n",
      "Iteration 63, loss = 0.58992009\n",
      "Iteration 64, loss = 0.58790342\n",
      "Iteration 65, loss = 0.58577599\n",
      "Iteration 66, loss = 0.58367792\n",
      "Iteration 67, loss = 0.58150020\n",
      "Iteration 68, loss = 0.57952154\n",
      "Iteration 69, loss = 0.57762765\n",
      "Iteration 70, loss = 0.57581753\n",
      "Iteration 71, loss = 0.57380996\n",
      "Iteration 72, loss = 0.57181272\n",
      "Iteration 73, loss = 0.56958352\n",
      "Iteration 74, loss = 0.56722217\n",
      "Iteration 75, loss = 0.56498835\n",
      "Iteration 76, loss = 0.56295383\n",
      "Iteration 77, loss = 0.56112545\n",
      "Iteration 78, loss = 0.55936278\n",
      "Iteration 79, loss = 0.55754967\n",
      "Iteration 80, loss = 0.55563622\n",
      "Iteration 81, loss = 0.55352820\n",
      "Iteration 82, loss = 0.55158944\n",
      "Iteration 83, loss = 0.54939629\n",
      "Iteration 84, loss = 0.54718189\n",
      "Iteration 85, loss = 0.54497943\n",
      "Iteration 86, loss = 0.54305985\n",
      "Iteration 87, loss = 0.54123823\n",
      "Iteration 88, loss = 0.53935700\n",
      "Iteration 89, loss = 0.53748477\n",
      "Iteration 90, loss = 0.53565196\n",
      "Iteration 91, loss = 0.53357771\n",
      "Iteration 92, loss = 0.53148467\n",
      "Iteration 93, loss = 0.52933464\n",
      "Iteration 94, loss = 0.52723657\n",
      "Iteration 95, loss = 0.52513725\n",
      "Iteration 96, loss = 0.52293429\n",
      "Iteration 97, loss = 0.52073658\n",
      "Iteration 98, loss = 0.51855243\n",
      "Iteration 99, loss = 0.51645620\n",
      "Iteration 100, loss = 0.51456256\n",
      "Iteration 101, loss = 0.51265449\n",
      "Iteration 102, loss = 0.51067808\n",
      "Iteration 103, loss = 0.50871920\n",
      "Iteration 104, loss = 0.50682177\n",
      "Iteration 105, loss = 0.50481474\n",
      "Iteration 106, loss = 0.50299654\n",
      "Iteration 107, loss = 0.50133313\n",
      "Iteration 108, loss = 0.49963506\n",
      "Iteration 109, loss = 0.49825119\n",
      "Iteration 110, loss = 0.49700194\n",
      "Iteration 111, loss = 0.49583676\n",
      "Iteration 112, loss = 0.49458739\n",
      "Iteration 113, loss = 0.49317842\n",
      "Iteration 114, loss = 0.49197347\n",
      "Iteration 115, loss = 0.49052699\n",
      "Iteration 116, loss = 0.48897332\n",
      "Iteration 117, loss = 0.48750177\n",
      "Iteration 118, loss = 0.48596221\n",
      "Iteration 119, loss = 0.48442935\n",
      "Iteration 120, loss = 0.48281036\n",
      "Iteration 121, loss = 0.48116215\n",
      "Iteration 122, loss = 0.47943972\n",
      "Iteration 123, loss = 0.47775946\n",
      "Iteration 124, loss = 0.47596584\n",
      "Iteration 125, loss = 0.47415091\n",
      "Iteration 126, loss = 0.47242638\n",
      "Iteration 127, loss = 0.47058923\n",
      "Iteration 128, loss = 0.46868847\n",
      "Iteration 129, loss = 0.46698130\n",
      "Iteration 130, loss = 0.46525060\n",
      "Iteration 131, loss = 0.46350196\n",
      "Iteration 132, loss = 0.46178446\n",
      "Iteration 133, loss = 0.46006263\n",
      "Iteration 134, loss = 0.45851186\n",
      "Iteration 135, loss = 0.45710939\n",
      "Iteration 136, loss = 0.45575718\n",
      "Iteration 137, loss = 0.45428345\n",
      "Iteration 138, loss = 0.45314472\n",
      "Iteration 139, loss = 0.45175353\n",
      "Iteration 140, loss = 0.45024694\n",
      "Iteration 141, loss = 0.44869004\n",
      "Iteration 142, loss = 0.44705359\n",
      "Iteration 143, loss = 0.44560928\n",
      "Iteration 144, loss = 0.44427382\n",
      "Iteration 145, loss = 0.44284854\n",
      "Iteration 146, loss = 0.44141506\n",
      "Iteration 147, loss = 0.44010306\n",
      "Iteration 148, loss = 0.43914194\n",
      "Iteration 149, loss = 0.43839694\n",
      "Iteration 150, loss = 0.43744993\n",
      "Iteration 151, loss = 0.43643412\n",
      "Iteration 152, loss = 0.43539307\n",
      "Iteration 153, loss = 0.43411439\n",
      "Iteration 154, loss = 0.43281333\n",
      "Iteration 155, loss = 0.43141829\n",
      "Iteration 156, loss = 0.42998831\n",
      "Iteration 157, loss = 0.42857487\n",
      "Iteration 158, loss = 0.42734621\n",
      "Iteration 159, loss = 0.42638397\n",
      "Iteration 160, loss = 0.42528509\n",
      "Iteration 161, loss = 0.42426450\n",
      "Iteration 162, loss = 0.42357150\n",
      "Iteration 163, loss = 0.42297192\n",
      "Iteration 164, loss = 0.42224495\n",
      "Iteration 165, loss = 0.42133597\n",
      "Iteration 166, loss = 0.42051914\n",
      "Iteration 167, loss = 0.41958741\n",
      "Iteration 168, loss = 0.41861378\n",
      "Iteration 169, loss = 0.41755645\n",
      "Iteration 170, loss = 0.41662705\n",
      "Iteration 171, loss = 0.41572897\n",
      "Iteration 172, loss = 0.41500331\n",
      "Iteration 173, loss = 0.41420901\n",
      "Iteration 174, loss = 0.41336560\n",
      "Iteration 175, loss = 0.41258225\n",
      "Iteration 176, loss = 0.41236140\n",
      "Iteration 177, loss = 0.41183449\n",
      "Iteration 178, loss = 0.41109835\n",
      "Iteration 179, loss = 0.41020888\n",
      "Iteration 180, loss = 0.40921305\n",
      "Iteration 181, loss = 0.40838463\n",
      "Iteration 182, loss = 0.40744065\n",
      "Iteration 183, loss = 0.40663146\n",
      "Iteration 184, loss = 0.40578414\n",
      "Iteration 185, loss = 0.40488296\n",
      "Iteration 186, loss = 0.40396940\n",
      "Iteration 187, loss = 0.40301504\n",
      "Iteration 188, loss = 0.40207399\n",
      "Iteration 189, loss = 0.40111355\n",
      "Iteration 190, loss = 0.40016122\n",
      "Iteration 191, loss = 0.39925677\n",
      "Iteration 192, loss = 0.39829980\n",
      "Iteration 193, loss = 0.39736450\n",
      "Iteration 194, loss = 0.39642878\n",
      "Iteration 195, loss = 0.39550740\n",
      "Iteration 196, loss = 0.39465347\n",
      "Iteration 197, loss = 0.39383790\n",
      "Iteration 198, loss = 0.39312298\n",
      "Iteration 199, loss = 0.39236816\n",
      "Iteration 200, loss = 0.39157878\n",
      "Iteration 201, loss = 0.39093533\n",
      "Iteration 202, loss = 0.39036559\n",
      "Iteration 203, loss = 0.38977060\n",
      "Iteration 204, loss = 0.38910772\n",
      "Iteration 205, loss = 0.38853714\n",
      "Iteration 206, loss = 0.38799115\n",
      "Iteration 207, loss = 0.38746704\n",
      "Iteration 208, loss = 0.38691193\n",
      "Iteration 209, loss = 0.38636642\n",
      "Iteration 210, loss = 0.38618822\n",
      "Iteration 211, loss = 0.38567439\n",
      "Iteration 212, loss = 0.38516197\n",
      "Iteration 213, loss = 0.38460484\n",
      "Iteration 214, loss = 0.38401056\n",
      "Iteration 215, loss = 0.38340310\n",
      "Iteration 216, loss = 0.38274648\n",
      "Iteration 217, loss = 0.38208579\n",
      "Iteration 218, loss = 0.38136522\n",
      "Iteration 219, loss = 0.38061321\n",
      "Iteration 220, loss = 0.37987761\n",
      "Iteration 221, loss = 0.37909075\n",
      "Iteration 222, loss = 0.37832639\n",
      "Iteration 223, loss = 0.37761733\n",
      "Iteration 224, loss = 0.37708625\n",
      "Iteration 225, loss = 0.37643280\n",
      "Iteration 226, loss = 0.37598614\n",
      "Iteration 227, loss = 0.37559106\n",
      "Iteration 228, loss = 0.37504088\n",
      "Iteration 229, loss = 0.37443938\n",
      "Iteration 230, loss = 0.37413694\n",
      "Iteration 231, loss = 0.37400837\n",
      "Iteration 232, loss = 0.37356537\n",
      "Iteration 233, loss = 0.37324389\n",
      "Iteration 234, loss = 0.37300400\n",
      "Iteration 235, loss = 0.37271578\n",
      "Iteration 236, loss = 0.37231818\n",
      "Iteration 237, loss = 0.37174789\n",
      "Iteration 238, loss = 0.37120389\n",
      "Iteration 239, loss = 0.37075710\n",
      "Iteration 240, loss = 0.37081003\n",
      "Iteration 241, loss = 0.37043255\n",
      "Iteration 242, loss = 0.36986490\n",
      "Iteration 243, loss = 0.36922669\n",
      "Iteration 244, loss = 0.36867219\n",
      "Iteration 245, loss = 0.36806361\n",
      "Iteration 246, loss = 0.36739961\n",
      "Iteration 247, loss = 0.36680394\n",
      "Iteration 248, loss = 0.36640338\n",
      "Iteration 249, loss = 0.36581827\n",
      "Iteration 250, loss = 0.36510492\n",
      "Iteration 251, loss = 0.36449085\n",
      "Iteration 252, loss = 0.36377553\n",
      "Iteration 253, loss = 0.36326661\n",
      "Iteration 254, loss = 0.36279417\n",
      "Iteration 255, loss = 0.36237246\n",
      "Iteration 256, loss = 0.36196126\n",
      "Iteration 257, loss = 0.36136289\n",
      "Iteration 258, loss = 0.36066633\n",
      "Iteration 259, loss = 0.35990607\n",
      "Iteration 260, loss = 0.35925842\n",
      "Iteration 261, loss = 0.35870066\n",
      "Iteration 262, loss = 0.35820883\n",
      "Iteration 263, loss = 0.35777517\n",
      "Iteration 264, loss = 0.35730729\n",
      "Iteration 265, loss = 0.35696012\n",
      "Iteration 266, loss = 0.35657850\n",
      "Iteration 267, loss = 0.35617184\n",
      "Iteration 268, loss = 0.35577417\n",
      "Iteration 269, loss = 0.35535086\n",
      "Iteration 270, loss = 0.35494226\n",
      "Iteration 271, loss = 0.35441618\n",
      "Iteration 272, loss = 0.35394614\n",
      "Iteration 273, loss = 0.35348448\n",
      "Iteration 274, loss = 0.35312009\n",
      "Iteration 275, loss = 0.35272697\n",
      "Iteration 276, loss = 0.35238777\n",
      "Iteration 277, loss = 0.35204578\n",
      "Iteration 278, loss = 0.35168730\n",
      "Iteration 279, loss = 0.35136237\n",
      "Iteration 280, loss = 0.35108319\n",
      "Iteration 281, loss = 0.35080487\n",
      "Iteration 282, loss = 0.35056783\n",
      "Iteration 283, loss = 0.35001596\n",
      "Iteration 284, loss = 0.34962639\n",
      "Iteration 285, loss = 0.34929742\n",
      "Iteration 286, loss = 0.34887368\n",
      "Iteration 287, loss = 0.34846143\n",
      "Iteration 288, loss = 0.34809243\n",
      "Iteration 289, loss = 0.34771287\n",
      "Iteration 290, loss = 0.34732433\n",
      "Iteration 291, loss = 0.34697584\n",
      "Iteration 292, loss = 0.34664436\n",
      "Iteration 293, loss = 0.34630538\n",
      "Iteration 294, loss = 0.34607451\n",
      "Iteration 295, loss = 0.34581670\n",
      "Iteration 296, loss = 0.34549505\n",
      "Iteration 297, loss = 0.34520690\n",
      "Iteration 298, loss = 0.34493933\n",
      "Iteration 299, loss = 0.34468949\n",
      "Iteration 300, loss = 0.34439485\n",
      "Iteration 301, loss = 0.34405016\n",
      "Iteration 302, loss = 0.34375511\n",
      "Iteration 303, loss = 0.34347037\n",
      "Iteration 304, loss = 0.34317198\n",
      "Iteration 305, loss = 0.34292568\n",
      "Iteration 306, loss = 0.34277637\n",
      "Iteration 307, loss = 0.34274256\n",
      "Iteration 308, loss = 0.34272558\n",
      "Iteration 309, loss = 0.34252320\n",
      "Iteration 310, loss = 0.34214075\n",
      "Iteration 311, loss = 0.34171784\n",
      "Iteration 312, loss = 0.34131690\n",
      "Iteration 313, loss = 0.34081569\n",
      "Iteration 314, loss = 0.34038829\n",
      "Iteration 315, loss = 0.34000420\n",
      "Iteration 316, loss = 0.33964679\n",
      "Iteration 317, loss = 0.33931054\n",
      "Iteration 318, loss = 0.33893454\n",
      "Iteration 319, loss = 0.33862095\n",
      "Iteration 320, loss = 0.33819412\n",
      "Iteration 321, loss = 0.33780299\n",
      "Iteration 322, loss = 0.33743257\n",
      "Iteration 323, loss = 0.33709231\n",
      "Iteration 324, loss = 0.33676488\n",
      "Iteration 325, loss = 0.33648551\n",
      "Iteration 326, loss = 0.33630315\n",
      "Iteration 327, loss = 0.33610385\n",
      "Iteration 328, loss = 0.33627874\n",
      "Iteration 329, loss = 0.33603226\n",
      "Iteration 330, loss = 0.33603606\n",
      "Iteration 331, loss = 0.33614722\n",
      "Iteration 332, loss = 0.33648513\n",
      "Iteration 333, loss = 0.33629332\n",
      "Iteration 334, loss = 0.33601349\n",
      "Iteration 335, loss = 0.33552164\n",
      "Iteration 336, loss = 0.33480140\n",
      "Iteration 337, loss = 0.33419367\n",
      "Iteration 338, loss = 0.33365665\n",
      "Iteration 339, loss = 0.33323921\n",
      "Iteration 340, loss = 0.33287489\n",
      "Iteration 341, loss = 0.33244014\n",
      "Iteration 342, loss = 0.33214282\n",
      "Iteration 343, loss = 0.33197274\n",
      "Iteration 344, loss = 0.33187976\n",
      "Iteration 345, loss = 0.33168151\n",
      "Iteration 346, loss = 0.33133878\n",
      "Iteration 347, loss = 0.33111312\n",
      "Iteration 348, loss = 0.33091069\n",
      "Iteration 349, loss = 0.33068626\n",
      "Iteration 350, loss = 0.33042145\n",
      "Iteration 351, loss = 0.33017843\n",
      "Iteration 352, loss = 0.32997112\n",
      "Iteration 353, loss = 0.32977795\n",
      "Iteration 354, loss = 0.32949685\n",
      "Iteration 355, loss = 0.32927046\n",
      "Iteration 356, loss = 0.32907834\n",
      "Iteration 357, loss = 0.32890482\n",
      "Iteration 358, loss = 0.32870298\n",
      "Iteration 359, loss = 0.32851274\n",
      "Iteration 360, loss = 0.32841100\n",
      "Iteration 361, loss = 0.32841462\n",
      "Iteration 362, loss = 0.32840834\n",
      "Iteration 363, loss = 0.32836501\n",
      "Iteration 364, loss = 0.32801472\n",
      "Iteration 365, loss = 0.32777112\n",
      "Iteration 366, loss = 0.32757517\n",
      "Iteration 367, loss = 0.32742691\n",
      "Iteration 368, loss = 0.32723535\n",
      "Iteration 369, loss = 0.32702784\n",
      "Iteration 370, loss = 0.32678942\n",
      "Iteration 371, loss = 0.32674980\n",
      "Iteration 372, loss = 0.32632077\n",
      "Iteration 373, loss = 0.32587664\n",
      "Iteration 374, loss = 0.32557822\n",
      "Iteration 375, loss = 0.32521428\n",
      "Iteration 376, loss = 0.32511915\n",
      "Iteration 377, loss = 0.32554844\n",
      "Iteration 378, loss = 0.32532586\n",
      "Iteration 379, loss = 0.32459620\n",
      "Iteration 380, loss = 0.32398052\n",
      "Iteration 381, loss = 0.32319303\n",
      "Iteration 382, loss = 0.32246208\n",
      "Iteration 383, loss = 0.32188197\n",
      "Iteration 384, loss = 0.32145161\n",
      "Iteration 385, loss = 0.32112831\n",
      "Iteration 386, loss = 0.32088322\n",
      "Iteration 387, loss = 0.32067862\n",
      "Iteration 388, loss = 0.32048253\n",
      "Iteration 389, loss = 0.32029567\n",
      "Iteration 390, loss = 0.32011730\n",
      "Iteration 391, loss = 0.31998329\n",
      "Iteration 392, loss = 0.31972893\n",
      "Iteration 393, loss = 0.31953431\n",
      "Iteration 394, loss = 0.31935697\n",
      "Iteration 395, loss = 0.31934077\n",
      "Iteration 396, loss = 0.31939214\n",
      "Iteration 397, loss = 0.31921976\n",
      "Iteration 398, loss = 0.31884590\n",
      "Iteration 399, loss = 0.31842850\n",
      "Iteration 400, loss = 0.31806076\n",
      "Iteration 401, loss = 0.31795532\n",
      "Iteration 402, loss = 0.31795023\n",
      "Iteration 403, loss = 0.31822223\n",
      "Iteration 404, loss = 0.31840256\n",
      "Iteration 405, loss = 0.31856501\n",
      "Iteration 406, loss = 0.31890457\n",
      "Iteration 407, loss = 0.31916057\n",
      "Iteration 408, loss = 0.31947862\n",
      "Iteration 409, loss = 0.32008668\n",
      "Iteration 410, loss = 0.32018235\n",
      "Iteration 411, loss = 0.31997946\n",
      "Iteration 412, loss = 0.32023213\n",
      "Iteration 413, loss = 0.32066918\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71895782\n",
      "Iteration 2, loss = 0.71843448\n",
      "Iteration 3, loss = 0.71764290\n",
      "Iteration 4, loss = 0.71684582\n",
      "Iteration 5, loss = 0.71586087\n",
      "Iteration 6, loss = 0.71461987\n",
      "Iteration 7, loss = 0.71295440\n",
      "Iteration 8, loss = 0.71091272\n",
      "Iteration 9, loss = 0.70874266\n",
      "Iteration 10, loss = 0.70655707\n",
      "Iteration 11, loss = 0.70475779\n",
      "Iteration 12, loss = 0.70281001\n",
      "Iteration 13, loss = 0.70037593\n",
      "Iteration 14, loss = 0.69797683\n",
      "Iteration 15, loss = 0.69561864\n",
      "Iteration 16, loss = 0.69323561\n",
      "Iteration 17, loss = 0.69065671\n",
      "Iteration 18, loss = 0.68815173\n",
      "Iteration 19, loss = 0.68576133\n",
      "Iteration 20, loss = 0.68342700\n",
      "Iteration 21, loss = 0.68106644\n",
      "Iteration 22, loss = 0.67871542\n",
      "Iteration 23, loss = 0.67633845\n",
      "Iteration 24, loss = 0.67386530\n",
      "Iteration 25, loss = 0.67128417\n",
      "Iteration 26, loss = 0.66883253\n",
      "Iteration 27, loss = 0.66642667\n",
      "Iteration 28, loss = 0.66412227\n",
      "Iteration 29, loss = 0.66176402\n",
      "Iteration 30, loss = 0.65949706\n",
      "Iteration 31, loss = 0.65715532\n",
      "Iteration 32, loss = 0.65474989\n",
      "Iteration 33, loss = 0.65245624\n",
      "Iteration 34, loss = 0.65009420\n",
      "Iteration 35, loss = 0.64785366\n",
      "Iteration 36, loss = 0.64576989\n",
      "Iteration 37, loss = 0.64352659\n",
      "Iteration 38, loss = 0.64128659\n",
      "Iteration 39, loss = 0.63908966\n",
      "Iteration 40, loss = 0.63704948\n",
      "Iteration 41, loss = 0.63509460\n",
      "Iteration 42, loss = 0.63307069\n",
      "Iteration 43, loss = 0.63102504\n",
      "Iteration 44, loss = 0.62907960\n",
      "Iteration 45, loss = 0.62699146\n",
      "Iteration 46, loss = 0.62491492\n",
      "Iteration 47, loss = 0.62291672\n",
      "Iteration 48, loss = 0.62091094\n",
      "Iteration 49, loss = 0.61885465\n",
      "Iteration 50, loss = 0.61676814\n",
      "Iteration 51, loss = 0.61471981\n",
      "Iteration 52, loss = 0.61260626\n",
      "Iteration 53, loss = 0.61036542\n",
      "Iteration 54, loss = 0.60810492\n",
      "Iteration 55, loss = 0.60594924\n",
      "Iteration 56, loss = 0.60390185\n",
      "Iteration 57, loss = 0.60171172\n",
      "Iteration 58, loss = 0.59936661\n",
      "Iteration 59, loss = 0.59717018\n",
      "Iteration 60, loss = 0.59510506\n",
      "Iteration 61, loss = 0.59314006\n",
      "Iteration 62, loss = 0.59119230\n",
      "Iteration 63, loss = 0.58929916\n",
      "Iteration 64, loss = 0.58738419\n",
      "Iteration 65, loss = 0.58531987\n",
      "Iteration 66, loss = 0.58328914\n",
      "Iteration 67, loss = 0.58121400\n",
      "Iteration 68, loss = 0.57937396\n",
      "Iteration 69, loss = 0.57761507\n",
      "Iteration 70, loss = 0.57595911\n",
      "Iteration 71, loss = 0.57403834\n",
      "Iteration 72, loss = 0.57215240\n",
      "Iteration 73, loss = 0.56997369\n",
      "Iteration 74, loss = 0.56762306\n",
      "Iteration 75, loss = 0.56544726\n",
      "Iteration 76, loss = 0.56352547\n",
      "Iteration 77, loss = 0.56186037\n",
      "Iteration 78, loss = 0.56016123\n",
      "Iteration 79, loss = 0.55817016\n",
      "Iteration 80, loss = 0.55609075\n",
      "Iteration 81, loss = 0.55386238\n",
      "Iteration 82, loss = 0.55173571\n",
      "Iteration 83, loss = 0.54943037\n",
      "Iteration 84, loss = 0.54712365\n",
      "Iteration 85, loss = 0.54485831\n",
      "Iteration 86, loss = 0.54286750\n",
      "Iteration 87, loss = 0.54107012\n",
      "Iteration 88, loss = 0.53922439\n",
      "Iteration 89, loss = 0.53743807\n",
      "Iteration 90, loss = 0.53570342\n",
      "Iteration 91, loss = 0.53389511\n",
      "Iteration 92, loss = 0.53203539\n",
      "Iteration 93, loss = 0.53012329\n",
      "Iteration 94, loss = 0.52820022\n",
      "Iteration 95, loss = 0.52624253\n",
      "Iteration 96, loss = 0.52415920\n",
      "Iteration 97, loss = 0.52204151\n",
      "Iteration 98, loss = 0.51986713\n",
      "Iteration 99, loss = 0.51777124\n",
      "Iteration 100, loss = 0.51585285\n",
      "Iteration 101, loss = 0.51387796\n",
      "Iteration 102, loss = 0.51187321\n",
      "Iteration 103, loss = 0.50997282\n",
      "Iteration 104, loss = 0.50822635\n",
      "Iteration 105, loss = 0.50649653\n",
      "Iteration 106, loss = 0.50494181\n",
      "Iteration 107, loss = 0.50370007\n",
      "Iteration 108, loss = 0.50222129\n",
      "Iteration 109, loss = 0.50086508\n",
      "Iteration 110, loss = 0.49965040\n",
      "Iteration 111, loss = 0.49856361\n",
      "Iteration 112, loss = 0.49739055\n",
      "Iteration 113, loss = 0.49605693\n",
      "Iteration 114, loss = 0.49468163\n",
      "Iteration 115, loss = 0.49305921\n",
      "Iteration 116, loss = 0.49133578\n",
      "Iteration 117, loss = 0.48980007\n",
      "Iteration 118, loss = 0.48822007\n",
      "Iteration 119, loss = 0.48669518\n",
      "Iteration 120, loss = 0.48510054\n",
      "Iteration 121, loss = 0.48349820\n",
      "Iteration 122, loss = 0.48188844\n",
      "Iteration 123, loss = 0.48035106\n",
      "Iteration 124, loss = 0.47863800\n",
      "Iteration 125, loss = 0.47701322\n",
      "Iteration 126, loss = 0.47557583\n",
      "Iteration 127, loss = 0.47380019\n",
      "Iteration 128, loss = 0.47191958\n",
      "Iteration 129, loss = 0.47013972\n",
      "Iteration 130, loss = 0.46828691\n",
      "Iteration 131, loss = 0.46644307\n",
      "Iteration 132, loss = 0.46472583\n",
      "Iteration 133, loss = 0.46301339\n",
      "Iteration 134, loss = 0.46152023\n",
      "Iteration 135, loss = 0.46014633\n",
      "Iteration 136, loss = 0.45880054\n",
      "Iteration 137, loss = 0.45734340\n",
      "Iteration 138, loss = 0.45613729\n",
      "Iteration 139, loss = 0.45470372\n",
      "Iteration 140, loss = 0.45318523\n",
      "Iteration 141, loss = 0.45162925\n",
      "Iteration 142, loss = 0.44999002\n",
      "Iteration 143, loss = 0.44855450\n",
      "Iteration 144, loss = 0.44721062\n",
      "Iteration 145, loss = 0.44580216\n",
      "Iteration 146, loss = 0.44442328\n",
      "Iteration 147, loss = 0.44313453\n",
      "Iteration 148, loss = 0.44219226\n",
      "Iteration 149, loss = 0.44161460\n",
      "Iteration 150, loss = 0.44093115\n",
      "Iteration 151, loss = 0.44015417\n",
      "Iteration 152, loss = 0.43931904\n",
      "Iteration 153, loss = 0.43821255\n",
      "Iteration 154, loss = 0.43704256\n",
      "Iteration 155, loss = 0.43576274\n",
      "Iteration 156, loss = 0.43448809\n",
      "Iteration 157, loss = 0.43322787\n",
      "Iteration 158, loss = 0.43202810\n",
      "Iteration 159, loss = 0.43107060\n",
      "Iteration 160, loss = 0.43019715\n",
      "Iteration 161, loss = 0.42930552\n",
      "Iteration 162, loss = 0.42857260\n",
      "Iteration 163, loss = 0.42791477\n",
      "Iteration 164, loss = 0.42717358\n",
      "Iteration 165, loss = 0.42639986\n",
      "Iteration 166, loss = 0.42571224\n",
      "Iteration 167, loss = 0.42486297\n",
      "Iteration 168, loss = 0.42407383\n",
      "Iteration 169, loss = 0.42327012\n",
      "Iteration 170, loss = 0.42259457\n",
      "Iteration 171, loss = 0.42180622\n",
      "Iteration 172, loss = 0.42118235\n",
      "Iteration 173, loss = 0.42039977\n",
      "Iteration 174, loss = 0.41962932\n",
      "Iteration 175, loss = 0.41916428\n",
      "Iteration 176, loss = 0.41934223\n",
      "Iteration 177, loss = 0.41902316\n",
      "Iteration 178, loss = 0.41852065\n",
      "Iteration 179, loss = 0.41754799\n",
      "Iteration 180, loss = 0.41646019\n",
      "Iteration 181, loss = 0.41557440\n",
      "Iteration 182, loss = 0.41455998\n",
      "Iteration 183, loss = 0.41366079\n",
      "Iteration 184, loss = 0.41270870\n",
      "Iteration 185, loss = 0.41168947\n",
      "Iteration 186, loss = 0.41066634\n",
      "Iteration 187, loss = 0.40963188\n",
      "Iteration 188, loss = 0.40855406\n",
      "Iteration 189, loss = 0.40744928\n",
      "Iteration 190, loss = 0.40636246\n",
      "Iteration 191, loss = 0.40536357\n",
      "Iteration 192, loss = 0.40434452\n",
      "Iteration 193, loss = 0.40337011\n",
      "Iteration 194, loss = 0.40242594\n",
      "Iteration 195, loss = 0.40146161\n",
      "Iteration 196, loss = 0.40063741\n",
      "Iteration 197, loss = 0.39977398\n",
      "Iteration 198, loss = 0.39896714\n",
      "Iteration 199, loss = 0.39812816\n",
      "Iteration 200, loss = 0.39734418\n",
      "Iteration 201, loss = 0.39660097\n",
      "Iteration 202, loss = 0.39591080\n",
      "Iteration 203, loss = 0.39522847\n",
      "Iteration 204, loss = 0.39449353\n",
      "Iteration 205, loss = 0.39388730\n",
      "Iteration 206, loss = 0.39332964\n",
      "Iteration 207, loss = 0.39280183\n",
      "Iteration 208, loss = 0.39224096\n",
      "Iteration 209, loss = 0.39166386\n",
      "Iteration 210, loss = 0.39133007\n",
      "Iteration 211, loss = 0.39089450\n",
      "Iteration 212, loss = 0.39037458\n",
      "Iteration 213, loss = 0.38980477\n",
      "Iteration 214, loss = 0.38920942\n",
      "Iteration 215, loss = 0.38857827\n",
      "Iteration 216, loss = 0.38795228\n",
      "Iteration 217, loss = 0.38720521\n",
      "Iteration 218, loss = 0.38643928\n",
      "Iteration 219, loss = 0.38564994\n",
      "Iteration 220, loss = 0.38491497\n",
      "Iteration 221, loss = 0.38410431\n",
      "Iteration 222, loss = 0.38335795\n",
      "Iteration 223, loss = 0.38270576\n",
      "Iteration 224, loss = 0.38238881\n",
      "Iteration 225, loss = 0.38186750\n",
      "Iteration 226, loss = 0.38154790\n",
      "Iteration 227, loss = 0.38122426\n",
      "Iteration 228, loss = 0.38103433\n",
      "Iteration 229, loss = 0.38063029\n",
      "Iteration 230, loss = 0.38073302\n",
      "Iteration 231, loss = 0.38100553\n",
      "Iteration 232, loss = 0.38076064\n",
      "Iteration 233, loss = 0.38062806\n",
      "Iteration 234, loss = 0.38050742\n",
      "Iteration 235, loss = 0.38032076\n",
      "Iteration 236, loss = 0.37981104\n",
      "Iteration 237, loss = 0.37909027\n",
      "Iteration 238, loss = 0.37838997\n",
      "Iteration 239, loss = 0.37779307\n",
      "Iteration 240, loss = 0.37769036\n",
      "Iteration 241, loss = 0.37711536\n",
      "Iteration 242, loss = 0.37640338\n",
      "Iteration 243, loss = 0.37563029\n",
      "Iteration 244, loss = 0.37495212\n",
      "Iteration 245, loss = 0.37422883\n",
      "Iteration 246, loss = 0.37338361\n",
      "Iteration 247, loss = 0.37260625\n",
      "Iteration 248, loss = 0.37202877\n",
      "Iteration 249, loss = 0.37138671\n",
      "Iteration 250, loss = 0.37066060\n",
      "Iteration 251, loss = 0.37000549\n",
      "Iteration 252, loss = 0.36937279\n",
      "Iteration 253, loss = 0.36894590\n",
      "Iteration 254, loss = 0.36856972\n",
      "Iteration 255, loss = 0.36834164\n",
      "Iteration 256, loss = 0.36800272\n",
      "Iteration 257, loss = 0.36739935\n",
      "Iteration 258, loss = 0.36672441\n",
      "Iteration 259, loss = 0.36603636\n",
      "Iteration 260, loss = 0.36545859\n",
      "Iteration 261, loss = 0.36494537\n",
      "Iteration 262, loss = 0.36447330\n",
      "Iteration 263, loss = 0.36401905\n",
      "Iteration 264, loss = 0.36358227\n",
      "Iteration 265, loss = 0.36328392\n",
      "Iteration 266, loss = 0.36288487\n",
      "Iteration 267, loss = 0.36247405\n",
      "Iteration 268, loss = 0.36203327\n",
      "Iteration 269, loss = 0.36157898\n",
      "Iteration 270, loss = 0.36117898\n",
      "Iteration 271, loss = 0.36066808\n",
      "Iteration 272, loss = 0.36020285\n",
      "Iteration 273, loss = 0.35976931\n",
      "Iteration 274, loss = 0.35943764\n",
      "Iteration 275, loss = 0.35901720\n",
      "Iteration 276, loss = 0.35866960\n",
      "Iteration 277, loss = 0.35832032\n",
      "Iteration 278, loss = 0.35792288\n",
      "Iteration 279, loss = 0.35753465\n",
      "Iteration 280, loss = 0.35719411\n",
      "Iteration 281, loss = 0.35685704\n",
      "Iteration 282, loss = 0.35657218\n",
      "Iteration 283, loss = 0.35606202\n",
      "Iteration 284, loss = 0.35567310\n",
      "Iteration 285, loss = 0.35546141\n",
      "Iteration 286, loss = 0.35510790\n",
      "Iteration 287, loss = 0.35475016\n",
      "Iteration 288, loss = 0.35434514\n",
      "Iteration 289, loss = 0.35395521\n",
      "Iteration 290, loss = 0.35343660\n",
      "Iteration 291, loss = 0.35302940\n",
      "Iteration 292, loss = 0.35267242\n",
      "Iteration 293, loss = 0.35233214\n",
      "Iteration 294, loss = 0.35204624\n",
      "Iteration 295, loss = 0.35172415\n",
      "Iteration 296, loss = 0.35131211\n",
      "Iteration 297, loss = 0.35092968\n",
      "Iteration 298, loss = 0.35057775\n",
      "Iteration 299, loss = 0.35028712\n",
      "Iteration 300, loss = 0.34996957\n",
      "Iteration 301, loss = 0.34965479\n",
      "Iteration 302, loss = 0.34934869\n",
      "Iteration 303, loss = 0.34903084\n",
      "Iteration 304, loss = 0.34874650\n",
      "Iteration 305, loss = 0.34840830\n",
      "Iteration 306, loss = 0.34803969\n",
      "Iteration 307, loss = 0.34773294\n",
      "Iteration 308, loss = 0.34750963\n",
      "Iteration 309, loss = 0.34722471\n",
      "Iteration 310, loss = 0.34687258\n",
      "Iteration 311, loss = 0.34654702\n",
      "Iteration 312, loss = 0.34624317\n",
      "Iteration 313, loss = 0.34593555\n",
      "Iteration 314, loss = 0.34563560\n",
      "Iteration 315, loss = 0.34530332\n",
      "Iteration 316, loss = 0.34498265\n",
      "Iteration 317, loss = 0.34467405\n",
      "Iteration 318, loss = 0.34430123\n",
      "Iteration 319, loss = 0.34401665\n",
      "Iteration 320, loss = 0.34354600\n",
      "Iteration 321, loss = 0.34313811\n",
      "Iteration 322, loss = 0.34275229\n",
      "Iteration 323, loss = 0.34241636\n",
      "Iteration 324, loss = 0.34208469\n",
      "Iteration 325, loss = 0.34180294\n",
      "Iteration 326, loss = 0.34154263\n",
      "Iteration 327, loss = 0.34132147\n",
      "Iteration 328, loss = 0.34138103\n",
      "Iteration 329, loss = 0.34109866\n",
      "Iteration 330, loss = 0.34100436\n",
      "Iteration 331, loss = 0.34102229\n",
      "Iteration 332, loss = 0.34124893\n",
      "Iteration 333, loss = 0.34107291\n",
      "Iteration 334, loss = 0.34085835\n",
      "Iteration 335, loss = 0.34046008\n",
      "Iteration 336, loss = 0.33990880\n",
      "Iteration 337, loss = 0.33942500\n",
      "Iteration 338, loss = 0.33896643\n",
      "Iteration 339, loss = 0.33864150\n",
      "Iteration 340, loss = 0.33826552\n",
      "Iteration 341, loss = 0.33782789\n",
      "Iteration 342, loss = 0.33750472\n",
      "Iteration 343, loss = 0.33724283\n",
      "Iteration 344, loss = 0.33701755\n",
      "Iteration 345, loss = 0.33676307\n",
      "Iteration 346, loss = 0.33652514\n",
      "Iteration 347, loss = 0.33634491\n",
      "Iteration 348, loss = 0.33614763\n",
      "Iteration 349, loss = 0.33596261\n",
      "Iteration 350, loss = 0.33574428\n",
      "Iteration 351, loss = 0.33553968\n",
      "Iteration 352, loss = 0.33535851\n",
      "Iteration 353, loss = 0.33514723\n",
      "Iteration 354, loss = 0.33493802\n",
      "Iteration 355, loss = 0.33476773\n",
      "Iteration 356, loss = 0.33462964\n",
      "Iteration 357, loss = 0.33449489\n",
      "Iteration 358, loss = 0.33432982\n",
      "Iteration 359, loss = 0.33417317\n",
      "Iteration 360, loss = 0.33408670\n",
      "Iteration 361, loss = 0.33409784\n",
      "Iteration 362, loss = 0.33398045\n",
      "Iteration 363, loss = 0.33382252\n",
      "Iteration 364, loss = 0.33343890\n",
      "Iteration 365, loss = 0.33316112\n",
      "Iteration 366, loss = 0.33296763\n",
      "Iteration 367, loss = 0.33282074\n",
      "Iteration 368, loss = 0.33254909\n",
      "Iteration 369, loss = 0.33217143\n",
      "Iteration 370, loss = 0.33182247\n",
      "Iteration 371, loss = 0.33163733\n",
      "Iteration 372, loss = 0.33114276\n",
      "Iteration 373, loss = 0.33069433\n",
      "Iteration 374, loss = 0.33042513\n",
      "Iteration 375, loss = 0.33019933\n",
      "Iteration 376, loss = 0.33009553\n",
      "Iteration 377, loss = 0.33038810\n",
      "Iteration 378, loss = 0.32975656\n",
      "Iteration 379, loss = 0.32890902\n",
      "Iteration 380, loss = 0.32822205\n",
      "Iteration 381, loss = 0.32761211\n",
      "Iteration 382, loss = 0.32713869\n",
      "Iteration 383, loss = 0.32686830\n",
      "Iteration 384, loss = 0.32669574\n",
      "Iteration 385, loss = 0.32668630\n",
      "Iteration 386, loss = 0.32634974\n",
      "Iteration 387, loss = 0.32612330\n",
      "Iteration 388, loss = 0.32587367\n",
      "Iteration 389, loss = 0.32562364\n",
      "Iteration 390, loss = 0.32537419\n",
      "Iteration 391, loss = 0.32507705\n",
      "Iteration 392, loss = 0.32482857\n",
      "Iteration 393, loss = 0.32462919\n",
      "Iteration 394, loss = 0.32445186\n",
      "Iteration 395, loss = 0.32442664\n",
      "Iteration 396, loss = 0.32448432\n",
      "Iteration 397, loss = 0.32460617\n",
      "Iteration 398, loss = 0.32438537\n",
      "Iteration 399, loss = 0.32390004\n",
      "Iteration 400, loss = 0.32352077\n",
      "Iteration 401, loss = 0.32311204\n",
      "Iteration 402, loss = 0.32286490\n",
      "Iteration 403, loss = 0.32278974\n",
      "Iteration 404, loss = 0.32267455\n",
      "Iteration 405, loss = 0.32261072\n",
      "Iteration 406, loss = 0.32270276\n",
      "Iteration 407, loss = 0.32258076\n",
      "Iteration 408, loss = 0.32251291\n",
      "Iteration 409, loss = 0.32271697\n",
      "Iteration 410, loss = 0.32266610\n",
      "Iteration 411, loss = 0.32248082\n",
      "Iteration 412, loss = 0.32261084\n",
      "Iteration 413, loss = 0.32296401\n",
      "Iteration 414, loss = 0.32303790\n",
      "Iteration 415, loss = 0.32324134\n",
      "Iteration 416, loss = 0.32316621\n",
      "Iteration 417, loss = 0.32308669\n",
      "Iteration 418, loss = 0.32287963\n",
      "Iteration 419, loss = 0.32321755\n",
      "Iteration 420, loss = 0.32376620\n",
      "Iteration 421, loss = 0.32411468\n",
      "Iteration 422, loss = 0.32418747\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71581920\n",
      "Iteration 2, loss = 0.71522460\n",
      "Iteration 3, loss = 0.71441696\n",
      "Iteration 4, loss = 0.71363142\n",
      "Iteration 5, loss = 0.71267595\n",
      "Iteration 6, loss = 0.71122527\n",
      "Iteration 7, loss = 0.70963513\n",
      "Iteration 8, loss = 0.70765217\n",
      "Iteration 9, loss = 0.70548865\n",
      "Iteration 10, loss = 0.70327690\n",
      "Iteration 11, loss = 0.70150255\n",
      "Iteration 12, loss = 0.69930140\n",
      "Iteration 13, loss = 0.69676495\n",
      "Iteration 14, loss = 0.69426984\n",
      "Iteration 15, loss = 0.69181025\n",
      "Iteration 16, loss = 0.68931614\n",
      "Iteration 17, loss = 0.68660063\n",
      "Iteration 18, loss = 0.68397443\n",
      "Iteration 19, loss = 0.68144597\n",
      "Iteration 20, loss = 0.67896466\n",
      "Iteration 21, loss = 0.67644475\n",
      "Iteration 22, loss = 0.67394046\n",
      "Iteration 23, loss = 0.67145485\n",
      "Iteration 24, loss = 0.66899901\n",
      "Iteration 25, loss = 0.66645710\n",
      "Iteration 26, loss = 0.66390994\n",
      "Iteration 27, loss = 0.66139340\n",
      "Iteration 28, loss = 0.65887000\n",
      "Iteration 29, loss = 0.65624058\n",
      "Iteration 30, loss = 0.65378244\n",
      "Iteration 31, loss = 0.65130170\n",
      "Iteration 32, loss = 0.64879913\n",
      "Iteration 33, loss = 0.64629038\n",
      "Iteration 34, loss = 0.64383562\n",
      "Iteration 35, loss = 0.64167036\n",
      "Iteration 36, loss = 0.63971316\n",
      "Iteration 37, loss = 0.63742406\n",
      "Iteration 38, loss = 0.63507775\n",
      "Iteration 39, loss = 0.63265457\n",
      "Iteration 40, loss = 0.63030349\n",
      "Iteration 41, loss = 0.62806915\n",
      "Iteration 42, loss = 0.62583954\n",
      "Iteration 43, loss = 0.62349099\n",
      "Iteration 44, loss = 0.62105315\n",
      "Iteration 45, loss = 0.61854456\n",
      "Iteration 46, loss = 0.61608973\n",
      "Iteration 47, loss = 0.61355642\n",
      "Iteration 48, loss = 0.61109136\n",
      "Iteration 49, loss = 0.60862691\n",
      "Iteration 50, loss = 0.60609881\n",
      "Iteration 51, loss = 0.60362924\n",
      "Iteration 52, loss = 0.60119594\n",
      "Iteration 53, loss = 0.59870526\n",
      "Iteration 54, loss = 0.59623860\n",
      "Iteration 55, loss = 0.59386357\n",
      "Iteration 56, loss = 0.59151989\n",
      "Iteration 57, loss = 0.58913437\n",
      "Iteration 58, loss = 0.58661664\n",
      "Iteration 59, loss = 0.58431926\n",
      "Iteration 60, loss = 0.58205045\n",
      "Iteration 61, loss = 0.57986082\n",
      "Iteration 62, loss = 0.57766731\n",
      "Iteration 63, loss = 0.57548827\n",
      "Iteration 64, loss = 0.57321985\n",
      "Iteration 65, loss = 0.57075721\n",
      "Iteration 66, loss = 0.56833720\n",
      "Iteration 67, loss = 0.56597578\n",
      "Iteration 68, loss = 0.56388177\n",
      "Iteration 69, loss = 0.56148005\n",
      "Iteration 70, loss = 0.55913966\n",
      "Iteration 71, loss = 0.55667701\n",
      "Iteration 72, loss = 0.55423459\n",
      "Iteration 73, loss = 0.55167560\n",
      "Iteration 74, loss = 0.54908503\n",
      "Iteration 75, loss = 0.54660094\n",
      "Iteration 76, loss = 0.54435406\n",
      "Iteration 77, loss = 0.54236225\n",
      "Iteration 78, loss = 0.54036088\n",
      "Iteration 79, loss = 0.53814331\n",
      "Iteration 80, loss = 0.53583253\n",
      "Iteration 81, loss = 0.53340542\n",
      "Iteration 82, loss = 0.53105042\n",
      "Iteration 83, loss = 0.52863876\n",
      "Iteration 84, loss = 0.52620022\n",
      "Iteration 85, loss = 0.52391652\n",
      "Iteration 86, loss = 0.52176866\n",
      "Iteration 87, loss = 0.51975054\n",
      "Iteration 88, loss = 0.51767410\n",
      "Iteration 89, loss = 0.51570777\n",
      "Iteration 90, loss = 0.51379576\n",
      "Iteration 91, loss = 0.51177491\n",
      "Iteration 92, loss = 0.50973575\n",
      "Iteration 93, loss = 0.50774526\n",
      "Iteration 94, loss = 0.50574704\n",
      "Iteration 95, loss = 0.50383158\n",
      "Iteration 96, loss = 0.50185280\n",
      "Iteration 97, loss = 0.49966672\n",
      "Iteration 98, loss = 0.49747763\n",
      "Iteration 99, loss = 0.49521133\n",
      "Iteration 100, loss = 0.49294677\n",
      "Iteration 101, loss = 0.49064524\n",
      "Iteration 102, loss = 0.48834810\n",
      "Iteration 103, loss = 0.48616615\n",
      "Iteration 104, loss = 0.48401914\n",
      "Iteration 105, loss = 0.48183361\n",
      "Iteration 106, loss = 0.47980794\n",
      "Iteration 107, loss = 0.47806879\n",
      "Iteration 108, loss = 0.47625769\n",
      "Iteration 109, loss = 0.47434565\n",
      "Iteration 110, loss = 0.47263454\n",
      "Iteration 111, loss = 0.47109686\n",
      "Iteration 112, loss = 0.46949347\n",
      "Iteration 113, loss = 0.46781173\n",
      "Iteration 114, loss = 0.46601282\n",
      "Iteration 115, loss = 0.46423373\n",
      "Iteration 116, loss = 0.46246915\n",
      "Iteration 117, loss = 0.46082362\n",
      "Iteration 118, loss = 0.45915660\n",
      "Iteration 119, loss = 0.45762108\n",
      "Iteration 120, loss = 0.45597083\n",
      "Iteration 121, loss = 0.45429480\n",
      "Iteration 122, loss = 0.45257992\n",
      "Iteration 123, loss = 0.45087436\n",
      "Iteration 124, loss = 0.44911172\n",
      "Iteration 125, loss = 0.44732311\n",
      "Iteration 126, loss = 0.44568746\n",
      "Iteration 127, loss = 0.44392748\n",
      "Iteration 128, loss = 0.44210422\n",
      "Iteration 129, loss = 0.44043992\n",
      "Iteration 130, loss = 0.43872230\n",
      "Iteration 131, loss = 0.43699238\n",
      "Iteration 132, loss = 0.43533522\n",
      "Iteration 133, loss = 0.43367001\n",
      "Iteration 134, loss = 0.43222622\n",
      "Iteration 135, loss = 0.43089375\n",
      "Iteration 136, loss = 0.42949173\n",
      "Iteration 137, loss = 0.42801407\n",
      "Iteration 138, loss = 0.42681705\n",
      "Iteration 139, loss = 0.42548007\n",
      "Iteration 140, loss = 0.42412634\n",
      "Iteration 141, loss = 0.42272661\n",
      "Iteration 142, loss = 0.42114822\n",
      "Iteration 143, loss = 0.41976122\n",
      "Iteration 144, loss = 0.41843508\n",
      "Iteration 145, loss = 0.41710367\n",
      "Iteration 146, loss = 0.41576739\n",
      "Iteration 147, loss = 0.41456032\n",
      "Iteration 148, loss = 0.41371289\n",
      "Iteration 149, loss = 0.41339010\n",
      "Iteration 150, loss = 0.41292981\n",
      "Iteration 151, loss = 0.41236196\n",
      "Iteration 152, loss = 0.41173592\n",
      "Iteration 153, loss = 0.41073084\n",
      "Iteration 154, loss = 0.40957095\n",
      "Iteration 155, loss = 0.40828082\n",
      "Iteration 156, loss = 0.40697624\n",
      "Iteration 157, loss = 0.40566266\n",
      "Iteration 158, loss = 0.40434874\n",
      "Iteration 159, loss = 0.40331458\n",
      "Iteration 160, loss = 0.40241413\n",
      "Iteration 161, loss = 0.40144123\n",
      "Iteration 162, loss = 0.40043749\n",
      "Iteration 163, loss = 0.39952386\n",
      "Iteration 164, loss = 0.39868543\n",
      "Iteration 165, loss = 0.39779810\n",
      "Iteration 166, loss = 0.39703275\n",
      "Iteration 167, loss = 0.39613694\n",
      "Iteration 168, loss = 0.39535905\n",
      "Iteration 169, loss = 0.39456861\n",
      "Iteration 170, loss = 0.39385767\n",
      "Iteration 171, loss = 0.39305956\n",
      "Iteration 172, loss = 0.39245160\n",
      "Iteration 173, loss = 0.39163017\n",
      "Iteration 174, loss = 0.39083079\n",
      "Iteration 175, loss = 0.39033118\n",
      "Iteration 176, loss = 0.38997941\n",
      "Iteration 177, loss = 0.38929463\n",
      "Iteration 178, loss = 0.38845050\n",
      "Iteration 179, loss = 0.38751025\n",
      "Iteration 180, loss = 0.38637949\n",
      "Iteration 181, loss = 0.38506591\n",
      "Iteration 182, loss = 0.38373302\n",
      "Iteration 183, loss = 0.38269247\n",
      "Iteration 184, loss = 0.38166695\n",
      "Iteration 185, loss = 0.38056268\n",
      "Iteration 186, loss = 0.37950753\n",
      "Iteration 187, loss = 0.37842448\n",
      "Iteration 188, loss = 0.37734366\n",
      "Iteration 189, loss = 0.37626313\n",
      "Iteration 190, loss = 0.37520740\n",
      "Iteration 191, loss = 0.37424100\n",
      "Iteration 192, loss = 0.37332733\n",
      "Iteration 193, loss = 0.37245735\n",
      "Iteration 194, loss = 0.37158321\n",
      "Iteration 195, loss = 0.37066611\n",
      "Iteration 196, loss = 0.36983979\n",
      "Iteration 197, loss = 0.36906041\n",
      "Iteration 198, loss = 0.36831116\n",
      "Iteration 199, loss = 0.36752765\n",
      "Iteration 200, loss = 0.36679056\n",
      "Iteration 201, loss = 0.36617240\n",
      "Iteration 202, loss = 0.36571525\n",
      "Iteration 203, loss = 0.36512507\n",
      "Iteration 204, loss = 0.36450895\n",
      "Iteration 205, loss = 0.36399723\n",
      "Iteration 206, loss = 0.36342509\n",
      "Iteration 207, loss = 0.36283114\n",
      "Iteration 208, loss = 0.36218017\n",
      "Iteration 209, loss = 0.36156382\n",
      "Iteration 210, loss = 0.36099975\n",
      "Iteration 211, loss = 0.36043430\n",
      "Iteration 212, loss = 0.35993717\n",
      "Iteration 213, loss = 0.35942194\n",
      "Iteration 214, loss = 0.35889320\n",
      "Iteration 215, loss = 0.35828944\n",
      "Iteration 216, loss = 0.35768344\n",
      "Iteration 217, loss = 0.35702128\n",
      "Iteration 218, loss = 0.35634578\n",
      "Iteration 219, loss = 0.35564700\n",
      "Iteration 220, loss = 0.35503250\n",
      "Iteration 221, loss = 0.35432814\n",
      "Iteration 222, loss = 0.35370086\n",
      "Iteration 223, loss = 0.35317429\n",
      "Iteration 224, loss = 0.35285840\n",
      "Iteration 225, loss = 0.35235799\n",
      "Iteration 226, loss = 0.35189692\n",
      "Iteration 227, loss = 0.35147048\n",
      "Iteration 228, loss = 0.35107654\n",
      "Iteration 229, loss = 0.35063648\n",
      "Iteration 230, loss = 0.35045634\n",
      "Iteration 231, loss = 0.35043794\n",
      "Iteration 232, loss = 0.35002495\n",
      "Iteration 233, loss = 0.34978571\n",
      "Iteration 234, loss = 0.34957704\n",
      "Iteration 235, loss = 0.34935546\n",
      "Iteration 236, loss = 0.34894992\n",
      "Iteration 237, loss = 0.34834052\n",
      "Iteration 238, loss = 0.34780059\n",
      "Iteration 239, loss = 0.34737595\n",
      "Iteration 240, loss = 0.34742653\n",
      "Iteration 241, loss = 0.34696962\n",
      "Iteration 242, loss = 0.34633794\n",
      "Iteration 243, loss = 0.34564797\n",
      "Iteration 244, loss = 0.34503040\n",
      "Iteration 245, loss = 0.34438494\n",
      "Iteration 246, loss = 0.34373250\n",
      "Iteration 247, loss = 0.34299689\n",
      "Iteration 248, loss = 0.34245805\n",
      "Iteration 249, loss = 0.34187747\n",
      "Iteration 250, loss = 0.34140825\n",
      "Iteration 251, loss = 0.34094516\n",
      "Iteration 252, loss = 0.34041961\n",
      "Iteration 253, loss = 0.33985532\n",
      "Iteration 254, loss = 0.33943830\n",
      "Iteration 255, loss = 0.33923236\n",
      "Iteration 256, loss = 0.33906918\n",
      "Iteration 257, loss = 0.33862959\n",
      "Iteration 258, loss = 0.33815762\n",
      "Iteration 259, loss = 0.33751445\n",
      "Iteration 260, loss = 0.33677326\n",
      "Iteration 261, loss = 0.33616447\n",
      "Iteration 262, loss = 0.33559077\n",
      "Iteration 263, loss = 0.33511574\n",
      "Iteration 264, loss = 0.33471814\n",
      "Iteration 265, loss = 0.33457303\n",
      "Iteration 266, loss = 0.33412381\n",
      "Iteration 267, loss = 0.33368973\n",
      "Iteration 268, loss = 0.33307367\n",
      "Iteration 269, loss = 0.33249953\n",
      "Iteration 270, loss = 0.33198587\n",
      "Iteration 271, loss = 0.33147181\n",
      "Iteration 272, loss = 0.33109400\n",
      "Iteration 273, loss = 0.33075722\n",
      "Iteration 274, loss = 0.33035069\n",
      "Iteration 275, loss = 0.32992850\n",
      "Iteration 276, loss = 0.32973905\n",
      "Iteration 277, loss = 0.32954979\n",
      "Iteration 278, loss = 0.32915278\n",
      "Iteration 279, loss = 0.32868288\n",
      "Iteration 280, loss = 0.32824678\n",
      "Iteration 281, loss = 0.32782929\n",
      "Iteration 282, loss = 0.32750717\n",
      "Iteration 283, loss = 0.32714399\n",
      "Iteration 284, loss = 0.32689012\n",
      "Iteration 285, loss = 0.32650849\n",
      "Iteration 286, loss = 0.32597097\n",
      "Iteration 287, loss = 0.32524697\n",
      "Iteration 288, loss = 0.32464231\n",
      "Iteration 289, loss = 0.32405043\n",
      "Iteration 290, loss = 0.32344706\n",
      "Iteration 291, loss = 0.32291910\n",
      "Iteration 292, loss = 0.32245556\n",
      "Iteration 293, loss = 0.32195295\n",
      "Iteration 294, loss = 0.32151416\n",
      "Iteration 295, loss = 0.32116173\n",
      "Iteration 296, loss = 0.32086239\n",
      "Iteration 297, loss = 0.32064024\n",
      "Iteration 298, loss = 0.32044225\n",
      "Iteration 299, loss = 0.32015031\n",
      "Iteration 300, loss = 0.31988529\n",
      "Iteration 301, loss = 0.31960758\n",
      "Iteration 302, loss = 0.31909318\n",
      "Iteration 303, loss = 0.31862011\n",
      "Iteration 304, loss = 0.31814218\n",
      "Iteration 305, loss = 0.31770876\n",
      "Iteration 306, loss = 0.31728278\n",
      "Iteration 307, loss = 0.31693971\n",
      "Iteration 308, loss = 0.31664590\n",
      "Iteration 309, loss = 0.31631857\n",
      "Iteration 310, loss = 0.31592098\n",
      "Iteration 311, loss = 0.31552444\n",
      "Iteration 312, loss = 0.31510928\n",
      "Iteration 313, loss = 0.31471860\n",
      "Iteration 314, loss = 0.31444222\n",
      "Iteration 315, loss = 0.31406015\n",
      "Iteration 316, loss = 0.31369646\n",
      "Iteration 317, loss = 0.31324779\n",
      "Iteration 318, loss = 0.31279427\n",
      "Iteration 319, loss = 0.31240280\n",
      "Iteration 320, loss = 0.31183845\n",
      "Iteration 321, loss = 0.31135385\n",
      "Iteration 322, loss = 0.31101487\n",
      "Iteration 323, loss = 0.31062927\n",
      "Iteration 324, loss = 0.31022030\n",
      "Iteration 325, loss = 0.30993132\n",
      "Iteration 326, loss = 0.30965818\n",
      "Iteration 327, loss = 0.30937907\n",
      "Iteration 328, loss = 0.30930738\n",
      "Iteration 329, loss = 0.30893998\n",
      "Iteration 330, loss = 0.30873375\n",
      "Iteration 331, loss = 0.30860566\n",
      "Iteration 332, loss = 0.30866896\n",
      "Iteration 333, loss = 0.30834317\n",
      "Iteration 334, loss = 0.30795534\n",
      "Iteration 335, loss = 0.30744145\n",
      "Iteration 336, loss = 0.30690168\n",
      "Iteration 337, loss = 0.30651677\n",
      "Iteration 338, loss = 0.30614954\n",
      "Iteration 339, loss = 0.30593206\n",
      "Iteration 340, loss = 0.30580295\n",
      "Iteration 341, loss = 0.30554826\n",
      "Iteration 342, loss = 0.30532032\n",
      "Iteration 343, loss = 0.30495160\n",
      "Iteration 344, loss = 0.30463832\n",
      "Iteration 345, loss = 0.30440025\n",
      "Iteration 346, loss = 0.30414211\n",
      "Iteration 347, loss = 0.30386157\n",
      "Iteration 348, loss = 0.30353892\n",
      "Iteration 349, loss = 0.30322532\n",
      "Iteration 350, loss = 0.30288970\n",
      "Iteration 351, loss = 0.30259443\n",
      "Iteration 352, loss = 0.30233975\n",
      "Iteration 353, loss = 0.30206765\n",
      "Iteration 354, loss = 0.30178419\n",
      "Iteration 355, loss = 0.30159246\n",
      "Iteration 356, loss = 0.30141739\n",
      "Iteration 357, loss = 0.30121017\n",
      "Iteration 358, loss = 0.30108179\n",
      "Iteration 359, loss = 0.30095034\n",
      "Iteration 360, loss = 0.30091524\n",
      "Iteration 361, loss = 0.30093753\n",
      "Iteration 362, loss = 0.30101385\n",
      "Iteration 363, loss = 0.30108536\n",
      "Iteration 364, loss = 0.30121271\n",
      "Iteration 365, loss = 0.30139518\n",
      "Iteration 366, loss = 0.30163921\n",
      "Iteration 367, loss = 0.30137809\n",
      "Iteration 368, loss = 0.30090784\n",
      "Iteration 369, loss = 0.30032854\n",
      "Iteration 370, loss = 0.29987276\n",
      "Iteration 371, loss = 0.29963637\n",
      "Iteration 372, loss = 0.29897919\n",
      "Iteration 373, loss = 0.29848455\n",
      "Iteration 374, loss = 0.29830366\n",
      "Iteration 375, loss = 0.29810979\n",
      "Iteration 376, loss = 0.29796701\n",
      "Iteration 377, loss = 0.29830747\n",
      "Iteration 378, loss = 0.29745232\n",
      "Iteration 379, loss = 0.29628573\n",
      "Iteration 380, loss = 0.29532576\n",
      "Iteration 381, loss = 0.29456329\n",
      "Iteration 382, loss = 0.29383980\n",
      "Iteration 383, loss = 0.29329560\n",
      "Iteration 384, loss = 0.29289453\n",
      "Iteration 385, loss = 0.29265348\n",
      "Iteration 386, loss = 0.29244399\n",
      "Iteration 387, loss = 0.29231333\n",
      "Iteration 388, loss = 0.29214155\n",
      "Iteration 389, loss = 0.29198504\n",
      "Iteration 390, loss = 0.29167963\n",
      "Iteration 391, loss = 0.29135189\n",
      "Iteration 392, loss = 0.29113849\n",
      "Iteration 393, loss = 0.29077697\n",
      "Iteration 394, loss = 0.29046058\n",
      "Iteration 395, loss = 0.29029301\n",
      "Iteration 396, loss = 0.29019262\n",
      "Iteration 397, loss = 0.29011685\n",
      "Iteration 398, loss = 0.28975059\n",
      "Iteration 399, loss = 0.28937961\n",
      "Iteration 400, loss = 0.28903553\n",
      "Iteration 401, loss = 0.28859626\n",
      "Iteration 402, loss = 0.28833196\n",
      "Iteration 403, loss = 0.28822121\n",
      "Iteration 404, loss = 0.28805891\n",
      "Iteration 405, loss = 0.28811690\n",
      "Iteration 406, loss = 0.28838649\n",
      "Iteration 407, loss = 0.28838250\n",
      "Iteration 408, loss = 0.28841767\n",
      "Iteration 409, loss = 0.28874062\n",
      "Iteration 410, loss = 0.28893029\n",
      "Iteration 411, loss = 0.28875321\n",
      "Iteration 412, loss = 0.28892058\n",
      "Iteration 413, loss = 0.28925303\n",
      "Iteration 414, loss = 0.28934424\n",
      "Iteration 415, loss = 0.28959022\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71756520\n",
      "Iteration 2, loss = 0.71698960\n",
      "Iteration 3, loss = 0.71616639\n",
      "Iteration 4, loss = 0.71539908\n",
      "Iteration 5, loss = 0.71450952\n",
      "Iteration 6, loss = 0.71341803\n",
      "Iteration 7, loss = 0.71203830\n",
      "Iteration 8, loss = 0.71025492\n",
      "Iteration 9, loss = 0.70829185\n",
      "Iteration 10, loss = 0.70622572\n",
      "Iteration 11, loss = 0.70452254\n",
      "Iteration 12, loss = 0.70254873\n",
      "Iteration 13, loss = 0.70010790\n",
      "Iteration 14, loss = 0.69766965\n",
      "Iteration 15, loss = 0.69526075\n",
      "Iteration 16, loss = 0.69288600\n",
      "Iteration 17, loss = 0.69033684\n",
      "Iteration 18, loss = 0.68781874\n",
      "Iteration 19, loss = 0.68541689\n",
      "Iteration 20, loss = 0.68307260\n",
      "Iteration 21, loss = 0.68063389\n",
      "Iteration 22, loss = 0.67825562\n",
      "Iteration 23, loss = 0.67592650\n",
      "Iteration 24, loss = 0.67363392\n",
      "Iteration 25, loss = 0.67132103\n",
      "Iteration 26, loss = 0.66912127\n",
      "Iteration 27, loss = 0.66691632\n",
      "Iteration 28, loss = 0.66469046\n",
      "Iteration 29, loss = 0.66233620\n",
      "Iteration 30, loss = 0.66006043\n",
      "Iteration 31, loss = 0.65777148\n",
      "Iteration 32, loss = 0.65547981\n",
      "Iteration 33, loss = 0.65322003\n",
      "Iteration 34, loss = 0.65108052\n",
      "Iteration 35, loss = 0.64919964\n",
      "Iteration 36, loss = 0.64751168\n",
      "Iteration 37, loss = 0.64548810\n",
      "Iteration 38, loss = 0.64342272\n",
      "Iteration 39, loss = 0.64128027\n",
      "Iteration 40, loss = 0.63925874\n",
      "Iteration 41, loss = 0.63720232\n",
      "Iteration 42, loss = 0.63515513\n",
      "Iteration 43, loss = 0.63307494\n",
      "Iteration 44, loss = 0.63089495\n",
      "Iteration 45, loss = 0.62865362\n",
      "Iteration 46, loss = 0.62654587\n",
      "Iteration 47, loss = 0.62433434\n",
      "Iteration 48, loss = 0.62217884\n",
      "Iteration 49, loss = 0.62000443\n",
      "Iteration 50, loss = 0.61778774\n",
      "Iteration 51, loss = 0.61580687\n",
      "Iteration 52, loss = 0.61380356\n",
      "Iteration 53, loss = 0.61168838\n",
      "Iteration 54, loss = 0.60954510\n",
      "Iteration 55, loss = 0.60746743\n",
      "Iteration 56, loss = 0.60539949\n",
      "Iteration 57, loss = 0.60328177\n",
      "Iteration 58, loss = 0.60102766\n",
      "Iteration 59, loss = 0.59891682\n",
      "Iteration 60, loss = 0.59681339\n",
      "Iteration 61, loss = 0.59486850\n",
      "Iteration 62, loss = 0.59290192\n",
      "Iteration 63, loss = 0.59108625\n",
      "Iteration 64, loss = 0.58916511\n",
      "Iteration 65, loss = 0.58702391\n",
      "Iteration 66, loss = 0.58493418\n",
      "Iteration 67, loss = 0.58291001\n",
      "Iteration 68, loss = 0.58090283\n",
      "Iteration 69, loss = 0.57877537\n",
      "Iteration 70, loss = 0.57682898\n",
      "Iteration 71, loss = 0.57477769\n",
      "Iteration 72, loss = 0.57271313\n",
      "Iteration 73, loss = 0.57048887\n",
      "Iteration 74, loss = 0.56824493\n",
      "Iteration 75, loss = 0.56618390\n",
      "Iteration 76, loss = 0.56433688\n",
      "Iteration 77, loss = 0.56271159\n",
      "Iteration 78, loss = 0.56109161\n",
      "Iteration 79, loss = 0.55922867\n",
      "Iteration 80, loss = 0.55743679\n",
      "Iteration 81, loss = 0.55546227\n",
      "Iteration 82, loss = 0.55351211\n",
      "Iteration 83, loss = 0.55146141\n",
      "Iteration 84, loss = 0.54957496\n",
      "Iteration 85, loss = 0.54784115\n",
      "Iteration 86, loss = 0.54591158\n",
      "Iteration 87, loss = 0.54398378\n",
      "Iteration 88, loss = 0.54206559\n",
      "Iteration 89, loss = 0.54026856\n",
      "Iteration 90, loss = 0.53852058\n",
      "Iteration 91, loss = 0.53674020\n",
      "Iteration 92, loss = 0.53490003\n",
      "Iteration 93, loss = 0.53327111\n",
      "Iteration 94, loss = 0.53156515\n",
      "Iteration 95, loss = 0.52987485\n",
      "Iteration 96, loss = 0.52809546\n",
      "Iteration 97, loss = 0.52618189\n",
      "Iteration 98, loss = 0.52434032\n",
      "Iteration 99, loss = 0.52237930\n",
      "Iteration 100, loss = 0.52044410\n",
      "Iteration 101, loss = 0.51840142\n",
      "Iteration 102, loss = 0.51629234\n",
      "Iteration 103, loss = 0.51423254\n",
      "Iteration 104, loss = 0.51208888\n",
      "Iteration 105, loss = 0.50988842\n",
      "Iteration 106, loss = 0.50763757\n",
      "Iteration 107, loss = 0.50567675\n",
      "Iteration 108, loss = 0.50371766\n",
      "Iteration 109, loss = 0.50179419\n",
      "Iteration 110, loss = 0.49998094\n",
      "Iteration 111, loss = 0.49822085\n",
      "Iteration 112, loss = 0.49638195\n",
      "Iteration 113, loss = 0.49449991\n",
      "Iteration 114, loss = 0.49254049\n",
      "Iteration 115, loss = 0.49069025\n",
      "Iteration 116, loss = 0.48897546\n",
      "Iteration 117, loss = 0.48723279\n",
      "Iteration 118, loss = 0.48548362\n",
      "Iteration 119, loss = 0.48377850\n",
      "Iteration 120, loss = 0.48202504\n",
      "Iteration 121, loss = 0.48036402\n",
      "Iteration 122, loss = 0.47861572\n",
      "Iteration 123, loss = 0.47683099\n",
      "Iteration 124, loss = 0.47506602\n",
      "Iteration 125, loss = 0.47322524\n",
      "Iteration 126, loss = 0.47185517\n",
      "Iteration 127, loss = 0.47037891\n",
      "Iteration 128, loss = 0.46928280\n",
      "Iteration 129, loss = 0.46831514\n",
      "Iteration 130, loss = 0.46723900\n",
      "Iteration 131, loss = 0.46594182\n",
      "Iteration 132, loss = 0.46473122\n",
      "Iteration 133, loss = 0.46336055\n",
      "Iteration 134, loss = 0.46193333\n",
      "Iteration 135, loss = 0.46061552\n",
      "Iteration 136, loss = 0.45926969\n",
      "Iteration 137, loss = 0.45773767\n",
      "Iteration 138, loss = 0.45649487\n",
      "Iteration 139, loss = 0.45507172\n",
      "Iteration 140, loss = 0.45364874\n",
      "Iteration 141, loss = 0.45218094\n",
      "Iteration 142, loss = 0.45041174\n",
      "Iteration 143, loss = 0.44878024\n",
      "Iteration 144, loss = 0.44721947\n",
      "Iteration 145, loss = 0.44565035\n",
      "Iteration 146, loss = 0.44418750\n",
      "Iteration 147, loss = 0.44290429\n",
      "Iteration 148, loss = 0.44203185\n",
      "Iteration 149, loss = 0.44143938\n",
      "Iteration 150, loss = 0.44080704\n",
      "Iteration 151, loss = 0.44005144\n",
      "Iteration 152, loss = 0.43924742\n",
      "Iteration 153, loss = 0.43818356\n",
      "Iteration 154, loss = 0.43706355\n",
      "Iteration 155, loss = 0.43597172\n",
      "Iteration 156, loss = 0.43486912\n",
      "Iteration 157, loss = 0.43373490\n",
      "Iteration 158, loss = 0.43258584\n",
      "Iteration 159, loss = 0.43165906\n",
      "Iteration 160, loss = 0.43090507\n",
      "Iteration 161, loss = 0.43000649\n",
      "Iteration 162, loss = 0.42901189\n",
      "Iteration 163, loss = 0.42803585\n",
      "Iteration 164, loss = 0.42718994\n",
      "Iteration 165, loss = 0.42644081\n",
      "Iteration 166, loss = 0.42565302\n",
      "Iteration 167, loss = 0.42485739\n",
      "Iteration 168, loss = 0.42408506\n",
      "Iteration 169, loss = 0.42330719\n",
      "Iteration 170, loss = 0.42256936\n",
      "Iteration 171, loss = 0.42175562\n",
      "Iteration 172, loss = 0.42102412\n",
      "Iteration 173, loss = 0.42013796\n",
      "Iteration 174, loss = 0.41924819\n",
      "Iteration 175, loss = 0.41867383\n",
      "Iteration 176, loss = 0.41791641\n",
      "Iteration 177, loss = 0.41736537\n",
      "Iteration 178, loss = 0.41680631\n",
      "Iteration 179, loss = 0.41612080\n",
      "Iteration 180, loss = 0.41590447\n",
      "Iteration 181, loss = 0.41530380\n",
      "Iteration 182, loss = 0.41449386\n",
      "Iteration 183, loss = 0.41388402\n",
      "Iteration 184, loss = 0.41317685\n",
      "Iteration 185, loss = 0.41225260\n",
      "Iteration 186, loss = 0.41114967\n",
      "Iteration 187, loss = 0.40979590\n",
      "Iteration 188, loss = 0.40847474\n",
      "Iteration 189, loss = 0.40721545\n",
      "Iteration 190, loss = 0.40608719\n",
      "Iteration 191, loss = 0.40525688\n",
      "Iteration 192, loss = 0.40454614\n",
      "Iteration 193, loss = 0.40381963\n",
      "Iteration 194, loss = 0.40290641\n",
      "Iteration 195, loss = 0.40204434\n",
      "Iteration 196, loss = 0.40115322\n",
      "Iteration 197, loss = 0.40030639\n",
      "Iteration 198, loss = 0.39945227\n",
      "Iteration 199, loss = 0.39856248\n",
      "Iteration 200, loss = 0.39774385\n",
      "Iteration 201, loss = 0.39705573\n",
      "Iteration 202, loss = 0.39655908\n",
      "Iteration 203, loss = 0.39599359\n",
      "Iteration 204, loss = 0.39528164\n",
      "Iteration 205, loss = 0.39469137\n",
      "Iteration 206, loss = 0.39409793\n",
      "Iteration 207, loss = 0.39357184\n",
      "Iteration 208, loss = 0.39309081\n",
      "Iteration 209, loss = 0.39264431\n",
      "Iteration 210, loss = 0.39217107\n",
      "Iteration 211, loss = 0.39165123\n",
      "Iteration 212, loss = 0.39114200\n",
      "Iteration 213, loss = 0.39062401\n",
      "Iteration 214, loss = 0.39008994\n",
      "Iteration 215, loss = 0.38949580\n",
      "Iteration 216, loss = 0.38888508\n",
      "Iteration 217, loss = 0.38821550\n",
      "Iteration 218, loss = 0.38754543\n",
      "Iteration 219, loss = 0.38696752\n",
      "Iteration 220, loss = 0.38664309\n",
      "Iteration 221, loss = 0.38605289\n",
      "Iteration 222, loss = 0.38549707\n",
      "Iteration 223, loss = 0.38507509\n",
      "Iteration 224, loss = 0.38441167\n",
      "Iteration 225, loss = 0.38383200\n",
      "Iteration 226, loss = 0.38346133\n",
      "Iteration 227, loss = 0.38267894\n",
      "Iteration 228, loss = 0.38205793\n",
      "Iteration 229, loss = 0.38145464\n",
      "Iteration 230, loss = 0.38102795\n",
      "Iteration 231, loss = 0.38077839\n",
      "Iteration 232, loss = 0.38032365\n",
      "Iteration 233, loss = 0.38003658\n",
      "Iteration 234, loss = 0.37981168\n",
      "Iteration 235, loss = 0.37963606\n",
      "Iteration 236, loss = 0.37911964\n",
      "Iteration 237, loss = 0.37851570\n",
      "Iteration 238, loss = 0.37791991\n",
      "Iteration 239, loss = 0.37749199\n",
      "Iteration 240, loss = 0.37755432\n",
      "Iteration 241, loss = 0.37736567\n",
      "Iteration 242, loss = 0.37702081\n",
      "Iteration 243, loss = 0.37657175\n",
      "Iteration 244, loss = 0.37619302\n",
      "Iteration 245, loss = 0.37571945\n",
      "Iteration 246, loss = 0.37528793\n",
      "Iteration 247, loss = 0.37463135\n",
      "Iteration 248, loss = 0.37398901\n",
      "Iteration 249, loss = 0.37312456\n",
      "Iteration 250, loss = 0.37242709\n",
      "Iteration 251, loss = 0.37181733\n",
      "Iteration 252, loss = 0.37116095\n",
      "Iteration 253, loss = 0.37041326\n",
      "Iteration 254, loss = 0.36983960\n",
      "Iteration 255, loss = 0.36939324\n",
      "Iteration 256, loss = 0.36898214\n",
      "Iteration 257, loss = 0.36859968\n",
      "Iteration 258, loss = 0.36819807\n",
      "Iteration 259, loss = 0.36780817\n",
      "Iteration 260, loss = 0.36729383\n",
      "Iteration 261, loss = 0.36685518\n",
      "Iteration 262, loss = 0.36635432\n",
      "Iteration 263, loss = 0.36584867\n",
      "Iteration 264, loss = 0.36533323\n",
      "Iteration 265, loss = 0.36490486\n",
      "Iteration 266, loss = 0.36451377\n",
      "Iteration 267, loss = 0.36418312\n",
      "Iteration 268, loss = 0.36384554\n",
      "Iteration 269, loss = 0.36349687\n",
      "Iteration 270, loss = 0.36317003\n",
      "Iteration 271, loss = 0.36279844\n",
      "Iteration 272, loss = 0.36243496\n",
      "Iteration 273, loss = 0.36208925\n",
      "Iteration 274, loss = 0.36168326\n",
      "Iteration 275, loss = 0.36127467\n",
      "Iteration 276, loss = 0.36097983\n",
      "Iteration 277, loss = 0.36078825\n",
      "Iteration 278, loss = 0.36051162\n",
      "Iteration 279, loss = 0.36017750\n",
      "Iteration 280, loss = 0.35983900\n",
      "Iteration 281, loss = 0.35950081\n",
      "Iteration 282, loss = 0.35921242\n",
      "Iteration 283, loss = 0.35894273\n",
      "Iteration 284, loss = 0.35877711\n",
      "Iteration 285, loss = 0.35848645\n",
      "Iteration 286, loss = 0.35809320\n",
      "Iteration 287, loss = 0.35749785\n",
      "Iteration 288, loss = 0.35695291\n",
      "Iteration 289, loss = 0.35643044\n",
      "Iteration 290, loss = 0.35587857\n",
      "Iteration 291, loss = 0.35539289\n",
      "Iteration 292, loss = 0.35496704\n",
      "Iteration 293, loss = 0.35453938\n",
      "Iteration 294, loss = 0.35419639\n",
      "Iteration 295, loss = 0.35392259\n",
      "Iteration 296, loss = 0.35370725\n",
      "Iteration 297, loss = 0.35357244\n",
      "Iteration 298, loss = 0.35341856\n",
      "Iteration 299, loss = 0.35312784\n",
      "Iteration 300, loss = 0.35271661\n",
      "Iteration 301, loss = 0.35231781\n",
      "Iteration 302, loss = 0.35181497\n",
      "Iteration 303, loss = 0.35135658\n",
      "Iteration 304, loss = 0.35096961\n",
      "Iteration 305, loss = 0.35062175\n",
      "Iteration 306, loss = 0.35035842\n",
      "Iteration 307, loss = 0.35020520\n",
      "Iteration 308, loss = 0.35012226\n",
      "Iteration 309, loss = 0.34997511\n",
      "Iteration 310, loss = 0.34965576\n",
      "Iteration 311, loss = 0.34920947\n",
      "Iteration 312, loss = 0.34865880\n",
      "Iteration 313, loss = 0.34812616\n",
      "Iteration 314, loss = 0.34770727\n",
      "Iteration 315, loss = 0.34728807\n",
      "Iteration 316, loss = 0.34691845\n",
      "Iteration 317, loss = 0.34650668\n",
      "Iteration 318, loss = 0.34609954\n",
      "Iteration 319, loss = 0.34573404\n",
      "Iteration 320, loss = 0.34525836\n",
      "Iteration 321, loss = 0.34488133\n",
      "Iteration 322, loss = 0.34465581\n",
      "Iteration 323, loss = 0.34431462\n",
      "Iteration 324, loss = 0.34394926\n",
      "Iteration 325, loss = 0.34372876\n",
      "Iteration 326, loss = 0.34340744\n",
      "Iteration 327, loss = 0.34313243\n",
      "Iteration 328, loss = 0.34295893\n",
      "Iteration 329, loss = 0.34269114\n",
      "Iteration 330, loss = 0.34249233\n",
      "Iteration 331, loss = 0.34233482\n",
      "Iteration 332, loss = 0.34237754\n",
      "Iteration 333, loss = 0.34178518\n",
      "Iteration 334, loss = 0.34144088\n",
      "Iteration 335, loss = 0.34112642\n",
      "Iteration 336, loss = 0.34103514\n",
      "Iteration 337, loss = 0.34102593\n",
      "Iteration 338, loss = 0.34104820\n",
      "Iteration 339, loss = 0.34136245\n",
      "Iteration 340, loss = 0.34180872\n",
      "Iteration 341, loss = 0.34157554\n",
      "Iteration 342, loss = 0.34128855\n",
      "Iteration 343, loss = 0.34070658\n",
      "Iteration 344, loss = 0.34015848\n",
      "Iteration 345, loss = 0.33977119\n",
      "Iteration 346, loss = 0.33933086\n",
      "Iteration 347, loss = 0.33904331\n",
      "Iteration 348, loss = 0.33863417\n",
      "Iteration 349, loss = 0.33820516\n",
      "Iteration 350, loss = 0.33777167\n",
      "Iteration 351, loss = 0.33741181\n",
      "Iteration 352, loss = 0.33712682\n",
      "Iteration 353, loss = 0.33688827\n",
      "Iteration 354, loss = 0.33663498\n",
      "Iteration 355, loss = 0.33653072\n",
      "Iteration 356, loss = 0.33645530\n",
      "Iteration 357, loss = 0.33646728\n",
      "Iteration 358, loss = 0.33665190\n",
      "Iteration 359, loss = 0.33646176\n",
      "Iteration 360, loss = 0.33641988\n",
      "Iteration 361, loss = 0.33618440\n",
      "Iteration 362, loss = 0.33596804\n",
      "Iteration 363, loss = 0.33571181\n",
      "Iteration 364, loss = 0.33553487\n",
      "Iteration 365, loss = 0.33543167\n",
      "Iteration 366, loss = 0.33542965\n",
      "Iteration 367, loss = 0.33508223\n",
      "Iteration 368, loss = 0.33455228\n",
      "Iteration 369, loss = 0.33394140\n",
      "Iteration 370, loss = 0.33340387\n",
      "Iteration 371, loss = 0.33303304\n",
      "Iteration 372, loss = 0.33259492\n",
      "Iteration 373, loss = 0.33226168\n",
      "Iteration 374, loss = 0.33213296\n",
      "Iteration 375, loss = 0.33204354\n",
      "Iteration 376, loss = 0.33174402\n",
      "Iteration 377, loss = 0.33173306\n",
      "Iteration 378, loss = 0.33125038\n",
      "Iteration 379, loss = 0.33072960\n",
      "Iteration 380, loss = 0.33044968\n",
      "Iteration 381, loss = 0.33015641\n",
      "Iteration 382, loss = 0.32969419\n",
      "Iteration 383, loss = 0.32924178\n",
      "Iteration 384, loss = 0.32881630\n",
      "Iteration 385, loss = 0.32844490\n",
      "Iteration 386, loss = 0.32816771\n",
      "Iteration 387, loss = 0.32787841\n",
      "Iteration 388, loss = 0.32757546\n",
      "Iteration 389, loss = 0.32731446\n",
      "Iteration 390, loss = 0.32707962\n",
      "Iteration 391, loss = 0.32675351\n",
      "Iteration 392, loss = 0.32649354\n",
      "Iteration 393, loss = 0.32610348\n",
      "Iteration 394, loss = 0.32576628\n",
      "Iteration 395, loss = 0.32548014\n",
      "Iteration 396, loss = 0.32525890\n",
      "Iteration 397, loss = 0.32512556\n",
      "Iteration 398, loss = 0.32482835\n",
      "Iteration 399, loss = 0.32458845\n",
      "Iteration 400, loss = 0.32433832\n",
      "Iteration 401, loss = 0.32410502\n",
      "Iteration 402, loss = 0.32396859\n",
      "Iteration 403, loss = 0.32391191\n",
      "Iteration 404, loss = 0.32392695\n",
      "Iteration 405, loss = 0.32407482\n",
      "Iteration 406, loss = 0.32439610\n",
      "Iteration 407, loss = 0.32462485\n",
      "Iteration 408, loss = 0.32476074\n",
      "Iteration 409, loss = 0.32518022\n",
      "Iteration 410, loss = 0.32545998\n",
      "Iteration 411, loss = 0.32528875\n",
      "Iteration 412, loss = 0.32541568\n",
      "Iteration 413, loss = 0.32598736\n",
      "Iteration 414, loss = 0.32650509\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71983728\n",
      "Iteration 2, loss = 0.71926183\n",
      "Iteration 3, loss = 0.71829177\n",
      "Iteration 4, loss = 0.71712101\n",
      "Iteration 5, loss = 0.71550192\n",
      "Iteration 6, loss = 0.71375888\n",
      "Iteration 7, loss = 0.71181125\n",
      "Iteration 8, loss = 0.70981159\n",
      "Iteration 9, loss = 0.70773822\n",
      "Iteration 10, loss = 0.70553253\n",
      "Iteration 11, loss = 0.70309762\n",
      "Iteration 12, loss = 0.70061257\n",
      "Iteration 13, loss = 0.69815914\n",
      "Iteration 14, loss = 0.69575077\n",
      "Iteration 15, loss = 0.69317480\n",
      "Iteration 16, loss = 0.69053605\n",
      "Iteration 17, loss = 0.68794558\n",
      "Iteration 18, loss = 0.68541080\n",
      "Iteration 19, loss = 0.68287383\n",
      "Iteration 20, loss = 0.68005533\n",
      "Iteration 21, loss = 0.67722675\n",
      "Iteration 22, loss = 0.67449379\n",
      "Iteration 23, loss = 0.67184832\n",
      "Iteration 24, loss = 0.66927496\n",
      "Iteration 25, loss = 0.66678610\n",
      "Iteration 26, loss = 0.66440035\n",
      "Iteration 27, loss = 0.66210733\n",
      "Iteration 28, loss = 0.65984761\n",
      "Iteration 29, loss = 0.65759399\n",
      "Iteration 30, loss = 0.65548777\n",
      "Iteration 31, loss = 0.65342520\n",
      "Iteration 32, loss = 0.65140777\n",
      "Iteration 33, loss = 0.64934063\n",
      "Iteration 34, loss = 0.64726714\n",
      "Iteration 35, loss = 0.64512369\n",
      "Iteration 36, loss = 0.64294051\n",
      "Iteration 37, loss = 0.64066058\n",
      "Iteration 38, loss = 0.63832243\n",
      "Iteration 39, loss = 0.63601089\n",
      "Iteration 40, loss = 0.63371323\n",
      "Iteration 41, loss = 0.63150609\n",
      "Iteration 42, loss = 0.62934996\n",
      "Iteration 43, loss = 0.62728907\n",
      "Iteration 44, loss = 0.62519517\n",
      "Iteration 45, loss = 0.62310449\n",
      "Iteration 46, loss = 0.62111189\n",
      "Iteration 47, loss = 0.61891932\n",
      "Iteration 48, loss = 0.61673729\n",
      "Iteration 49, loss = 0.61467318\n",
      "Iteration 50, loss = 0.61257674\n",
      "Iteration 51, loss = 0.61037135\n",
      "Iteration 52, loss = 0.60811676\n",
      "Iteration 53, loss = 0.60602500\n",
      "Iteration 54, loss = 0.60381075\n",
      "Iteration 55, loss = 0.60158207\n",
      "Iteration 56, loss = 0.59940441\n",
      "Iteration 57, loss = 0.59716517\n",
      "Iteration 58, loss = 0.59496106\n",
      "Iteration 59, loss = 0.59274079\n",
      "Iteration 60, loss = 0.59046519\n",
      "Iteration 61, loss = 0.58816893\n",
      "Iteration 62, loss = 0.58604710\n",
      "Iteration 63, loss = 0.58373933\n",
      "Iteration 64, loss = 0.58158043\n",
      "Iteration 65, loss = 0.57928976\n",
      "Iteration 66, loss = 0.57696677\n",
      "Iteration 67, loss = 0.57468813\n",
      "Iteration 68, loss = 0.57238803\n",
      "Iteration 69, loss = 0.57005133\n",
      "Iteration 70, loss = 0.56766499\n",
      "Iteration 71, loss = 0.56533285\n",
      "Iteration 72, loss = 0.56295542\n",
      "Iteration 73, loss = 0.56046891\n",
      "Iteration 74, loss = 0.55796335\n",
      "Iteration 75, loss = 0.55566592\n",
      "Iteration 76, loss = 0.55340971\n",
      "Iteration 77, loss = 0.55122443\n",
      "Iteration 78, loss = 0.54899151\n",
      "Iteration 79, loss = 0.54682502\n",
      "Iteration 80, loss = 0.54466391\n",
      "Iteration 81, loss = 0.54264700\n",
      "Iteration 82, loss = 0.54066385\n",
      "Iteration 83, loss = 0.53858832\n",
      "Iteration 84, loss = 0.53650802\n",
      "Iteration 85, loss = 0.53457748\n",
      "Iteration 86, loss = 0.53249476\n",
      "Iteration 87, loss = 0.53040444\n",
      "Iteration 88, loss = 0.52838551\n",
      "Iteration 89, loss = 0.52617712\n",
      "Iteration 90, loss = 0.52408281\n",
      "Iteration 91, loss = 0.52202545\n",
      "Iteration 92, loss = 0.51993208\n",
      "Iteration 93, loss = 0.51806907\n",
      "Iteration 94, loss = 0.51595816\n",
      "Iteration 95, loss = 0.51380444\n",
      "Iteration 96, loss = 0.51158373\n",
      "Iteration 97, loss = 0.50958551\n",
      "Iteration 98, loss = 0.50737039\n",
      "Iteration 99, loss = 0.50524702\n",
      "Iteration 100, loss = 0.50315289\n",
      "Iteration 101, loss = 0.50105061\n",
      "Iteration 102, loss = 0.49899863\n",
      "Iteration 103, loss = 0.49709941\n",
      "Iteration 104, loss = 0.49517937\n",
      "Iteration 105, loss = 0.49327715\n",
      "Iteration 106, loss = 0.49137653\n",
      "Iteration 107, loss = 0.48938588\n",
      "Iteration 108, loss = 0.48730951\n",
      "Iteration 109, loss = 0.48530936\n",
      "Iteration 110, loss = 0.48333215\n",
      "Iteration 111, loss = 0.48141058\n",
      "Iteration 112, loss = 0.47960709\n",
      "Iteration 113, loss = 0.47770476\n",
      "Iteration 114, loss = 0.47573138\n",
      "Iteration 115, loss = 0.47375586\n",
      "Iteration 116, loss = 0.47182566\n",
      "Iteration 117, loss = 0.46985770\n",
      "Iteration 118, loss = 0.46797410\n",
      "Iteration 119, loss = 0.46597306\n",
      "Iteration 120, loss = 0.46400125\n",
      "Iteration 121, loss = 0.46208802\n",
      "Iteration 122, loss = 0.46035437\n",
      "Iteration 123, loss = 0.45876261\n",
      "Iteration 124, loss = 0.45706918\n",
      "Iteration 125, loss = 0.45524056\n",
      "Iteration 126, loss = 0.45340266\n",
      "Iteration 127, loss = 0.45161681\n",
      "Iteration 128, loss = 0.44987248\n",
      "Iteration 129, loss = 0.44812323\n",
      "Iteration 130, loss = 0.44634238\n",
      "Iteration 131, loss = 0.44454423\n",
      "Iteration 132, loss = 0.44291904\n",
      "Iteration 133, loss = 0.44129353\n",
      "Iteration 134, loss = 0.43950644\n",
      "Iteration 135, loss = 0.43783685\n",
      "Iteration 136, loss = 0.43634059\n",
      "Iteration 137, loss = 0.43542509\n",
      "Iteration 138, loss = 0.43459676\n",
      "Iteration 139, loss = 0.43426496\n",
      "Iteration 140, loss = 0.43365395\n",
      "Iteration 141, loss = 0.43284565\n",
      "Iteration 142, loss = 0.43176746\n",
      "Iteration 143, loss = 0.43010585\n",
      "Iteration 144, loss = 0.42836523\n",
      "Iteration 145, loss = 0.42645942\n",
      "Iteration 146, loss = 0.42455094\n",
      "Iteration 147, loss = 0.42254257\n",
      "Iteration 148, loss = 0.42065796\n",
      "Iteration 149, loss = 0.41907251\n",
      "Iteration 150, loss = 0.41764987\n",
      "Iteration 151, loss = 0.41615550\n",
      "Iteration 152, loss = 0.41467220\n",
      "Iteration 153, loss = 0.41334718\n",
      "Iteration 154, loss = 0.41230743\n",
      "Iteration 155, loss = 0.41117757\n",
      "Iteration 156, loss = 0.40996332\n",
      "Iteration 157, loss = 0.40887579\n",
      "Iteration 158, loss = 0.40774770\n",
      "Iteration 159, loss = 0.40657993\n",
      "Iteration 160, loss = 0.40549279\n",
      "Iteration 161, loss = 0.40439956\n",
      "Iteration 162, loss = 0.40322923\n",
      "Iteration 163, loss = 0.40192789\n",
      "Iteration 164, loss = 0.40081177\n",
      "Iteration 165, loss = 0.39994161\n",
      "Iteration 166, loss = 0.39905479\n",
      "Iteration 167, loss = 0.39817751\n",
      "Iteration 168, loss = 0.39736027\n",
      "Iteration 169, loss = 0.39658719\n",
      "Iteration 170, loss = 0.39589099\n",
      "Iteration 171, loss = 0.39509696\n",
      "Iteration 172, loss = 0.39455713\n",
      "Iteration 173, loss = 0.39372680\n",
      "Iteration 174, loss = 0.39289820\n",
      "Iteration 175, loss = 0.39200020\n",
      "Iteration 176, loss = 0.39113209\n",
      "Iteration 177, loss = 0.39011275\n",
      "Iteration 178, loss = 0.38905594\n",
      "Iteration 179, loss = 0.38791043\n",
      "Iteration 180, loss = 0.38682915\n",
      "Iteration 181, loss = 0.38574796\n",
      "Iteration 182, loss = 0.38461125\n",
      "Iteration 183, loss = 0.38350517\n",
      "Iteration 184, loss = 0.38242948\n",
      "Iteration 185, loss = 0.38130152\n",
      "Iteration 186, loss = 0.38020661\n",
      "Iteration 187, loss = 0.37931219\n",
      "Iteration 188, loss = 0.37836359\n",
      "Iteration 189, loss = 0.37766582\n",
      "Iteration 190, loss = 0.37710918\n",
      "Iteration 191, loss = 0.37650433\n",
      "Iteration 192, loss = 0.37588232\n",
      "Iteration 193, loss = 0.37521704\n",
      "Iteration 194, loss = 0.37453849\n",
      "Iteration 195, loss = 0.37384998\n",
      "Iteration 196, loss = 0.37330436\n",
      "Iteration 197, loss = 0.37277179\n",
      "Iteration 198, loss = 0.37217901\n",
      "Iteration 199, loss = 0.37162581\n",
      "Iteration 200, loss = 0.37106840\n",
      "Iteration 201, loss = 0.37044987\n",
      "Iteration 202, loss = 0.36977350\n",
      "Iteration 203, loss = 0.36907093\n",
      "Iteration 204, loss = 0.36827934\n",
      "Iteration 205, loss = 0.36749931\n",
      "Iteration 206, loss = 0.36675332\n",
      "Iteration 207, loss = 0.36604929\n",
      "Iteration 208, loss = 0.36534803\n",
      "Iteration 209, loss = 0.36465434\n",
      "Iteration 210, loss = 0.36395058\n",
      "Iteration 211, loss = 0.36328476\n",
      "Iteration 212, loss = 0.36278551\n",
      "Iteration 213, loss = 0.36217703\n",
      "Iteration 214, loss = 0.36165560\n",
      "Iteration 215, loss = 0.36114330\n",
      "Iteration 216, loss = 0.36064966\n",
      "Iteration 217, loss = 0.36045807\n",
      "Iteration 218, loss = 0.36024045\n",
      "Iteration 219, loss = 0.35972208\n",
      "Iteration 220, loss = 0.35931110\n",
      "Iteration 221, loss = 0.35885454\n",
      "Iteration 222, loss = 0.35843306\n",
      "Iteration 223, loss = 0.35792535\n",
      "Iteration 224, loss = 0.35732301\n",
      "Iteration 225, loss = 0.35647974\n",
      "Iteration 226, loss = 0.35559285\n",
      "Iteration 227, loss = 0.35477044\n",
      "Iteration 228, loss = 0.35407890\n",
      "Iteration 229, loss = 0.35350508\n",
      "Iteration 230, loss = 0.35293535\n",
      "Iteration 231, loss = 0.35248389\n",
      "Iteration 232, loss = 0.35202558\n",
      "Iteration 233, loss = 0.35157530\n",
      "Iteration 234, loss = 0.35116112\n",
      "Iteration 235, loss = 0.35074491\n",
      "Iteration 236, loss = 0.35040629\n",
      "Iteration 237, loss = 0.35010809\n",
      "Iteration 238, loss = 0.34988125\n",
      "Iteration 239, loss = 0.34941124\n",
      "Iteration 240, loss = 0.34904046\n",
      "Iteration 241, loss = 0.34859532\n",
      "Iteration 242, loss = 0.34808054\n",
      "Iteration 243, loss = 0.34741778\n",
      "Iteration 244, loss = 0.34671416\n",
      "Iteration 245, loss = 0.34613800\n",
      "Iteration 246, loss = 0.34563111\n",
      "Iteration 247, loss = 0.34519758\n",
      "Iteration 248, loss = 0.34492125\n",
      "Iteration 249, loss = 0.34480068\n",
      "Iteration 250, loss = 0.34479203\n",
      "Iteration 251, loss = 0.34484527\n",
      "Iteration 252, loss = 0.34478493\n",
      "Iteration 253, loss = 0.34510321\n",
      "Iteration 254, loss = 0.34482427\n",
      "Iteration 255, loss = 0.34466787\n",
      "Iteration 256, loss = 0.34485947\n",
      "Iteration 257, loss = 0.34531831\n",
      "Iteration 258, loss = 0.34552503\n",
      "Iteration 259, loss = 0.34548788\n",
      "Iteration 260, loss = 0.34512396\n",
      "Iteration 261, loss = 0.34465259\n",
      "Iteration 262, loss = 0.34406713\n",
      "Iteration 263, loss = 0.34313443\n",
      "Iteration 264, loss = 0.34203868\n",
      "Iteration 265, loss = 0.34109235\n",
      "Iteration 266, loss = 0.34003092\n",
      "Iteration 267, loss = 0.33907205\n",
      "Iteration 268, loss = 0.33830433\n",
      "Iteration 269, loss = 0.33757004\n",
      "Iteration 270, loss = 0.33690354\n",
      "Iteration 271, loss = 0.33626764\n",
      "Iteration 272, loss = 0.33577635\n",
      "Iteration 273, loss = 0.33528385\n",
      "Iteration 274, loss = 0.33477116\n",
      "Iteration 275, loss = 0.33428999\n",
      "Iteration 276, loss = 0.33381595\n",
      "Iteration 277, loss = 0.33334957\n",
      "Iteration 278, loss = 0.33290257\n",
      "Iteration 279, loss = 0.33248167\n",
      "Iteration 280, loss = 0.33197582\n",
      "Iteration 281, loss = 0.33152099\n",
      "Iteration 282, loss = 0.33111226\n",
      "Iteration 283, loss = 0.33066378\n",
      "Iteration 284, loss = 0.33035025\n",
      "Iteration 285, loss = 0.32988523\n",
      "Iteration 286, loss = 0.32949878\n",
      "Iteration 287, loss = 0.32904132\n",
      "Iteration 288, loss = 0.32858460\n",
      "Iteration 289, loss = 0.32832997\n",
      "Iteration 290, loss = 0.32816251\n",
      "Iteration 291, loss = 0.32787765\n",
      "Iteration 292, loss = 0.32760249\n",
      "Iteration 293, loss = 0.32711558\n",
      "Iteration 294, loss = 0.32666717\n",
      "Iteration 295, loss = 0.32613759\n",
      "Iteration 296, loss = 0.32566268\n",
      "Iteration 297, loss = 0.32502565\n",
      "Iteration 298, loss = 0.32447095\n",
      "Iteration 299, loss = 0.32399099\n",
      "Iteration 300, loss = 0.32357362\n",
      "Iteration 301, loss = 0.32314082\n",
      "Iteration 302, loss = 0.32271469\n",
      "Iteration 303, loss = 0.32222402\n",
      "Iteration 304, loss = 0.32186716\n",
      "Iteration 305, loss = 0.32144599\n",
      "Iteration 306, loss = 0.32099305\n",
      "Iteration 307, loss = 0.32057962\n",
      "Iteration 308, loss = 0.32013712\n",
      "Iteration 309, loss = 0.31968118\n",
      "Iteration 310, loss = 0.31926436\n",
      "Iteration 311, loss = 0.31890330\n",
      "Iteration 312, loss = 0.31861420\n",
      "Iteration 313, loss = 0.31825476\n",
      "Iteration 314, loss = 0.31790162\n",
      "Iteration 315, loss = 0.31758862\n",
      "Iteration 316, loss = 0.31733096\n",
      "Iteration 317, loss = 0.31706347\n",
      "Iteration 318, loss = 0.31681341\n",
      "Iteration 319, loss = 0.31678258\n",
      "Iteration 320, loss = 0.31657116\n",
      "Iteration 321, loss = 0.31654466\n",
      "Iteration 322, loss = 0.31659692\n",
      "Iteration 323, loss = 0.31693193\n",
      "Iteration 324, loss = 0.31743206\n",
      "Iteration 325, loss = 0.31753498\n",
      "Iteration 326, loss = 0.31742605\n",
      "Iteration 327, loss = 0.31709242\n",
      "Iteration 328, loss = 0.31623455\n",
      "Iteration 329, loss = 0.31530827\n",
      "Iteration 330, loss = 0.31467318\n",
      "Iteration 331, loss = 0.31438407\n",
      "Iteration 332, loss = 0.31407761\n",
      "Iteration 333, loss = 0.31346790\n",
      "Iteration 334, loss = 0.31286668\n",
      "Iteration 335, loss = 0.31234436\n",
      "Iteration 336, loss = 0.31196944\n",
      "Iteration 337, loss = 0.31137642\n",
      "Iteration 338, loss = 0.31085577\n",
      "Iteration 339, loss = 0.31032498\n",
      "Iteration 340, loss = 0.30983597\n",
      "Iteration 341, loss = 0.30938982\n",
      "Iteration 342, loss = 0.30911953\n",
      "Iteration 343, loss = 0.30882471\n",
      "Iteration 344, loss = 0.30855865\n",
      "Iteration 345, loss = 0.30829847\n",
      "Iteration 346, loss = 0.30803534\n",
      "Iteration 347, loss = 0.30786742\n",
      "Iteration 348, loss = 0.30769709\n",
      "Iteration 349, loss = 0.30771573\n",
      "Iteration 350, loss = 0.30741937\n",
      "Iteration 351, loss = 0.30704578\n",
      "Iteration 352, loss = 0.30639313\n",
      "Iteration 353, loss = 0.30598148\n",
      "Iteration 354, loss = 0.30567275\n",
      "Iteration 355, loss = 0.30555071\n",
      "Iteration 356, loss = 0.30495420\n",
      "Iteration 357, loss = 0.30439244\n",
      "Iteration 358, loss = 0.30394085\n",
      "Iteration 359, loss = 0.30321935\n",
      "Iteration 360, loss = 0.30279602\n",
      "Iteration 361, loss = 0.30256684\n",
      "Iteration 362, loss = 0.30226000\n",
      "Iteration 363, loss = 0.30198797\n",
      "Iteration 364, loss = 0.30162694\n",
      "Iteration 365, loss = 0.30126540\n",
      "Iteration 366, loss = 0.30096918\n",
      "Iteration 367, loss = 0.30062882\n",
      "Iteration 368, loss = 0.30037627\n",
      "Iteration 369, loss = 0.30016172\n",
      "Iteration 370, loss = 0.29988776\n",
      "Iteration 371, loss = 0.29962953\n",
      "Iteration 372, loss = 0.29938368\n",
      "Iteration 373, loss = 0.29903913\n",
      "Iteration 374, loss = 0.29870876\n",
      "Iteration 375, loss = 0.29827552\n",
      "Iteration 376, loss = 0.29785090\n",
      "Iteration 377, loss = 0.29744101\n",
      "Iteration 378, loss = 0.29707637\n",
      "Iteration 379, loss = 0.29670570\n",
      "Iteration 380, loss = 0.29624052\n",
      "Iteration 381, loss = 0.29587622\n",
      "Iteration 382, loss = 0.29550316\n",
      "Iteration 383, loss = 0.29517269\n",
      "Iteration 384, loss = 0.29487316\n",
      "Iteration 385, loss = 0.29463285\n",
      "Iteration 386, loss = 0.29434833\n",
      "Iteration 387, loss = 0.29401548\n",
      "Iteration 388, loss = 0.29370040\n",
      "Iteration 389, loss = 0.29337071\n",
      "Iteration 390, loss = 0.29303218\n",
      "Iteration 391, loss = 0.29271352\n",
      "Iteration 392, loss = 0.29261582\n",
      "Iteration 393, loss = 0.29210831\n",
      "Iteration 394, loss = 0.29158284\n",
      "Iteration 395, loss = 0.29112222\n",
      "Iteration 396, loss = 0.29067636\n",
      "Iteration 397, loss = 0.29026024\n",
      "Iteration 398, loss = 0.28993990\n",
      "Iteration 399, loss = 0.28953800\n",
      "Iteration 400, loss = 0.28919102\n",
      "Iteration 401, loss = 0.28890136\n",
      "Iteration 402, loss = 0.28864801\n",
      "Iteration 403, loss = 0.28834180\n",
      "Iteration 404, loss = 0.28802084\n",
      "Iteration 405, loss = 0.28768165\n",
      "Iteration 406, loss = 0.28739941\n",
      "Iteration 407, loss = 0.28712232\n",
      "Iteration 408, loss = 0.28693709\n",
      "Iteration 409, loss = 0.28664853\n",
      "Iteration 410, loss = 0.28628930\n",
      "Iteration 411, loss = 0.28593889\n",
      "Iteration 412, loss = 0.28559637\n",
      "Iteration 413, loss = 0.28522845\n",
      "Iteration 414, loss = 0.28472888\n",
      "Iteration 415, loss = 0.28444824\n",
      "Iteration 416, loss = 0.28394140\n",
      "Iteration 417, loss = 0.28359673\n",
      "Iteration 418, loss = 0.28323875\n",
      "Iteration 419, loss = 0.28295264\n",
      "Iteration 420, loss = 0.28265396\n",
      "Iteration 421, loss = 0.28239266\n",
      "Iteration 422, loss = 0.28214721\n",
      "Iteration 423, loss = 0.28189556\n",
      "Iteration 424, loss = 0.28159045\n",
      "Iteration 425, loss = 0.28132570\n",
      "Iteration 426, loss = 0.28105040\n",
      "Iteration 427, loss = 0.28079766\n",
      "Iteration 428, loss = 0.28087513\n",
      "Iteration 429, loss = 0.28109164\n",
      "Iteration 430, loss = 0.28054683\n",
      "Iteration 431, loss = 0.28013380\n",
      "Iteration 432, loss = 0.27971791\n",
      "Iteration 433, loss = 0.27944622\n",
      "Iteration 434, loss = 0.27917944\n",
      "Iteration 435, loss = 0.27910386\n",
      "Iteration 436, loss = 0.27901975\n",
      "Iteration 437, loss = 0.27893527\n",
      "Iteration 438, loss = 0.27883901\n",
      "Iteration 439, loss = 0.27859956\n",
      "Iteration 440, loss = 0.27832766\n",
      "Iteration 441, loss = 0.27807031\n",
      "Iteration 442, loss = 0.27749935\n",
      "Iteration 443, loss = 0.27701157\n",
      "Iteration 444, loss = 0.27657573\n",
      "Iteration 445, loss = 0.27616503\n",
      "Iteration 446, loss = 0.27580753\n",
      "Iteration 447, loss = 0.27544649\n",
      "Iteration 448, loss = 0.27507243\n",
      "Iteration 449, loss = 0.27480251\n",
      "Iteration 450, loss = 0.27463160\n",
      "Iteration 451, loss = 0.27433842\n",
      "Iteration 452, loss = 0.27401822\n",
      "Iteration 453, loss = 0.27360025\n",
      "Iteration 454, loss = 0.27323851\n",
      "Iteration 455, loss = 0.27296990\n",
      "Iteration 456, loss = 0.27249391\n",
      "Iteration 457, loss = 0.27197613\n",
      "Iteration 458, loss = 0.27155843\n",
      "Iteration 459, loss = 0.27122668\n",
      "Iteration 460, loss = 0.27111439\n",
      "Iteration 461, loss = 0.27078748\n",
      "Iteration 462, loss = 0.27057256\n",
      "Iteration 463, loss = 0.27037802\n",
      "Iteration 464, loss = 0.27018102\n",
      "Iteration 465, loss = 0.26997674\n",
      "Iteration 466, loss = 0.26985918\n",
      "Iteration 467, loss = 0.26966134\n",
      "Iteration 468, loss = 0.26943247\n",
      "Iteration 469, loss = 0.26927937\n",
      "Iteration 470, loss = 0.26916291\n",
      "Iteration 471, loss = 0.26906918\n",
      "Iteration 472, loss = 0.26880336\n",
      "Iteration 473, loss = 0.26869018\n",
      "Iteration 474, loss = 0.26824119\n",
      "Iteration 475, loss = 0.26789721\n",
      "Iteration 476, loss = 0.26755871\n",
      "Iteration 477, loss = 0.26712242\n",
      "Iteration 478, loss = 0.26653935\n",
      "Iteration 479, loss = 0.26603247\n",
      "Iteration 480, loss = 0.26563413\n",
      "Iteration 481, loss = 0.26531942\n",
      "Iteration 482, loss = 0.26512833\n",
      "Iteration 483, loss = 0.26485713\n",
      "Iteration 484, loss = 0.26455898\n",
      "Iteration 485, loss = 0.26430993\n",
      "Iteration 486, loss = 0.26394701\n",
      "Iteration 487, loss = 0.26364083\n",
      "Iteration 488, loss = 0.26331228\n",
      "Iteration 489, loss = 0.26300907\n",
      "Iteration 490, loss = 0.26263628\n",
      "Iteration 491, loss = 0.26221515\n",
      "Iteration 492, loss = 0.26209943\n",
      "Iteration 493, loss = 0.26200452\n",
      "Iteration 494, loss = 0.26184201\n",
      "Iteration 495, loss = 0.26156368\n",
      "Iteration 496, loss = 0.26141889\n",
      "Iteration 497, loss = 0.26119825\n",
      "Iteration 498, loss = 0.26074486\n",
      "Iteration 499, loss = 0.26022334\n",
      "Iteration 500, loss = 0.25968489\n",
      "Iteration 1, loss = 0.71897185\n",
      "Iteration 2, loss = 0.71841692\n",
      "Iteration 3, loss = 0.71744589\n",
      "Iteration 4, loss = 0.71632847\n",
      "Iteration 5, loss = 0.71490255\n",
      "Iteration 6, loss = 0.71332678\n",
      "Iteration 7, loss = 0.71162503\n",
      "Iteration 8, loss = 0.70966509\n",
      "Iteration 9, loss = 0.70777842\n",
      "Iteration 10, loss = 0.70576786\n",
      "Iteration 11, loss = 0.70345031\n",
      "Iteration 12, loss = 0.70103370\n",
      "Iteration 13, loss = 0.69859125\n",
      "Iteration 14, loss = 0.69616405\n",
      "Iteration 15, loss = 0.69368481\n",
      "Iteration 16, loss = 0.69117191\n",
      "Iteration 17, loss = 0.68864325\n",
      "Iteration 18, loss = 0.68610317\n",
      "Iteration 19, loss = 0.68355867\n",
      "Iteration 20, loss = 0.68083284\n",
      "Iteration 21, loss = 0.67808040\n",
      "Iteration 22, loss = 0.67545003\n",
      "Iteration 23, loss = 0.67286394\n",
      "Iteration 24, loss = 0.67039565\n",
      "Iteration 25, loss = 0.66808873\n",
      "Iteration 26, loss = 0.66574039\n",
      "Iteration 27, loss = 0.66346088\n",
      "Iteration 28, loss = 0.66126039\n",
      "Iteration 29, loss = 0.65903655\n",
      "Iteration 30, loss = 0.65689100\n",
      "Iteration 31, loss = 0.65485559\n",
      "Iteration 32, loss = 0.65286824\n",
      "Iteration 33, loss = 0.65091646\n",
      "Iteration 34, loss = 0.64899187\n",
      "Iteration 35, loss = 0.64683997\n",
      "Iteration 36, loss = 0.64472086\n",
      "Iteration 37, loss = 0.64262515\n",
      "Iteration 38, loss = 0.64040769\n",
      "Iteration 39, loss = 0.63825170\n",
      "Iteration 40, loss = 0.63606646\n",
      "Iteration 41, loss = 0.63389799\n",
      "Iteration 42, loss = 0.63177931\n",
      "Iteration 43, loss = 0.62963548\n",
      "Iteration 44, loss = 0.62749352\n",
      "Iteration 45, loss = 0.62533840\n",
      "Iteration 46, loss = 0.62340187\n",
      "Iteration 47, loss = 0.62139958\n",
      "Iteration 48, loss = 0.61934563\n",
      "Iteration 49, loss = 0.61740042\n",
      "Iteration 50, loss = 0.61540046\n",
      "Iteration 51, loss = 0.61335877\n",
      "Iteration 52, loss = 0.61135339\n",
      "Iteration 53, loss = 0.60957529\n",
      "Iteration 54, loss = 0.60757065\n",
      "Iteration 55, loss = 0.60558556\n",
      "Iteration 56, loss = 0.60349398\n",
      "Iteration 57, loss = 0.60152920\n",
      "Iteration 58, loss = 0.59936841\n",
      "Iteration 59, loss = 0.59722652\n",
      "Iteration 60, loss = 0.59504409\n",
      "Iteration 61, loss = 0.59285393\n",
      "Iteration 62, loss = 0.59060438\n",
      "Iteration 63, loss = 0.58824495\n",
      "Iteration 64, loss = 0.58595915\n",
      "Iteration 65, loss = 0.58364893\n",
      "Iteration 66, loss = 0.58134742\n",
      "Iteration 67, loss = 0.57917308\n",
      "Iteration 68, loss = 0.57698764\n",
      "Iteration 69, loss = 0.57483406\n",
      "Iteration 70, loss = 0.57256214\n",
      "Iteration 71, loss = 0.57028002\n",
      "Iteration 72, loss = 0.56804632\n",
      "Iteration 73, loss = 0.56583606\n",
      "Iteration 74, loss = 0.56363440\n",
      "Iteration 75, loss = 0.56173825\n",
      "Iteration 76, loss = 0.55974326\n",
      "Iteration 77, loss = 0.55772497\n",
      "Iteration 78, loss = 0.55582140\n",
      "Iteration 79, loss = 0.55403054\n",
      "Iteration 80, loss = 0.55203436\n",
      "Iteration 81, loss = 0.55012667\n",
      "Iteration 82, loss = 0.54814019\n",
      "Iteration 83, loss = 0.54605465\n",
      "Iteration 84, loss = 0.54398749\n",
      "Iteration 85, loss = 0.54209538\n",
      "Iteration 86, loss = 0.54011396\n",
      "Iteration 87, loss = 0.53811375\n",
      "Iteration 88, loss = 0.53616634\n",
      "Iteration 89, loss = 0.53415577\n",
      "Iteration 90, loss = 0.53226610\n",
      "Iteration 91, loss = 0.53041540\n",
      "Iteration 92, loss = 0.52849975\n",
      "Iteration 93, loss = 0.52690044\n",
      "Iteration 94, loss = 0.52498860\n",
      "Iteration 95, loss = 0.52300171\n",
      "Iteration 96, loss = 0.52094299\n",
      "Iteration 97, loss = 0.51910774\n",
      "Iteration 98, loss = 0.51700576\n",
      "Iteration 99, loss = 0.51501303\n",
      "Iteration 100, loss = 0.51304419\n",
      "Iteration 101, loss = 0.51108953\n",
      "Iteration 102, loss = 0.50915741\n",
      "Iteration 103, loss = 0.50714408\n",
      "Iteration 104, loss = 0.50513644\n",
      "Iteration 105, loss = 0.50322125\n",
      "Iteration 106, loss = 0.50139961\n",
      "Iteration 107, loss = 0.49954548\n",
      "Iteration 108, loss = 0.49762465\n",
      "Iteration 109, loss = 0.49583556\n",
      "Iteration 110, loss = 0.49408839\n",
      "Iteration 111, loss = 0.49232825\n",
      "Iteration 112, loss = 0.49070315\n",
      "Iteration 113, loss = 0.48899162\n",
      "Iteration 114, loss = 0.48725130\n",
      "Iteration 115, loss = 0.48553314\n",
      "Iteration 116, loss = 0.48378162\n",
      "Iteration 117, loss = 0.48196270\n",
      "Iteration 118, loss = 0.48019799\n",
      "Iteration 119, loss = 0.47841128\n",
      "Iteration 120, loss = 0.47662005\n",
      "Iteration 121, loss = 0.47490735\n",
      "Iteration 122, loss = 0.47348544\n",
      "Iteration 123, loss = 0.47218000\n",
      "Iteration 124, loss = 0.47075643\n",
      "Iteration 125, loss = 0.46914674\n",
      "Iteration 126, loss = 0.46751542\n",
      "Iteration 127, loss = 0.46606556\n",
      "Iteration 128, loss = 0.46460298\n",
      "Iteration 129, loss = 0.46309132\n",
      "Iteration 130, loss = 0.46147803\n",
      "Iteration 131, loss = 0.45978324\n",
      "Iteration 132, loss = 0.45821572\n",
      "Iteration 133, loss = 0.45657417\n",
      "Iteration 134, loss = 0.45484429\n",
      "Iteration 135, loss = 0.45318963\n",
      "Iteration 136, loss = 0.45173849\n",
      "Iteration 137, loss = 0.45075008\n",
      "Iteration 138, loss = 0.44985563\n",
      "Iteration 139, loss = 0.44906354\n",
      "Iteration 140, loss = 0.44808375\n",
      "Iteration 141, loss = 0.44697744\n",
      "Iteration 142, loss = 0.44571068\n",
      "Iteration 143, loss = 0.44434763\n",
      "Iteration 144, loss = 0.44289886\n",
      "Iteration 145, loss = 0.44137554\n",
      "Iteration 146, loss = 0.43995499\n",
      "Iteration 147, loss = 0.43832970\n",
      "Iteration 148, loss = 0.43691099\n",
      "Iteration 149, loss = 0.43566452\n",
      "Iteration 150, loss = 0.43452149\n",
      "Iteration 151, loss = 0.43330687\n",
      "Iteration 152, loss = 0.43206211\n",
      "Iteration 153, loss = 0.43102706\n",
      "Iteration 154, loss = 0.43042027\n",
      "Iteration 155, loss = 0.42955600\n",
      "Iteration 156, loss = 0.42843894\n",
      "Iteration 157, loss = 0.42746471\n",
      "Iteration 158, loss = 0.42642764\n",
      "Iteration 159, loss = 0.42529439\n",
      "Iteration 160, loss = 0.42418877\n",
      "Iteration 161, loss = 0.42306349\n",
      "Iteration 162, loss = 0.42183617\n",
      "Iteration 163, loss = 0.42067928\n",
      "Iteration 164, loss = 0.41968798\n",
      "Iteration 165, loss = 0.41899870\n",
      "Iteration 166, loss = 0.41818208\n",
      "Iteration 167, loss = 0.41738101\n",
      "Iteration 168, loss = 0.41659345\n",
      "Iteration 169, loss = 0.41594588\n",
      "Iteration 170, loss = 0.41505883\n",
      "Iteration 171, loss = 0.41408993\n",
      "Iteration 172, loss = 0.41356780\n",
      "Iteration 173, loss = 0.41274279\n",
      "Iteration 174, loss = 0.41185733\n",
      "Iteration 175, loss = 0.41111682\n",
      "Iteration 176, loss = 0.41013786\n",
      "Iteration 177, loss = 0.40901139\n",
      "Iteration 178, loss = 0.40784758\n",
      "Iteration 179, loss = 0.40670828\n",
      "Iteration 180, loss = 0.40558448\n",
      "Iteration 181, loss = 0.40446228\n",
      "Iteration 182, loss = 0.40326822\n",
      "Iteration 183, loss = 0.40209572\n",
      "Iteration 184, loss = 0.40095278\n",
      "Iteration 185, loss = 0.39978463\n",
      "Iteration 186, loss = 0.39863178\n",
      "Iteration 187, loss = 0.39768519\n",
      "Iteration 188, loss = 0.39674039\n",
      "Iteration 189, loss = 0.39609150\n",
      "Iteration 190, loss = 0.39549798\n",
      "Iteration 191, loss = 0.39489796\n",
      "Iteration 192, loss = 0.39420164\n",
      "Iteration 193, loss = 0.39361917\n",
      "Iteration 194, loss = 0.39303661\n",
      "Iteration 195, loss = 0.39255291\n",
      "Iteration 196, loss = 0.39234590\n",
      "Iteration 197, loss = 0.39230095\n",
      "Iteration 198, loss = 0.39190955\n",
      "Iteration 199, loss = 0.39163033\n",
      "Iteration 200, loss = 0.39143604\n",
      "Iteration 201, loss = 0.39098062\n",
      "Iteration 202, loss = 0.39027157\n",
      "Iteration 203, loss = 0.38943566\n",
      "Iteration 204, loss = 0.38823472\n",
      "Iteration 205, loss = 0.38699122\n",
      "Iteration 206, loss = 0.38582185\n",
      "Iteration 207, loss = 0.38479633\n",
      "Iteration 208, loss = 0.38391345\n",
      "Iteration 209, loss = 0.38309897\n",
      "Iteration 210, loss = 0.38234303\n",
      "Iteration 211, loss = 0.38167363\n",
      "Iteration 212, loss = 0.38109559\n",
      "Iteration 213, loss = 0.38043316\n",
      "Iteration 214, loss = 0.37983666\n",
      "Iteration 215, loss = 0.37928557\n",
      "Iteration 216, loss = 0.37872088\n",
      "Iteration 217, loss = 0.37832528\n",
      "Iteration 218, loss = 0.37788561\n",
      "Iteration 219, loss = 0.37730172\n",
      "Iteration 220, loss = 0.37680648\n",
      "Iteration 221, loss = 0.37628994\n",
      "Iteration 222, loss = 0.37598212\n",
      "Iteration 223, loss = 0.37559721\n",
      "Iteration 224, loss = 0.37511750\n",
      "Iteration 225, loss = 0.37438018\n",
      "Iteration 226, loss = 0.37353225\n",
      "Iteration 227, loss = 0.37272903\n",
      "Iteration 228, loss = 0.37200617\n",
      "Iteration 229, loss = 0.37142147\n",
      "Iteration 230, loss = 0.37089411\n",
      "Iteration 231, loss = 0.37042095\n",
      "Iteration 232, loss = 0.36997350\n",
      "Iteration 233, loss = 0.36954362\n",
      "Iteration 234, loss = 0.36915532\n",
      "Iteration 235, loss = 0.36877833\n",
      "Iteration 236, loss = 0.36835096\n",
      "Iteration 237, loss = 0.36805612\n",
      "Iteration 238, loss = 0.36782568\n",
      "Iteration 239, loss = 0.36745157\n",
      "Iteration 240, loss = 0.36712051\n",
      "Iteration 241, loss = 0.36679444\n",
      "Iteration 242, loss = 0.36638615\n",
      "Iteration 243, loss = 0.36590372\n",
      "Iteration 244, loss = 0.36532132\n",
      "Iteration 245, loss = 0.36471312\n",
      "Iteration 246, loss = 0.36417496\n",
      "Iteration 247, loss = 0.36362864\n",
      "Iteration 248, loss = 0.36313865\n",
      "Iteration 249, loss = 0.36263431\n",
      "Iteration 250, loss = 0.36223560\n",
      "Iteration 251, loss = 0.36183348\n",
      "Iteration 252, loss = 0.36137875\n",
      "Iteration 253, loss = 0.36106664\n",
      "Iteration 254, loss = 0.36063701\n",
      "Iteration 255, loss = 0.36027372\n",
      "Iteration 256, loss = 0.35999357\n",
      "Iteration 257, loss = 0.35991522\n",
      "Iteration 258, loss = 0.35977868\n",
      "Iteration 259, loss = 0.35957260\n",
      "Iteration 260, loss = 0.35924701\n",
      "Iteration 261, loss = 0.35891237\n",
      "Iteration 262, loss = 0.35865596\n",
      "Iteration 263, loss = 0.35823453\n",
      "Iteration 264, loss = 0.35765958\n",
      "Iteration 265, loss = 0.35714035\n",
      "Iteration 266, loss = 0.35650816\n",
      "Iteration 267, loss = 0.35589965\n",
      "Iteration 268, loss = 0.35531271\n",
      "Iteration 269, loss = 0.35475986\n",
      "Iteration 270, loss = 0.35432308\n",
      "Iteration 271, loss = 0.35383456\n",
      "Iteration 272, loss = 0.35346299\n",
      "Iteration 273, loss = 0.35301390\n",
      "Iteration 274, loss = 0.35265916\n",
      "Iteration 275, loss = 0.35233535\n",
      "Iteration 276, loss = 0.35198703\n",
      "Iteration 277, loss = 0.35163570\n",
      "Iteration 278, loss = 0.35127731\n",
      "Iteration 279, loss = 0.35095118\n",
      "Iteration 280, loss = 0.35055510\n",
      "Iteration 281, loss = 0.35021443\n",
      "Iteration 282, loss = 0.34988354\n",
      "Iteration 283, loss = 0.34953993\n",
      "Iteration 284, loss = 0.34932067\n",
      "Iteration 285, loss = 0.34902491\n",
      "Iteration 286, loss = 0.34875608\n",
      "Iteration 287, loss = 0.34842490\n",
      "Iteration 288, loss = 0.34807286\n",
      "Iteration 289, loss = 0.34799042\n",
      "Iteration 290, loss = 0.34795444\n",
      "Iteration 291, loss = 0.34797291\n",
      "Iteration 292, loss = 0.34796297\n",
      "Iteration 293, loss = 0.34767659\n",
      "Iteration 294, loss = 0.34745915\n",
      "Iteration 295, loss = 0.34677697\n",
      "Iteration 296, loss = 0.34620340\n",
      "Iteration 297, loss = 0.34549261\n",
      "Iteration 298, loss = 0.34492716\n",
      "Iteration 299, loss = 0.34444245\n",
      "Iteration 300, loss = 0.34394506\n",
      "Iteration 301, loss = 0.34347390\n",
      "Iteration 302, loss = 0.34300658\n",
      "Iteration 303, loss = 0.34252372\n",
      "Iteration 304, loss = 0.34206331\n",
      "Iteration 305, loss = 0.34161839\n",
      "Iteration 306, loss = 0.34117582\n",
      "Iteration 307, loss = 0.34068849\n",
      "Iteration 308, loss = 0.34023795\n",
      "Iteration 309, loss = 0.34004543\n",
      "Iteration 310, loss = 0.33975330\n",
      "Iteration 311, loss = 0.33945257\n",
      "Iteration 312, loss = 0.33915207\n",
      "Iteration 313, loss = 0.33888264\n",
      "Iteration 314, loss = 0.33872945\n",
      "Iteration 315, loss = 0.33838273\n",
      "Iteration 316, loss = 0.33806096\n",
      "Iteration 317, loss = 0.33773755\n",
      "Iteration 318, loss = 0.33740295\n",
      "Iteration 319, loss = 0.33705403\n",
      "Iteration 320, loss = 0.33669752\n",
      "Iteration 321, loss = 0.33641647\n",
      "Iteration 322, loss = 0.33609604\n",
      "Iteration 323, loss = 0.33588895\n",
      "Iteration 324, loss = 0.33576768\n",
      "Iteration 325, loss = 0.33559964\n",
      "Iteration 326, loss = 0.33538659\n",
      "Iteration 327, loss = 0.33521880\n",
      "Iteration 328, loss = 0.33479758\n",
      "Iteration 329, loss = 0.33444627\n",
      "Iteration 330, loss = 0.33413255\n",
      "Iteration 331, loss = 0.33405755\n",
      "Iteration 332, loss = 0.33392282\n",
      "Iteration 333, loss = 0.33358826\n",
      "Iteration 334, loss = 0.33322381\n",
      "Iteration 335, loss = 0.33288952\n",
      "Iteration 336, loss = 0.33267733\n",
      "Iteration 337, loss = 0.33243412\n",
      "Iteration 338, loss = 0.33227198\n",
      "Iteration 339, loss = 0.33191356\n",
      "Iteration 340, loss = 0.33156680\n",
      "Iteration 341, loss = 0.33122269\n",
      "Iteration 342, loss = 0.33097624\n",
      "Iteration 343, loss = 0.33082162\n",
      "Iteration 344, loss = 0.33070467\n",
      "Iteration 345, loss = 0.33058891\n",
      "Iteration 346, loss = 0.33029068\n",
      "Iteration 347, loss = 0.33011211\n",
      "Iteration 348, loss = 0.32980187\n",
      "Iteration 349, loss = 0.32969039\n",
      "Iteration 350, loss = 0.32935395\n",
      "Iteration 351, loss = 0.32897877\n",
      "Iteration 352, loss = 0.32831714\n",
      "Iteration 353, loss = 0.32779706\n",
      "Iteration 354, loss = 0.32738269\n",
      "Iteration 355, loss = 0.32696383\n",
      "Iteration 356, loss = 0.32636743\n",
      "Iteration 357, loss = 0.32587748\n",
      "Iteration 358, loss = 0.32542416\n",
      "Iteration 359, loss = 0.32502092\n",
      "Iteration 360, loss = 0.32483379\n",
      "Iteration 361, loss = 0.32483395\n",
      "Iteration 362, loss = 0.32460980\n",
      "Iteration 363, loss = 0.32433324\n",
      "Iteration 364, loss = 0.32419966\n",
      "Iteration 365, loss = 0.32386092\n",
      "Iteration 366, loss = 0.32360415\n",
      "Iteration 367, loss = 0.32342458\n",
      "Iteration 368, loss = 0.32333270\n",
      "Iteration 369, loss = 0.32314169\n",
      "Iteration 370, loss = 0.32279547\n",
      "Iteration 371, loss = 0.32246431\n",
      "Iteration 372, loss = 0.32211666\n",
      "Iteration 373, loss = 0.32170500\n",
      "Iteration 374, loss = 0.32130984\n",
      "Iteration 375, loss = 0.32093980\n",
      "Iteration 376, loss = 0.32073800\n",
      "Iteration 377, loss = 0.32041838\n",
      "Iteration 378, loss = 0.32004972\n",
      "Iteration 379, loss = 0.31973592\n",
      "Iteration 380, loss = 0.31955749\n",
      "Iteration 381, loss = 0.31952775\n",
      "Iteration 382, loss = 0.31928810\n",
      "Iteration 383, loss = 0.31895472\n",
      "Iteration 384, loss = 0.31864915\n",
      "Iteration 385, loss = 0.31836766\n",
      "Iteration 386, loss = 0.31829261\n",
      "Iteration 387, loss = 0.31825954\n",
      "Iteration 388, loss = 0.31815309\n",
      "Iteration 389, loss = 0.31794983\n",
      "Iteration 390, loss = 0.31776348\n",
      "Iteration 391, loss = 0.31751334\n",
      "Iteration 392, loss = 0.31748440\n",
      "Iteration 393, loss = 0.31680081\n",
      "Iteration 394, loss = 0.31602377\n",
      "Iteration 395, loss = 0.31539370\n",
      "Iteration 396, loss = 0.31479292\n",
      "Iteration 397, loss = 0.31418395\n",
      "Iteration 398, loss = 0.31375356\n",
      "Iteration 399, loss = 0.31333054\n",
      "Iteration 400, loss = 0.31306350\n",
      "Iteration 401, loss = 0.31282294\n",
      "Iteration 402, loss = 0.31256973\n",
      "Iteration 403, loss = 0.31238782\n",
      "Iteration 404, loss = 0.31200850\n",
      "Iteration 405, loss = 0.31166975\n",
      "Iteration 406, loss = 0.31138149\n",
      "Iteration 407, loss = 0.31112828\n",
      "Iteration 408, loss = 0.31089789\n",
      "Iteration 409, loss = 0.31067518\n",
      "Iteration 410, loss = 0.31045394\n",
      "Iteration 411, loss = 0.31021927\n",
      "Iteration 412, loss = 0.30989988\n",
      "Iteration 413, loss = 0.30972923\n",
      "Iteration 414, loss = 0.30942619\n",
      "Iteration 415, loss = 0.30916462\n",
      "Iteration 416, loss = 0.30879363\n",
      "Iteration 417, loss = 0.30844267\n",
      "Iteration 418, loss = 0.30812830\n",
      "Iteration 419, loss = 0.30785235\n",
      "Iteration 420, loss = 0.30759526\n",
      "Iteration 421, loss = 0.30737121\n",
      "Iteration 422, loss = 0.30714714\n",
      "Iteration 423, loss = 0.30693172\n",
      "Iteration 424, loss = 0.30670745\n",
      "Iteration 425, loss = 0.30645099\n",
      "Iteration 426, loss = 0.30618240\n",
      "Iteration 427, loss = 0.30589068\n",
      "Iteration 428, loss = 0.30576267\n",
      "Iteration 429, loss = 0.30591141\n",
      "Iteration 430, loss = 0.30544001\n",
      "Iteration 431, loss = 0.30509997\n",
      "Iteration 432, loss = 0.30481544\n",
      "Iteration 433, loss = 0.30461560\n",
      "Iteration 434, loss = 0.30455354\n",
      "Iteration 435, loss = 0.30464695\n",
      "Iteration 436, loss = 0.30466038\n",
      "Iteration 437, loss = 0.30463674\n",
      "Iteration 438, loss = 0.30459788\n",
      "Iteration 439, loss = 0.30435078\n",
      "Iteration 440, loss = 0.30403642\n",
      "Iteration 441, loss = 0.30379460\n",
      "Iteration 442, loss = 0.30334522\n",
      "Iteration 443, loss = 0.30296383\n",
      "Iteration 444, loss = 0.30253537\n",
      "Iteration 445, loss = 0.30213392\n",
      "Iteration 446, loss = 0.30189699\n",
      "Iteration 447, loss = 0.30156753\n",
      "Iteration 448, loss = 0.30120691\n",
      "Iteration 449, loss = 0.30092551\n",
      "Iteration 450, loss = 0.30071928\n",
      "Iteration 451, loss = 0.30038116\n",
      "Iteration 452, loss = 0.30002411\n",
      "Iteration 453, loss = 0.29974762\n",
      "Iteration 454, loss = 0.29949297\n",
      "Iteration 455, loss = 0.29940192\n",
      "Iteration 456, loss = 0.29910296\n",
      "Iteration 457, loss = 0.29857020\n",
      "Iteration 458, loss = 0.29801443\n",
      "Iteration 459, loss = 0.29755863\n",
      "Iteration 460, loss = 0.29724246\n",
      "Iteration 461, loss = 0.29692399\n",
      "Iteration 462, loss = 0.29670231\n",
      "Iteration 463, loss = 0.29648507\n",
      "Iteration 464, loss = 0.29623335\n",
      "Iteration 465, loss = 0.29594975\n",
      "Iteration 466, loss = 0.29577977\n",
      "Iteration 467, loss = 0.29551981\n",
      "Iteration 468, loss = 0.29533288\n",
      "Iteration 469, loss = 0.29515684\n",
      "Iteration 470, loss = 0.29498283\n",
      "Iteration 471, loss = 0.29473296\n",
      "Iteration 472, loss = 0.29452187\n",
      "Iteration 473, loss = 0.29430463\n",
      "Iteration 474, loss = 0.29416439\n",
      "Iteration 475, loss = 0.29382538\n",
      "Iteration 476, loss = 0.29349939\n",
      "Iteration 477, loss = 0.29338359\n",
      "Iteration 478, loss = 0.29366248\n",
      "Iteration 479, loss = 0.29342508\n",
      "Iteration 480, loss = 0.29328582\n",
      "Iteration 481, loss = 0.29329582\n",
      "Iteration 482, loss = 0.29333980\n",
      "Iteration 483, loss = 0.29300550\n",
      "Iteration 484, loss = 0.29249986\n",
      "Iteration 485, loss = 0.29205545\n",
      "Iteration 486, loss = 0.29144926\n",
      "Iteration 487, loss = 0.29091059\n",
      "Iteration 488, loss = 0.29049051\n",
      "Iteration 489, loss = 0.29012992\n",
      "Iteration 490, loss = 0.28980864\n",
      "Iteration 491, loss = 0.28956227\n",
      "Iteration 492, loss = 0.28937070\n",
      "Iteration 493, loss = 0.28912813\n",
      "Iteration 494, loss = 0.28881045\n",
      "Iteration 495, loss = 0.28849484\n",
      "Iteration 496, loss = 0.28823068\n",
      "Iteration 497, loss = 0.28799419\n",
      "Iteration 498, loss = 0.28755083\n",
      "Iteration 499, loss = 0.28717823\n",
      "Iteration 500, loss = 0.28679236\n",
      "Iteration 1, loss = 0.71669237\n",
      "Iteration 2, loss = 0.71613537\n",
      "Iteration 3, loss = 0.71517297\n",
      "Iteration 4, loss = 0.71406516\n",
      "Iteration 5, loss = 0.71268046\n",
      "Iteration 6, loss = 0.71106292\n",
      "Iteration 7, loss = 0.70939032\n",
      "Iteration 8, loss = 0.70744188\n",
      "Iteration 9, loss = 0.70546687\n",
      "Iteration 10, loss = 0.70337125\n",
      "Iteration 11, loss = 0.70099784\n",
      "Iteration 12, loss = 0.69853604\n",
      "Iteration 13, loss = 0.69605216\n",
      "Iteration 14, loss = 0.69355950\n",
      "Iteration 15, loss = 0.69107264\n",
      "Iteration 16, loss = 0.68853897\n",
      "Iteration 17, loss = 0.68599182\n",
      "Iteration 18, loss = 0.68345530\n",
      "Iteration 19, loss = 0.68094560\n",
      "Iteration 20, loss = 0.67817388\n",
      "Iteration 21, loss = 0.67540057\n",
      "Iteration 22, loss = 0.67270340\n",
      "Iteration 23, loss = 0.67008814\n",
      "Iteration 24, loss = 0.66752769\n",
      "Iteration 25, loss = 0.66506137\n",
      "Iteration 26, loss = 0.66269433\n",
      "Iteration 27, loss = 0.66045631\n",
      "Iteration 28, loss = 0.65825722\n",
      "Iteration 29, loss = 0.65598864\n",
      "Iteration 30, loss = 0.65380864\n",
      "Iteration 31, loss = 0.65175027\n",
      "Iteration 32, loss = 0.64970890\n",
      "Iteration 33, loss = 0.64765204\n",
      "Iteration 34, loss = 0.64561558\n",
      "Iteration 35, loss = 0.64349439\n",
      "Iteration 36, loss = 0.64139138\n",
      "Iteration 37, loss = 0.63936381\n",
      "Iteration 38, loss = 0.63724987\n",
      "Iteration 39, loss = 0.63520451\n",
      "Iteration 40, loss = 0.63311666\n",
      "Iteration 41, loss = 0.63107565\n",
      "Iteration 42, loss = 0.62904297\n",
      "Iteration 43, loss = 0.62702491\n",
      "Iteration 44, loss = 0.62493791\n",
      "Iteration 45, loss = 0.62290149\n",
      "Iteration 46, loss = 0.62103376\n",
      "Iteration 47, loss = 0.61904738\n",
      "Iteration 48, loss = 0.61702582\n",
      "Iteration 49, loss = 0.61511958\n",
      "Iteration 50, loss = 0.61314483\n",
      "Iteration 51, loss = 0.61109420\n",
      "Iteration 52, loss = 0.60905441\n",
      "Iteration 53, loss = 0.60723112\n",
      "Iteration 54, loss = 0.60522646\n",
      "Iteration 55, loss = 0.60324197\n",
      "Iteration 56, loss = 0.60122954\n",
      "Iteration 57, loss = 0.59928578\n",
      "Iteration 58, loss = 0.59720052\n",
      "Iteration 59, loss = 0.59515008\n",
      "Iteration 60, loss = 0.59307708\n",
      "Iteration 61, loss = 0.59102189\n",
      "Iteration 62, loss = 0.58895569\n",
      "Iteration 63, loss = 0.58661011\n",
      "Iteration 64, loss = 0.58436699\n",
      "Iteration 65, loss = 0.58214090\n",
      "Iteration 66, loss = 0.57989471\n",
      "Iteration 67, loss = 0.57777473\n",
      "Iteration 68, loss = 0.57568799\n",
      "Iteration 69, loss = 0.57360533\n",
      "Iteration 70, loss = 0.57146621\n",
      "Iteration 71, loss = 0.56935621\n",
      "Iteration 72, loss = 0.56722148\n",
      "Iteration 73, loss = 0.56500236\n",
      "Iteration 74, loss = 0.56267433\n",
      "Iteration 75, loss = 0.56072123\n",
      "Iteration 76, loss = 0.55867494\n",
      "Iteration 77, loss = 0.55665444\n",
      "Iteration 78, loss = 0.55471260\n",
      "Iteration 79, loss = 0.55287023\n",
      "Iteration 80, loss = 0.55087516\n",
      "Iteration 81, loss = 0.54910148\n",
      "Iteration 82, loss = 0.54727385\n",
      "Iteration 83, loss = 0.54536432\n",
      "Iteration 84, loss = 0.54347994\n",
      "Iteration 85, loss = 0.54176228\n",
      "Iteration 86, loss = 0.53984706\n",
      "Iteration 87, loss = 0.53791190\n",
      "Iteration 88, loss = 0.53602853\n",
      "Iteration 89, loss = 0.53414321\n",
      "Iteration 90, loss = 0.53221975\n",
      "Iteration 91, loss = 0.53037691\n",
      "Iteration 92, loss = 0.52847044\n",
      "Iteration 93, loss = 0.52676953\n",
      "Iteration 94, loss = 0.52499951\n",
      "Iteration 95, loss = 0.52300187\n",
      "Iteration 96, loss = 0.52096851\n",
      "Iteration 97, loss = 0.51914360\n",
      "Iteration 98, loss = 0.51713514\n",
      "Iteration 99, loss = 0.51524696\n",
      "Iteration 100, loss = 0.51335611\n",
      "Iteration 101, loss = 0.51143653\n",
      "Iteration 102, loss = 0.50953941\n",
      "Iteration 103, loss = 0.50753747\n",
      "Iteration 104, loss = 0.50553664\n",
      "Iteration 105, loss = 0.50356938\n",
      "Iteration 106, loss = 0.50170713\n",
      "Iteration 107, loss = 0.49985704\n",
      "Iteration 108, loss = 0.49795790\n",
      "Iteration 109, loss = 0.49627692\n",
      "Iteration 110, loss = 0.49469277\n",
      "Iteration 111, loss = 0.49310176\n",
      "Iteration 112, loss = 0.49162660\n",
      "Iteration 113, loss = 0.48999964\n",
      "Iteration 114, loss = 0.48830148\n",
      "Iteration 115, loss = 0.48659530\n",
      "Iteration 116, loss = 0.48491736\n",
      "Iteration 117, loss = 0.48315858\n",
      "Iteration 118, loss = 0.48141384\n",
      "Iteration 119, loss = 0.47968178\n",
      "Iteration 120, loss = 0.47794005\n",
      "Iteration 121, loss = 0.47631276\n",
      "Iteration 122, loss = 0.47501916\n",
      "Iteration 123, loss = 0.47376715\n",
      "Iteration 124, loss = 0.47236593\n",
      "Iteration 125, loss = 0.47081473\n",
      "Iteration 126, loss = 0.46920137\n",
      "Iteration 127, loss = 0.46776699\n",
      "Iteration 128, loss = 0.46631296\n",
      "Iteration 129, loss = 0.46478919\n",
      "Iteration 130, loss = 0.46316656\n",
      "Iteration 131, loss = 0.46144616\n",
      "Iteration 132, loss = 0.45975983\n",
      "Iteration 133, loss = 0.45801449\n",
      "Iteration 134, loss = 0.45623045\n",
      "Iteration 135, loss = 0.45452577\n",
      "Iteration 136, loss = 0.45304045\n",
      "Iteration 137, loss = 0.45191628\n",
      "Iteration 138, loss = 0.45076717\n",
      "Iteration 139, loss = 0.44973479\n",
      "Iteration 140, loss = 0.44859056\n",
      "Iteration 141, loss = 0.44735972\n",
      "Iteration 142, loss = 0.44604645\n",
      "Iteration 143, loss = 0.44477972\n",
      "Iteration 144, loss = 0.44345015\n",
      "Iteration 145, loss = 0.44207634\n",
      "Iteration 146, loss = 0.44088645\n",
      "Iteration 147, loss = 0.43949098\n",
      "Iteration 148, loss = 0.43825779\n",
      "Iteration 149, loss = 0.43712558\n",
      "Iteration 150, loss = 0.43599649\n",
      "Iteration 151, loss = 0.43478608\n",
      "Iteration 152, loss = 0.43355529\n",
      "Iteration 153, loss = 0.43251714\n",
      "Iteration 154, loss = 0.43185582\n",
      "Iteration 155, loss = 0.43097828\n",
      "Iteration 156, loss = 0.42987779\n",
      "Iteration 157, loss = 0.42889736\n",
      "Iteration 158, loss = 0.42777158\n",
      "Iteration 159, loss = 0.42651104\n",
      "Iteration 160, loss = 0.42529488\n",
      "Iteration 161, loss = 0.42407890\n",
      "Iteration 162, loss = 0.42281650\n",
      "Iteration 163, loss = 0.42150705\n",
      "Iteration 164, loss = 0.42037848\n",
      "Iteration 165, loss = 0.41948349\n",
      "Iteration 166, loss = 0.41874058\n",
      "Iteration 167, loss = 0.41790883\n",
      "Iteration 168, loss = 0.41710794\n",
      "Iteration 169, loss = 0.41610543\n",
      "Iteration 170, loss = 0.41506134\n",
      "Iteration 171, loss = 0.41402544\n",
      "Iteration 172, loss = 0.41319868\n",
      "Iteration 173, loss = 0.41234626\n",
      "Iteration 174, loss = 0.41156106\n",
      "Iteration 175, loss = 0.41085596\n",
      "Iteration 176, loss = 0.40998421\n",
      "Iteration 177, loss = 0.40906327\n",
      "Iteration 178, loss = 0.40811223\n",
      "Iteration 179, loss = 0.40729060\n",
      "Iteration 180, loss = 0.40647465\n",
      "Iteration 181, loss = 0.40570598\n",
      "Iteration 182, loss = 0.40492474\n",
      "Iteration 183, loss = 0.40396432\n",
      "Iteration 184, loss = 0.40297163\n",
      "Iteration 185, loss = 0.40207130\n",
      "Iteration 186, loss = 0.40112486\n",
      "Iteration 187, loss = 0.40033651\n",
      "Iteration 188, loss = 0.39935735\n",
      "Iteration 189, loss = 0.39860719\n",
      "Iteration 190, loss = 0.39794964\n",
      "Iteration 191, loss = 0.39730488\n",
      "Iteration 192, loss = 0.39677223\n",
      "Iteration 193, loss = 0.39627094\n",
      "Iteration 194, loss = 0.39576780\n",
      "Iteration 195, loss = 0.39525280\n",
      "Iteration 196, loss = 0.39489143\n",
      "Iteration 197, loss = 0.39464128\n",
      "Iteration 198, loss = 0.39412716\n",
      "Iteration 199, loss = 0.39363703\n",
      "Iteration 200, loss = 0.39322637\n",
      "Iteration 201, loss = 0.39258885\n",
      "Iteration 202, loss = 0.39179224\n",
      "Iteration 203, loss = 0.39092538\n",
      "Iteration 204, loss = 0.38993290\n",
      "Iteration 205, loss = 0.38892912\n",
      "Iteration 206, loss = 0.38802345\n",
      "Iteration 207, loss = 0.38716370\n",
      "Iteration 208, loss = 0.38638165\n",
      "Iteration 209, loss = 0.38564649\n",
      "Iteration 210, loss = 0.38496595\n",
      "Iteration 211, loss = 0.38434490\n",
      "Iteration 212, loss = 0.38379209\n",
      "Iteration 213, loss = 0.38314457\n",
      "Iteration 214, loss = 0.38254982\n",
      "Iteration 215, loss = 0.38196266\n",
      "Iteration 216, loss = 0.38138700\n",
      "Iteration 217, loss = 0.38093165\n",
      "Iteration 218, loss = 0.38044648\n",
      "Iteration 219, loss = 0.37988384\n",
      "Iteration 220, loss = 0.37936863\n",
      "Iteration 221, loss = 0.37882449\n",
      "Iteration 222, loss = 0.37838346\n",
      "Iteration 223, loss = 0.37792531\n",
      "Iteration 224, loss = 0.37741702\n",
      "Iteration 225, loss = 0.37678443\n",
      "Iteration 226, loss = 0.37611024\n",
      "Iteration 227, loss = 0.37545502\n",
      "Iteration 228, loss = 0.37487974\n",
      "Iteration 229, loss = 0.37434587\n",
      "Iteration 230, loss = 0.37381605\n",
      "Iteration 231, loss = 0.37335915\n",
      "Iteration 232, loss = 0.37286140\n",
      "Iteration 233, loss = 0.37245468\n",
      "Iteration 234, loss = 0.37210980\n",
      "Iteration 235, loss = 0.37174910\n",
      "Iteration 236, loss = 0.37132752\n",
      "Iteration 237, loss = 0.37101574\n",
      "Iteration 238, loss = 0.37074349\n",
      "Iteration 239, loss = 0.37047825\n",
      "Iteration 240, loss = 0.37021878\n",
      "Iteration 241, loss = 0.36998121\n",
      "Iteration 242, loss = 0.36962635\n",
      "Iteration 243, loss = 0.36920406\n",
      "Iteration 244, loss = 0.36866828\n",
      "Iteration 245, loss = 0.36816725\n",
      "Iteration 246, loss = 0.36766525\n",
      "Iteration 247, loss = 0.36708526\n",
      "Iteration 248, loss = 0.36659679\n",
      "Iteration 249, loss = 0.36614491\n",
      "Iteration 250, loss = 0.36579042\n",
      "Iteration 251, loss = 0.36543981\n",
      "Iteration 252, loss = 0.36499410\n",
      "Iteration 253, loss = 0.36472464\n",
      "Iteration 254, loss = 0.36430793\n",
      "Iteration 255, loss = 0.36398936\n",
      "Iteration 256, loss = 0.36379096\n",
      "Iteration 257, loss = 0.36378750\n",
      "Iteration 258, loss = 0.36371834\n",
      "Iteration 259, loss = 0.36357922\n",
      "Iteration 260, loss = 0.36329810\n",
      "Iteration 261, loss = 0.36304272\n",
      "Iteration 262, loss = 0.36281638\n",
      "Iteration 263, loss = 0.36237461\n",
      "Iteration 264, loss = 0.36179652\n",
      "Iteration 265, loss = 0.36142623\n",
      "Iteration 266, loss = 0.36086798\n",
      "Iteration 267, loss = 0.36029244\n",
      "Iteration 268, loss = 0.35963460\n",
      "Iteration 269, loss = 0.35900115\n",
      "Iteration 270, loss = 0.35842005\n",
      "Iteration 271, loss = 0.35786554\n",
      "Iteration 272, loss = 0.35732777\n",
      "Iteration 273, loss = 0.35685602\n",
      "Iteration 274, loss = 0.35648370\n",
      "Iteration 275, loss = 0.35609034\n",
      "Iteration 276, loss = 0.35569019\n",
      "Iteration 277, loss = 0.35531328\n",
      "Iteration 278, loss = 0.35494810\n",
      "Iteration 279, loss = 0.35453101\n",
      "Iteration 280, loss = 0.35411084\n",
      "Iteration 281, loss = 0.35380402\n",
      "Iteration 282, loss = 0.35346499\n",
      "Iteration 283, loss = 0.35298511\n",
      "Iteration 284, loss = 0.35260429\n",
      "Iteration 285, loss = 0.35219272\n",
      "Iteration 286, loss = 0.35193366\n",
      "Iteration 287, loss = 0.35150890\n",
      "Iteration 288, loss = 0.35111661\n",
      "Iteration 289, loss = 0.35088672\n",
      "Iteration 290, loss = 0.35062867\n",
      "Iteration 291, loss = 0.35043056\n",
      "Iteration 292, loss = 0.35024360\n",
      "Iteration 293, loss = 0.34986879\n",
      "Iteration 294, loss = 0.34954809\n",
      "Iteration 295, loss = 0.34894734\n",
      "Iteration 296, loss = 0.34842630\n",
      "Iteration 297, loss = 0.34782583\n",
      "Iteration 298, loss = 0.34734108\n",
      "Iteration 299, loss = 0.34684725\n",
      "Iteration 300, loss = 0.34637731\n",
      "Iteration 301, loss = 0.34594153\n",
      "Iteration 302, loss = 0.34550060\n",
      "Iteration 303, loss = 0.34508079\n",
      "Iteration 304, loss = 0.34470871\n",
      "Iteration 305, loss = 0.34435488\n",
      "Iteration 306, loss = 0.34398621\n",
      "Iteration 307, loss = 0.34357499\n",
      "Iteration 308, loss = 0.34321117\n",
      "Iteration 309, loss = 0.34286899\n",
      "Iteration 310, loss = 0.34253234\n",
      "Iteration 311, loss = 0.34224644\n",
      "Iteration 312, loss = 0.34196889\n",
      "Iteration 313, loss = 0.34167373\n",
      "Iteration 314, loss = 0.34127449\n",
      "Iteration 315, loss = 0.34095673\n",
      "Iteration 316, loss = 0.34068273\n",
      "Iteration 317, loss = 0.34040055\n",
      "Iteration 318, loss = 0.34008379\n",
      "Iteration 319, loss = 0.33986178\n",
      "Iteration 320, loss = 0.33959620\n",
      "Iteration 321, loss = 0.33945756\n",
      "Iteration 322, loss = 0.33935457\n",
      "Iteration 323, loss = 0.33951935\n",
      "Iteration 324, loss = 0.33964996\n",
      "Iteration 325, loss = 0.33957024\n",
      "Iteration 326, loss = 0.33934568\n",
      "Iteration 327, loss = 0.33906833\n",
      "Iteration 328, loss = 0.33847836\n",
      "Iteration 329, loss = 0.33792729\n",
      "Iteration 330, loss = 0.33759650\n",
      "Iteration 331, loss = 0.33741345\n",
      "Iteration 332, loss = 0.33708236\n",
      "Iteration 333, loss = 0.33667550\n",
      "Iteration 334, loss = 0.33633655\n",
      "Iteration 335, loss = 0.33599902\n",
      "Iteration 336, loss = 0.33568237\n",
      "Iteration 337, loss = 0.33538328\n",
      "Iteration 338, loss = 0.33514822\n",
      "Iteration 339, loss = 0.33473456\n",
      "Iteration 340, loss = 0.33435006\n",
      "Iteration 341, loss = 0.33396466\n",
      "Iteration 342, loss = 0.33371773\n",
      "Iteration 343, loss = 0.33355840\n",
      "Iteration 344, loss = 0.33341195\n",
      "Iteration 345, loss = 0.33323864\n",
      "Iteration 346, loss = 0.33291914\n",
      "Iteration 347, loss = 0.33269257\n",
      "Iteration 348, loss = 0.33233227\n",
      "Iteration 349, loss = 0.33210637\n",
      "Iteration 350, loss = 0.33169290\n",
      "Iteration 351, loss = 0.33125040\n",
      "Iteration 352, loss = 0.33063199\n",
      "Iteration 353, loss = 0.33013081\n",
      "Iteration 354, loss = 0.32973372\n",
      "Iteration 355, loss = 0.32925095\n",
      "Iteration 356, loss = 0.32884484\n",
      "Iteration 357, loss = 0.32841684\n",
      "Iteration 358, loss = 0.32800720\n",
      "Iteration 359, loss = 0.32765516\n",
      "Iteration 360, loss = 0.32752109\n",
      "Iteration 361, loss = 0.32757584\n",
      "Iteration 362, loss = 0.32735520\n",
      "Iteration 363, loss = 0.32702063\n",
      "Iteration 364, loss = 0.32673563\n",
      "Iteration 365, loss = 0.32633058\n",
      "Iteration 366, loss = 0.32602521\n",
      "Iteration 367, loss = 0.32573748\n",
      "Iteration 368, loss = 0.32560905\n",
      "Iteration 369, loss = 0.32541578\n",
      "Iteration 370, loss = 0.32506052\n",
      "Iteration 371, loss = 0.32472002\n",
      "Iteration 372, loss = 0.32436850\n",
      "Iteration 373, loss = 0.32397223\n",
      "Iteration 374, loss = 0.32359888\n",
      "Iteration 375, loss = 0.32323262\n",
      "Iteration 376, loss = 0.32303042\n",
      "Iteration 377, loss = 0.32274036\n",
      "Iteration 378, loss = 0.32241930\n",
      "Iteration 379, loss = 0.32215906\n",
      "Iteration 380, loss = 0.32207002\n",
      "Iteration 381, loss = 0.32212736\n",
      "Iteration 382, loss = 0.32195685\n",
      "Iteration 383, loss = 0.32171112\n",
      "Iteration 384, loss = 0.32147296\n",
      "Iteration 385, loss = 0.32127178\n",
      "Iteration 386, loss = 0.32130927\n",
      "Iteration 387, loss = 0.32152022\n",
      "Iteration 388, loss = 0.32162820\n",
      "Iteration 389, loss = 0.32156170\n",
      "Iteration 390, loss = 0.32145736\n",
      "Iteration 391, loss = 0.32126801\n",
      "Iteration 392, loss = 0.32140925\n",
      "Iteration 393, loss = 0.32045322\n",
      "Iteration 394, loss = 0.31945354\n",
      "Iteration 395, loss = 0.31865575\n",
      "Iteration 396, loss = 0.31801212\n",
      "Iteration 397, loss = 0.31734869\n",
      "Iteration 398, loss = 0.31682717\n",
      "Iteration 399, loss = 0.31636603\n",
      "Iteration 400, loss = 0.31599678\n",
      "Iteration 401, loss = 0.31569023\n",
      "Iteration 402, loss = 0.31540887\n",
      "Iteration 403, loss = 0.31514579\n",
      "Iteration 404, loss = 0.31482223\n",
      "Iteration 405, loss = 0.31465844\n",
      "Iteration 406, loss = 0.31444075\n",
      "Iteration 407, loss = 0.31430384\n",
      "Iteration 408, loss = 0.31424656\n",
      "Iteration 409, loss = 0.31412194\n",
      "Iteration 410, loss = 0.31393335\n",
      "Iteration 411, loss = 0.31364009\n",
      "Iteration 412, loss = 0.31339879\n",
      "Iteration 413, loss = 0.31334528\n",
      "Iteration 414, loss = 0.31298929\n",
      "Iteration 415, loss = 0.31251118\n",
      "Iteration 416, loss = 0.31207971\n",
      "Iteration 417, loss = 0.31160677\n",
      "Iteration 418, loss = 0.31124064\n",
      "Iteration 419, loss = 0.31086636\n",
      "Iteration 420, loss = 0.31055939\n",
      "Iteration 421, loss = 0.31030484\n",
      "Iteration 422, loss = 0.31008139\n",
      "Iteration 423, loss = 0.30983971\n",
      "Iteration 424, loss = 0.30961851\n",
      "Iteration 425, loss = 0.30928640\n",
      "Iteration 426, loss = 0.30892247\n",
      "Iteration 427, loss = 0.30849772\n",
      "Iteration 428, loss = 0.30818573\n",
      "Iteration 429, loss = 0.30821272\n",
      "Iteration 430, loss = 0.30776558\n",
      "Iteration 431, loss = 0.30748400\n",
      "Iteration 432, loss = 0.30728518\n",
      "Iteration 433, loss = 0.30698248\n",
      "Iteration 434, loss = 0.30681449\n",
      "Iteration 435, loss = 0.30670498\n",
      "Iteration 436, loss = 0.30658467\n",
      "Iteration 437, loss = 0.30643365\n",
      "Iteration 438, loss = 0.30634985\n",
      "Iteration 439, loss = 0.30612289\n",
      "Iteration 440, loss = 0.30584961\n",
      "Iteration 441, loss = 0.30564321\n",
      "Iteration 442, loss = 0.30519577\n",
      "Iteration 443, loss = 0.30481825\n",
      "Iteration 444, loss = 0.30445788\n",
      "Iteration 445, loss = 0.30406861\n",
      "Iteration 446, loss = 0.30369790\n",
      "Iteration 447, loss = 0.30337529\n",
      "Iteration 448, loss = 0.30309408\n",
      "Iteration 449, loss = 0.30277511\n",
      "Iteration 450, loss = 0.30245325\n",
      "Iteration 451, loss = 0.30217783\n",
      "Iteration 452, loss = 0.30191962\n",
      "Iteration 453, loss = 0.30172568\n",
      "Iteration 454, loss = 0.30151143\n",
      "Iteration 455, loss = 0.30119636\n",
      "Iteration 456, loss = 0.30094541\n",
      "Iteration 457, loss = 0.30076844\n",
      "Iteration 458, loss = 0.30067667\n",
      "Iteration 459, loss = 0.30069696\n",
      "Iteration 460, loss = 0.30084364\n",
      "Iteration 461, loss = 0.30052031\n",
      "Iteration 462, loss = 0.30024703\n",
      "Iteration 463, loss = 0.29998873\n",
      "Iteration 464, loss = 0.29994161\n",
      "Iteration 465, loss = 0.29997972\n",
      "Iteration 466, loss = 0.30020593\n",
      "Iteration 467, loss = 0.29990676\n",
      "Iteration 468, loss = 0.29957629\n",
      "Iteration 469, loss = 0.29934473\n",
      "Iteration 470, loss = 0.29907076\n",
      "Iteration 471, loss = 0.29898574\n",
      "Iteration 472, loss = 0.29851807\n",
      "Iteration 473, loss = 0.29819285\n",
      "Iteration 474, loss = 0.29765388\n",
      "Iteration 475, loss = 0.29724437\n",
      "Iteration 476, loss = 0.29658615\n",
      "Iteration 477, loss = 0.29595245\n",
      "Iteration 478, loss = 0.29567226\n",
      "Iteration 479, loss = 0.29527591\n",
      "Iteration 480, loss = 0.29506315\n",
      "Iteration 481, loss = 0.29496602\n",
      "Iteration 482, loss = 0.29481457\n",
      "Iteration 483, loss = 0.29454026\n",
      "Iteration 484, loss = 0.29419518\n",
      "Iteration 485, loss = 0.29384311\n",
      "Iteration 486, loss = 0.29344597\n",
      "Iteration 487, loss = 0.29322434\n",
      "Iteration 488, loss = 0.29301000\n",
      "Iteration 489, loss = 0.29281897\n",
      "Iteration 490, loss = 0.29250721\n",
      "Iteration 491, loss = 0.29201785\n",
      "Iteration 492, loss = 0.29161405\n",
      "Iteration 493, loss = 0.29127186\n",
      "Iteration 494, loss = 0.29095270\n",
      "Iteration 495, loss = 0.29070404\n",
      "Iteration 496, loss = 0.29034695\n",
      "Iteration 497, loss = 0.29004399\n",
      "Iteration 498, loss = 0.28974977\n",
      "Iteration 499, loss = 0.28949004\n",
      "Iteration 500, loss = 0.28927588\n",
      "Mean Accuracy of Cross Validation: % 77.96\n",
      "Std of Accuracy of Cross Validation: % 5.0\n",
      "Accuracy Score of the MLP classifier:  0.8552631578947368\n"
     ]
    }
   ],
   "source": [
    "cv = ShuffleSplit(n_splits=10, test_size=0.21, random_state=42)\n",
    "scores = cross_val_score(mlp, x_train, y_train, cv=10)\n",
    "print(\"Mean Accuracy of Cross Validation: %\", round(scores.mean()*100,2))\n",
    "print(\"Std of Accuracy of Cross Validation: %\", round(scores.std()*100))\n",
    "y_pred_mlp = mlp.predict(x_test)\n",
    "print(\"Accuracy Score of the MLP classifier: \", accuracy_score(y_test, y_pred_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "scores = []\n",
    "for each in range(1,55):\n",
    "    RF = RandomForestClassifier(n_estimators = each,random_state=5)\n",
    "    RF.fit(x_train,y_train)\n",
    "    scores.append(RF.score(x_test, y_test))\n",
    "    \n",
    "plt.figure(1, figsize=(10, 5))\n",
    "plt.plot(range(1,55),scores,color=\"purple\",linewidth=3)\n",
    "plt.title(\" N Estimator Value of Random Forest\")\n",
    "plt.xlabel(\"N Estimators\")\n",
    "plt.ylabel(\"Score(Accuracy)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "scores = []\n",
    "for n in range(1,55):\n",
    "    KNNfind = KNeighborsClassifier(n_neighbors = n)\n",
    "    KNNfind.fit(x_train,y_train)\n",
    "    scores.append(KNNfind.score(x_test,y_test))\n",
    "    \n",
    "plt.plot(range(1,55),scores,color=\"blue\")\n",
    "plt.xlabel(\"K Values of KNN\")\n",
    "plt.ylabel(\"Accuracy for K values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy of Cross Validation: % 70.7\n",
      "Std of Accuracy of Cross Validation: % 3.0\n",
      "Accuracy Score of the KNN classifier:  0.82\n"
     ]
    }
   ],
   "source": [
    "KNN = KNeighborsClassifier(n_neighbors = 41) \n",
    "KNN=KNN.fit(x_train, y_train)\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.21, random_state=42)\n",
    "scores = cross_val_score(KNN, x_train, y_train, cv=10)\n",
    "print(\"Mean Accuracy of Cross Validation: %\", round(scores.mean()*100,2))\n",
    "print(\"Std of Accuracy of Cross Validation: %\", round(scores.std()*100))\n",
    "y_pred_knn = KNN.predict(x_test)\n",
    "print(\"Accuracy Score of the KNN classifier: \", accuracy_score(y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy of Cross Validation: % 71.01\n",
      "Std of Accuracy of Cross Validation: % 14.0\n",
      "Accuracy Score of the DT classifier:  0.58\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DT = DecisionTreeClassifier()\n",
    "DT=DT.fit(x_train, y_train)\n",
    "y_pred_dt = DT.predict(x_test)\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.21, random_state=42)\n",
    "scores = cross_val_score(DT, x_train, y_train, cv=10)\n",
    "print(\"Mean Accuracy of Cross Validation: %\", round(scores.mean()*100,2))\n",
    "print(\"Std of Accuracy of Cross Validation: %\", round(scores.std()*100))\n",
    "y_pred_dt = DT.predict(x_test)\n",
    "print(\"Accuracy Score of the DT classifier: \", accuracy_score(y_test, y_pred_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy of Cross Validation: % 65.55\n",
      "Std of Accuracy of Cross Validation: % 10.0\n",
      "Accuracy Score of the NB classifier:  0.62\n"
     ]
    }
   ],
   "source": [
    "gnb=GaussianNB()\n",
    "gnb=gnb.fit(x_train, y_train)\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.21, random_state=42)\n",
    "scores = cross_val_score(gnb, x_train, y_train, cv=10)\n",
    "print(\"Mean Accuracy of Cross Validation: %\", round(scores.mean()*100,2))\n",
    "print(\"Std of Accuracy of Cross Validation: %\", round(scores.std()*100))\n",
    "y_pred_gnb = gnb.predict(x_test)\n",
    "print(\"Accuracy Score of the NB classifier: \", accuracy_score(y_test, y_pred_gnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy of Cross Validation: % 74.71\n",
      "Std of Accuracy of Cross Validation: % 11.0\n",
      "Accuracy Score of the SVM classifier:  0.86\n"
     ]
    }
   ],
   "source": [
    "SVM = SVC()\n",
    "SVM=SVM.fit(x_train, y_train)\n",
    "y_pred_svm = SVM.predict(x_test)\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.21, random_state=42)\n",
    "scores = cross_val_score(SVM, x_train, y_train, cv=10)\n",
    "print(\"Mean Accuracy of Cross Validation: %\", round(scores.mean()*100,2))\n",
    "print(\"Std of Accuracy of Cross Validation: %\", round(scores.std()*100))\n",
    "y_pred_svm = SVM.predict(x_test)\n",
    "print(\"Accuracy Score of the SVM classifier: \", accuracy_score(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy of Cross Validation: % 76.62\n",
      "Std of Accuracy of Cross Validation: % 10.0\n",
      "Accuracy Score of the DT classifier:  0.72\n"
     ]
    }
   ],
   "source": [
    "RF= RandomForestClassifier(n_estimators = 42) \n",
    "RF=RF.fit(x_train, y_train)\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.21, random_state=42)\n",
    "scores = cross_val_score(RF, x_train, y_train, cv=10)\n",
    "print(\"Mean Accuracy of Cross Validation: %\", round(scores.mean()*100,2))\n",
    "print(\"Std of Accuracy of Cross Validation: %\", round(scores.std()*100))\n",
    "y_pred_rf = RF.predict(x_test)\n",
    "print(\"Accuracy Score of the DT classifier: \", accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy of Cross Validation: % 72.7\n",
      "Std of Accuracy of Cross Validation: % 11.0\n",
      "Accuracy Score of the XGB classifier:  0.58\n"
     ]
    }
   ],
   "source": [
    "xg = XGBClassifier()\n",
    "xg=xg.fit(x_train, y_train)\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.21, random_state=42)\n",
    "scores = cross_val_score(xg, x_train, y_train, cv=10)\n",
    "print(\"Mean Accuracy of Cross Validation: %\", round(scores.mean()*100,2))\n",
    "print(\"Std of Accuracy of Cross Validation: %\", round(scores.std()*100))\n",
    "y_pred_xg = xg.predict(x_test)\n",
    "print(\"Accuracy Score of the XGB classifier: \", accuracy_score(y_test, y_pred_xg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for SVM:  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "    shrinking=True, tol=0.001, verbose=False)\n",
      "Best Score for SVM:  0.7666666666666667\n",
      "Mean Accuracy of Cross Validation: % 69.24\n",
      "Std of Accuracy of Cross Validation: % 9.0\n"
     ]
    }
   ],
   "source": [
    "svc_params = {'C':[0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0],\n",
    "                      'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\n",
    "grid_svc = GridSearchCV(SVC(), svc_params)\n",
    "grid_svc.fit(x_train, y_train)\n",
    "svc = grid_svc.best_estimator_\n",
    "print(\"Best Parameters for SVM: \", grid_svc.best_estimator_)\n",
    "print(\"Best Score for SVM: \", grid_svc.best_score_)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.21, random_state=42)\n",
    "scores = cross_val_score(grid_svc, x_train, y_train, cv=10)\n",
    "print(\"Mean Accuracy of Cross Validation: %\", round(scores.mean()*100,2))\n",
    "print(\"Std of Accuracy of Cross Validation: %\", round(scores.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for MLP:  MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100, 14, 7), learning_rate='adaptive',\n",
      "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "              validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "Best Score for MLP:  0.72\n",
      "Mean Accuracy of Cross Validation: % 72.67\n",
      "Std of Accuracy of Cross Validation: % 11.0\n"
     ]
    }
   ],
   "source": [
    "mlp_params = {'alpha':[0.0001], 'activation': ['relu', 'tanh',], \n",
    "              'solver':['lbfgs', 'adam'], 'hidden_layer_sizes': [(100,14,7)], \n",
    "              'learning_rate':['adaptive']}\n",
    "              \n",
    "              \n",
    "grid_mlp = GridSearchCV(MLPClassifier(), mlp_params)\n",
    "grid_mlp.fit(x_train, y_train)\n",
    "              \n",
    "\n",
    "mlp_new = grid_mlp.best_estimator_\n",
    "print(\"Best Parameters for MLP: \", grid_mlp.best_estimator_)\n",
    "print(\"Best Score for MLP: \", grid_mlp.best_score_)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.21, random_state=42)\n",
    "scores = cross_val_score(grid_mlp, x_train, y_train, cv=10)\n",
    "print(\"Mean Accuracy of Cross Validation: %\", round(scores.mean()*100,2))\n",
    "print(\"Std of Accuracy of Cross Validation: %\", round(scores.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.DataFrame(y_pred_gnb, columns=['GNB_predHeart'])   # save the file\n",
    "pred.to_csv('heart_d.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
